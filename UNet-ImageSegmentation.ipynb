{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c60eb65-31bd-4532-a62c-c149520f8c76",
   "metadata": {},
   "source": [
    "# U-Net for Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bb222f-483f-4e1c-9ffd-67d7d141da23",
   "metadata": {},
   "source": [
    "* The Montgomery datast\n",
    "* With CV and embedded Early Stopping to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc7fa47-127e-4bd1-bc5f-1a847792c6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m308s\u001b[0m 18s/step - accuracy: 0.6711 - loss: 0.6987 - val_accuracy: 0.7376 - val_loss: 0.5244\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m267s\u001b[0m 17s/step - accuracy: 0.7505 - loss: 0.4633 - val_accuracy: 0.7376 - val_loss: 0.3808\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m264s\u001b[0m 16s/step - accuracy: 0.7381 - loss: 0.3631 - val_accuracy: 0.7376 - val_loss: 0.3461\n",
      "Epoch 4/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 16s/step - accuracy: 0.7978 - loss: 0.3292 - val_accuracy: 0.9075 - val_loss: 0.3041\n",
      "Epoch 5/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 16s/step - accuracy: 0.9273 - loss: 0.2312 - val_accuracy: 0.8824 - val_loss: 0.3208\n",
      "Epoch 6/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 16s/step - accuracy: 0.9240 - loss: 0.2036 - val_accuracy: 0.9441 - val_loss: 0.1476\n",
      "Epoch 7/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m254s\u001b[0m 16s/step - accuracy: 0.9498 - loss: 0.1389 - val_accuracy: 0.9441 - val_loss: 0.1508\n",
      "Epoch 8/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 15s/step - accuracy: 0.9541 - loss: 0.1222 - val_accuracy: 0.9564 - val_loss: 0.1230\n",
      "Epoch 9/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 15s/step - accuracy: 0.9608 - loss: 0.1056 - val_accuracy: 0.9603 - val_loss: 0.1104\n",
      "Epoch 10/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 15s/step - accuracy: 0.9674 - loss: 0.0907 - val_accuracy: 0.9658 - val_loss: 0.1017\n",
      "Epoch 11/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 15s/step - accuracy: 0.9711 - loss: 0.0781 - val_accuracy: 0.9663 - val_loss: 0.1017\n",
      "Epoch 12/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 14s/step - accuracy: 0.9747 - loss: 0.0656 - val_accuracy: 0.9676 - val_loss: 0.1036\n",
      "Epoch 13/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m231s\u001b[0m 14s/step - accuracy: 0.9745 - loss: 0.0679 - val_accuracy: 0.9735 - val_loss: 0.0817\n",
      "Epoch 14/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 14s/step - accuracy: 0.9791 - loss: 0.0537 - val_accuracy: 0.9673 - val_loss: 0.1001\n",
      "Epoch 15/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 14s/step - accuracy: 0.9777 - loss: 0.0593 - val_accuracy: 0.9738 - val_loss: 0.0904\n",
      "Epoch 16/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 14s/step - accuracy: 0.9818 - loss: 0.0467 - val_accuracy: 0.9735 - val_loss: 0.0888\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m244s\u001b[0m 14s/step - accuracy: 0.6585 - loss: 0.7385 - val_accuracy: 0.7330 - val_loss: 0.5313\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 14s/step - accuracy: 0.7387 - loss: 0.4853 - val_accuracy: 0.7330 - val_loss: 0.3856\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 14s/step - accuracy: 0.7491 - loss: 0.3525 - val_accuracy: 0.7330 - val_loss: 0.3298\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 14s/step - accuracy: 0.6579 - loss: 0.7182 - val_accuracy: 0.7323 - val_loss: 0.5278\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m228s\u001b[0m 14s/step - accuracy: 0.7458 - loss: 0.5015 - val_accuracy: 0.7323 - val_loss: 0.3937\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 14s/step - accuracy: 0.7465 - loss: 0.3685 - val_accuracy: 0.7323 - val_loss: 0.3394\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 14s/step - accuracy: 0.6623 - loss: 0.6168 - val_accuracy: 0.7427 - val_loss: 0.4296\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m227s\u001b[0m 14s/step - accuracy: 0.7379 - loss: 0.4000 - val_accuracy: 0.9194 - val_loss: 0.3054\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m226s\u001b[0m 14s/step - accuracy: 0.9049 - loss: 0.2703 - val_accuracy: 0.9274 - val_loss: 0.1828\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 14s/step - accuracy: 0.6685 - loss: 0.5905 - val_accuracy: 0.7446 - val_loss: 0.3948\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 14s/step - accuracy: 0.7428 - loss: 0.4079 - val_accuracy: 0.7537 - val_loss: 0.3534\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 14s/step - accuracy: 0.8394 - loss: 0.3210 - val_accuracy: 0.9257 - val_loss: 0.2554\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017C8B841BC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 14s/step - accuracy: 0.6779 - loss: 0.6063 - val_accuracy: 0.7673 - val_loss: 0.3891\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 14s/step - accuracy: 0.7490 - loss: 0.3841 - val_accuracy: 0.8656 - val_loss: 0.3036\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 14s/step - accuracy: 0.8930 - loss: 0.2952 - val_accuracy: 0.9375 - val_loss: 0.2397\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000017CFF1F1300> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 14s/step - accuracy: 0.6516 - loss: 0.6975 - val_accuracy: 0.7481 - val_loss: 0.5048\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m225s\u001b[0m 14s/step - accuracy: 0.7527 - loss: 0.4617 - val_accuracy: 0.7481 - val_loss: 0.3710\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 14s/step - accuracy: 0.7462 - loss: 0.3709 - val_accuracy: 0.8621 - val_loss: 0.3216\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m235s\u001b[0m 14s/step - accuracy: 0.7232 - loss: 0.7294 - val_accuracy: 0.7271 - val_loss: 0.5095\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 14s/step - accuracy: 0.7476 - loss: 0.4572 - val_accuracy: 0.7271 - val_loss: 0.3997\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 14s/step - accuracy: 0.7431 - loss: 0.3701 - val_accuracy: 0.7821 - val_loss: 0.3310\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 15s/step - accuracy: 0.6625 - loss: 0.6589 - val_accuracy: 0.7696 - val_loss: 0.4760\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m290s\u001b[0m 18s/step - accuracy: 0.7354 - loss: 0.4787 - val_accuracy: 0.7696 - val_loss: 0.3208\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m243s\u001b[0m 15s/step - accuracy: 0.7423 - loss: 0.3909 - val_accuracy: 0.7696 - val_loss: 0.3115\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m281s\u001b[0m 17s/step - accuracy: 0.7172 - loss: 0.6525 - val_accuracy: 0.7401 - val_loss: 0.5271\n",
      "Epoch 2/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m251s\u001b[0m 16s/step - accuracy: 0.7409 - loss: 0.4810 - val_accuracy: 0.7401 - val_loss: 0.3935\n",
      "Epoch 3/50\n",
      "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m246s\u001b[0m 15s/step - accuracy: 0.7454 - loss: 0.3689 - val_accuracy: 0.8830 - val_loss: 0.3180\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.7683\n",
      "Average Recall (Sensitivity): 0.0931\n",
      "Average Precision: 0.0968\n",
      "Average F1 Score: 0.0949\n",
      "Average Specificity: 0.9989\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Directories for Montgomery dataset images and masks\n",
    "image_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Montgomery_Dataset\\CXR_png'\n",
    "left_mask_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Montgomery_Dataset\\Masks_png\\leftMask'\n",
    "right_mask_dir = r'C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Montgomery_Dataset\\Masks_png\\rightMask'\n",
    "\n",
    "img_size = (256, 256)\n",
    "\n",
    "def load_images_and_masks(image_dir, left_mask_dir, right_mask_dir, img_size):\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    for img_name in os.listdir(image_dir):\n",
    "        # No need to append .png since the filenames already have it\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        left_mask_path = os.path.join(left_mask_dir, img_name)\n",
    "        right_mask_path = os.path.join(right_mask_dir, img_name)\n",
    "        \n",
    "        # Load image\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Ensure the image is loaded correctly\n",
    "        if img is None:\n",
    "            print(f\"Image not found: {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = img / 255.0  # Normalize image to range 0-1\n",
    "        \n",
    "        # Load left and right lung masks\n",
    "        left_mask = cv2.imread(left_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        right_mask = cv2.imread(right_mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        \n",
    "        # Ensure both masks are loaded correctly\n",
    "        if left_mask is None or right_mask is None:\n",
    "            print(f\"Mask not found: {left_mask_path} or {right_mask_path}\")\n",
    "            continue\n",
    "        \n",
    "        left_mask = cv2.resize(left_mask, img_size)\n",
    "        right_mask = cv2.resize(right_mask, img_size)\n",
    "        \n",
    "        # Combine left and right masks into a single mask\n",
    "        combined_mask = np.maximum(left_mask, right_mask)\n",
    "        combined_mask = combined_mask / 255.0  # Normalize mask to range 0-1\n",
    "        \n",
    "        # Append the image and the mask\n",
    "        images.append(np.expand_dims(img, axis=-1))  # Add channel dimension to the image\n",
    "        masks.append(np.expand_dims(combined_mask, axis=-1))  # Add channel dimension to the mask\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Load the images and combined masks\n",
    "images, masks = load_images_and_masks(image_dir, left_mask_dir, right_mask_dir, img_size)\n",
    "\n",
    "# Ensure data was loaded correctly\n",
    "if len(images) == 0 or len(masks) == 0:\n",
    "    raise ValueError(\"No images or masks were loaded. Check the dataset paths and file names.\")\n",
    "\n",
    "# U-Net Architecture\n",
    "def unet_model(input_size=(256, 256, 1)):\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    \n",
    "    # Contracting Path\n",
    "    c1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(c1)\n",
    "    p1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c1)\n",
    "    \n",
    "    c2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(c2)\n",
    "    p2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c2)\n",
    "    \n",
    "    c3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(c3)\n",
    "    p3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c3)\n",
    "    \n",
    "    # Bottleneck\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(b)\n",
    "    \n",
    "    # Expansive Path\n",
    "    u1 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(b)\n",
    "    u1 = tf.keras.layers.concatenate([u1, c3])\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(c4)\n",
    "    \n",
    "    u2 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(c4)\n",
    "    u2 = tf.keras.layers.concatenate([u2, c2])\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(c5)\n",
    "    \n",
    "    u3 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(c5)\n",
    "    u3 = tf.keras.layers.concatenate([u3, c1])\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(u3)\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(c6)\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(c6)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Early stopping callback to avoid overfitting\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Cross-validation setup\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "# Arrays to store performance metrics for each fold\n",
    "accuracy_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "f1_scores = []\n",
    "specificity_scores = []\n",
    "\n",
    "# Loop through each fold in the 10-fold cross-validation\n",
    "for train_index, test_index in kf.split(images):\n",
    "    X_train, X_test = images[train_index], images[test_index]\n",
    "    y_train, y_test = masks[train_index], masks[test_index]\n",
    "    \n",
    "    # Create the model for each fold\n",
    "    model = unet_model()\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=8, callbacks=[early_stopping], verbose=1)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = (y_pred > 0.5).astype(np.uint8)  # Ensure binary predictions\n",
    "    \n",
    "    y_test_binary = (y_test > 0.5).astype(np.uint8)  # Convert y_test to binary for comparison\n",
    "\n",
    "    # Calculate performance metrics for this fold\n",
    "    def calculate_metrics(y_true, y_pred):\n",
    "        y_true_flat = y_true.flatten()\n",
    "        y_pred_flat = y_pred.flatten()\n",
    "        \n",
    "        accuracy = accuracy_score(y_true_flat, y_pred_flat)\n",
    "        recall = recall_score(y_true_flat, y_pred_flat)\n",
    "        precision = precision_score(y_true_flat, y_pred_flat)\n",
    "        f1 = f1_score(y_true_flat, y_pred_flat)\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_true_flat, y_pred_flat).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        \n",
    "        return accuracy, recall, precision, f1, specificity\n",
    "\n",
    "    # Calculate metrics for this fold\n",
    "    accuracy, recall, precision, f1, specificity = calculate_metrics(y_test_binary, y_pred)\n",
    "    \n",
    "    # Append metrics to the respective arrays\n",
    "    accuracy_scores.append(accuracy)\n",
    "    recall_scores.append(recall)\n",
    "    precision_scores.append(precision)\n",
    "    f1_scores.append(f1)\n",
    "    specificity_scores.append(specificity)\n",
    "\n",
    "# Calculate the average performance across all 10 folds\n",
    "avg_accuracy = np.mean(accuracy_scores)\n",
    "avg_recall = np.mean(recall_scores)\n",
    "avg_precision = np.mean(precision_scores)\n",
    "avg_f1 = np.mean(f1_scores)\n",
    "avg_specificity = np.mean(specificity_scores)\n",
    "\n",
    "# Print the average performance metrics\n",
    "print(f'Average Accuracy: {avg_accuracy:.4f}')\n",
    "print(f'Average Recall (Sensitivity): {avg_recall:.4f}')\n",
    "print(f'Average Precision: {avg_precision:.4f}')\n",
    "print(f'Average F1 Score: {avg_f1:.4f}')\n",
    "print(f'Average Specificity: {avg_specificity:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b971695-5b30-4c16-a853-789d4296e068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
