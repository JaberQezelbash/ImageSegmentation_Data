{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f5abb7-3b22-4c9a-bc5f-1bb543debe3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask not found for train_mask, skipping this image.\n",
      "Mask not found for valid_mask, skipping this image.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Batch 1/299 ━━━━━━━━━━━━━━━━━━━━ 10:31:57\n",
      "Accuracy: 0.4738 - Precision: 0.0251 - Recall: 0.5223 - Specificity: 0.4725 - F1: 0.0479 - Loss: 0.1278\n",
      "\n",
      "Batch 2/299 ━━━━━━━━━━━━━━━━━━━━ 10:32:44\n",
      "Accuracy: 0.7232 - Precision: 0.0126 - Recall: 0.2612 - Specificity: 0.7363 - F1: 0.0240 - Loss: 0.1264\n",
      "\n",
      "Batch 3/299 ━━━━━━━━━━━━━━━━━━━━ 10:33:24\n",
      "Accuracy: 0.8032 - Precision: 0.0084 - Recall: 0.1741 - Specificity: 0.8242 - F1: 0.0160 - Loss: 0.1240\n",
      "\n",
      "Batch 4/299 ━━━━━━━━━━━━━━━━━━━━ 10:34:49\n",
      "Accuracy: 0.8452 - Precision: 0.0063 - Recall: 0.1306 - Specificity: 0.8681 - F1: 0.0120 - Loss: 0.1199\n",
      "\n",
      "Batch 5/299 ━━━━━━━━━━━━━━━━━━━━ 10:35:46\n",
      "Accuracy: 0.8701 - Precision: 0.0050 - Recall: 0.1045 - Specificity: 0.8945 - F1: 0.0096 - Loss: 0.1100\n",
      "\n",
      "Batch 6/299 ━━━━━━━━━━━━━━━━━━━━ 10:37:13\n",
      "Accuracy: 0.8885 - Precision: 0.0042 - Recall: 0.0871 - Specificity: 0.9121 - F1: 0.0080 - Loss: 0.0936\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime  # Import datetime for the custom callback\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Import EarlyStopping\n",
    "\n",
    "# Directory paths\n",
    "train_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\"\n",
    "train_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\\train_mask\"\n",
    "test_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\"\n",
    "test_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\\test_mask\"\n",
    "valid_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\"\n",
    "valid_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\\valid_mask\"\n",
    "\n",
    "# Load images and masks\n",
    "def load_images_and_masks(img_dir, mask_dir, img_size=(256, 256)):\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    img_files = os.listdir(img_dir)\n",
    "    for img_file in img_files:\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        # Adjust mask filename to include \"_mask.png\"\n",
    "        mask_file = img_file + \"_mask.png\"\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            # Load image as RGB\n",
    "            img = load_img(img_path, color_mode='rgb', target_size=img_size)\n",
    "            img = img_to_array(img) / 255.0\n",
    "            # Load mask as grayscale\n",
    "            mask = load_img(mask_path, color_mode='grayscale', target_size=img_size)\n",
    "            mask = img_to_array(mask) / 255.0\n",
    "\n",
    "            images.append(img)\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            print(f\"Mask not found for {img_file}, skipping this image.\")\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Load training and validation data\n",
    "X_train, y_train = load_images_and_masks(train_img_dir, train_mask_dir)\n",
    "X_valid, y_valid = load_images_and_masks(valid_img_dir, valid_mask_dir)\n",
    "\n",
    "# Capsule Layer with Dynamic Routing\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"Squashing function to ensure output vectors' lengths are between 0 and 1\"\"\"\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, num_routing=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Define the weight matrix W for transformation between input and capsule dimensions\n",
    "        self.W = self.add_weight(shape=[input_shape[-1], self.num_capsules * self.dim_capsule],\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs, [-1, inputs.shape[1] * inputs.shape[2], inputs.shape[3]])\n",
    "        u_hat = tf.einsum('...ij,jk->...ik', inputs, self.W)\n",
    "        u_hat = tf.reshape(u_hat, [-1, inputs.shape[1], self.num_capsules, self.dim_capsule])\n",
    "        \n",
    "        b = tf.zeros(shape=[tf.shape(inputs)[0], inputs.shape[1], self.num_capsules])\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(b, axis=-1)\n",
    "            s = tf.reduce_sum(c[..., tf.newaxis] * u_hat, axis=1)\n",
    "            v = squash(s)\n",
    "            if i < self.num_routing - 1:\n",
    "                b += tf.reduce_sum(u_hat * v[:, tf.newaxis, :, :], axis=-1)\n",
    "        return v\n",
    "\n",
    "# Attention Gate\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    theta_x = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(x)\n",
    "    phi_g = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(g)\n",
    "    concat_xg = tf.keras.layers.add([theta_x, phi_g])\n",
    "    act_xg = tf.keras.layers.Activation('relu')(concat_xg)\n",
    "    psi = tf.keras.layers.Conv2D(1, 1, padding='same')(act_xg)\n",
    "    sigmoid_xg = tf.keras.layers.Activation('sigmoid')(psi)\n",
    "    y = tf.keras.layers.multiply([x, sigmoid_xg])\n",
    "    return y\n",
    "\n",
    "# U-Net with Capsule Network Layers, Attention Mechanisms, and Sobel Edge Detection (from Code1)\n",
    "def unet_capsule_model(input_size=(256, 256, 3)):\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    \n",
    "    # Contracting Path with Capsules\n",
    "    c1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = CapsuleLayer(num_capsules=8, dim_capsule=16)(c1)\n",
    "    c1_flattened = tf.keras.layers.Flatten()(c1)\n",
    "    c1_reshaped = tf.keras.layers.Dense(256*256, activation='relu')(c1_flattened)\n",
    "    c1_reshaped = tf.keras.layers.Reshape((256, 256, 1))(c1_reshaped)\n",
    "    p1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c1_reshaped)\n",
    "    \n",
    "    c2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = CapsuleLayer(num_capsules=16, dim_capsule=32)(c2)\n",
    "    c2_flattened = tf.keras.layers.Flatten()(c2)\n",
    "    c2_reshaped = tf.keras.layers.Dense(128*128, activation='relu')(c2_flattened)\n",
    "    c2_reshaped = tf.keras.layers.Reshape((128, 128, 1))(c2_reshaped)\n",
    "    p2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c2_reshaped)\n",
    "    \n",
    "    c3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = CapsuleLayer(num_capsules=32, dim_capsule=64)(c3)\n",
    "    c3_flattened = tf.keras.layers.Flatten()(c3)\n",
    "    c3_reshaped = tf.keras.layers.Dense(64*64, activation='relu')(c3_flattened)\n",
    "    c3_reshaped = tf.keras.layers.Reshape((64, 64, 1))(c3_reshaped)\n",
    "    p3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c3_reshaped)\n",
    "    \n",
    "    # Bottleneck\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(b)\n",
    "    \n",
    "    # Expansive Path with Attention Gates\n",
    "    u1 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(b)\n",
    "    c3_attention = attention_gate(c3_reshaped, u1, inter_channels=128)\n",
    "    u1 = tf.keras.layers.concatenate([u1, c3_attention])\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(c4)\n",
    "    \n",
    "    u2 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(c4)\n",
    "    c2_attention = attention_gate(c2_reshaped, u2, inter_channels=64)\n",
    "    u2 = tf.keras.layers.concatenate([u2, c2_attention])\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(c5)\n",
    "    \n",
    "    u3 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(c5)\n",
    "    c1_attention = attention_gate(c1_reshaped, u3, inter_channels=32)\n",
    "    u3 = tf.keras.layers.concatenate([u3, c1_attention])\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(u3)\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(c6)\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(c6)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Define custom metrics\n",
    "def custom_precision(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    predicted_positives = K.sum(y_pred_bin)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    possible_positives = K.sum(y_true)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def custom_specificity(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_negatives = K.sum(K.round((1 - y_true) * (1 - y_pred_bin)))\n",
    "    possible_negatives = K.sum(1 - y_true)\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    precision = custom_precision(y_true, y_pred)\n",
    "    recall = custom_recall(y_true, y_pred)\n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "# Define Focal Loss\n",
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "    weight = alpha * y_true * K.pow((1 - y_pred), gamma) + (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma)\n",
    "    loss = weight * cross_entropy\n",
    "    return K.mean(loss)\n",
    "\n",
    "# Compile the model\n",
    "model = unet_capsule_model()\n",
    "model.compile(optimizer='adam', loss=focal_loss_fixed, metrics=['accuracy', custom_precision, custom_recall, custom_specificity, custom_f1])\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 16\n",
    "\n",
    "# Calculate total batches for training\n",
    "total_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "\n",
    "# Custom callback to print more metrics at each batch with epoch tracking\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches):\n",
    "        super().__init__()\n",
    "        self.batch_counter = 1  # Initialize the batch counter\n",
    "        self.total_batches = total_batches  # Total number of batches per epoch\n",
    "        self.current_epoch = 1  # Initialize current epoch\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch + 1  # Epochs are zero-indexed\n",
    "        print(f\"\\nEpoch {self.current_epoch}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        \n",
    "        # Time formatting for current step\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        # Print the metrics with proper formatting\n",
    "        print(f\"Batch {self.batch_counter}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Specificity: {specificity:.4f} - F1: {f1:.4f} - Loss: {loss:.4f}\\n\")\n",
    "        \n",
    "        # Increment batch counter\n",
    "        self.batch_counter += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Reset batch counter at the end of each epoch\n",
    "        self.batch_counter = 1\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Metric to monitor\n",
    "    patience=5,                # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "# Initialize the custom callback\n",
    "metrics_callback = MetricsCallback(total_batches=total_batches)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,               # Number of epochs\n",
    "    batch_size=batch_size,   # Batch size of 16\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[metrics_callback, early_stopping],  # Add early_stopping to the callbacks list\n",
    "    verbose=0                # Suppress default Keras logging\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('dental_xray_unet_capsule_model.h5')\n",
    "\n",
    "# Load test data\n",
    "X_test, y_test = load_images_and_masks(test_img_dir, test_mask_dir)\n",
    "\n",
    "# Evaluate on the training set\n",
    "y_train_pred = model.predict(X_train, batch_size=batch_size)\n",
    "y_train_pred_bin = (y_train_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Confusion Matrix for training\n",
    "conf_matrix_train = confusion_matrix(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_train, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Train\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_valid_pred = model.predict(X_valid, batch_size=batch_size)\n",
    "y_valid_pred_bin = (y_valid_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Confusion Matrix for validation\n",
    "conf_matrix_valid = confusion_matrix(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_valid, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Validation\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = model.predict(X_test, batch_size=batch_size)\n",
    "y_test_pred_bin = (y_test_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Confusion Matrix for testing\n",
    "conf_matrix_test = confusion_matrix(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Test\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Performance report for training set\n",
    "train_accuracy = accuracy_score(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "train_recall = recall_score(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "train_precision = precision_score(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "train_f1 = f1_score(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "train_tn, train_fp, train_fn, train_tp = confusion_matrix(y_train.flatten(), y_train_pred_bin.flatten()).ravel()\n",
    "train_specificity = train_tn / (train_tn + train_fp)\n",
    "\n",
    "print(f'Training Set Results:')\n",
    "print(f'Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Recall (Sensitivity): {train_recall:.4f}')\n",
    "print(f'Precision: {train_precision:.4f}')\n",
    "print(f'F1 Score: {train_f1:.4f}')\n",
    "print(f'Specificity: {train_specificity:.4f}')\n",
    "\n",
    "# Performance report for validation set\n",
    "valid_accuracy = accuracy_score(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "valid_recall = recall_score(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "valid_precision = precision_score(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "valid_f1 = f1_score(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "valid_tn, valid_fp, valid_fn, valid_tp = confusion_matrix(y_valid.flatten(), y_valid_pred_bin.flatten()).ravel()\n",
    "valid_specificity = valid_tn / (valid_tn + valid_fp)\n",
    "\n",
    "print(f'Validation Set Results:')\n",
    "print(f'Accuracy: {valid_accuracy:.4f}')\n",
    "print(f'Recall (Sensitivity): {valid_recall:.4f}')\n",
    "print(f'Precision: {valid_precision:.4f}')\n",
    "print(f'F1 Score: {valid_f1:.4f}')\n",
    "print(f'Specificity: {valid_specificity:.4f}')\n",
    "\n",
    "# Performance report for testing set\n",
    "test_accuracy = accuracy_score(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "test_recall = recall_score(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "test_precision = precision_score(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "test_f1 = f1_score(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "test_tn, test_fp, test_fn, test_tp = confusion_matrix(y_test.flatten(), y_test_pred_bin.flatten()).ravel()\n",
    "test_specificity = test_tn / (test_tn + test_fp)\n",
    "\n",
    "print(f'Testing Set Results:')\n",
    "print(f'Accuracy: {test_accuracy:.4f}')\n",
    "print(f'Recall (Sensitivity): {test_recall:.4f}')\n",
    "print(f'Precision: {test_precision:.4f}')\n",
    "print(f'F1 Score: {test_f1:.4f}')\n",
    "print(f'Specificity: {test_specificity:.4f}')\n",
    "\n",
    "# Visualization: Show input image, true mask, and predicted mask for a few samples\n",
    "def visualize_predictions(images, true_masks, pred_masks, title):\n",
    "    for i in range(3):  # Visualize first 3 predictions\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(true_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        \n",
    "        # Predicted mask\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        \n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions for training set\n",
    "visualize_predictions(X_train, y_train, y_train_pred_bin, \"Train Set Predictions\")\n",
    "\n",
    "# Visualize predictions for validation set\n",
    "visualize_predictions(X_valid, y_valid, y_valid_pred_bin, \"Validation Set Predictions\")\n",
    "\n",
    "# Visualize predictions for testing set\n",
    "visualize_predictions(X_test, y_test, y_test_pred_bin, \"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe384a-5b0b-4961-9952-8e6baca8a4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43ede41a-2a71-4201-ab11-dd860fcb22b2",
   "metadata": {},
   "source": [
    "## With Edge Detection techniques\n",
    "* Canny Edge Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231b0f2b-8cc3-4ee8-af7d-ec1accfd8365",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask not found for train_mask, skipping this image.\n",
      "Mask not found for valid_mask, skipping this image.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Batch 1/299 ━━━━━━━━━━━━━━━━━━━━ 11:14:52\n",
      "Accuracy: 0.7564 - Precision: 0.0250 - Recall: 0.2213 - Specificity: 0.7706 - F1: 0.0449 - Loss: 0.1277\n",
      "\n",
      "Batch 2/299 ━━━━━━━━━━━━━━━━━━━━ 11:16:18\n",
      "Accuracy: 0.8675 - Precision: 0.0125 - Recall: 0.1106 - Specificity: 0.8853 - F1: 0.0224 - Loss: 0.1263\n",
      "\n",
      "Batch 3/299 ━━━━━━━━━━━━━━━━━━━━ 11:17:51\n",
      "Accuracy: 0.9040 - Precision: 0.0083 - Recall: 0.0738 - Specificity: 0.9235 - F1: 0.0150 - Loss: 0.1237\n",
      "\n",
      "Batch 4/299 ━━━━━━━━━━━━━━━━━━━━ 11:19:03\n",
      "Accuracy: 0.9224 - Precision: 0.0062 - Recall: 0.0553 - Specificity: 0.9427 - F1: 0.0112 - Loss: 0.1189\n",
      "\n",
      "Batch 5/299 ━━━━━━━━━━━━━━━━━━━━ 11:20:26\n",
      "Accuracy: 0.9336 - Precision: 0.0050 - Recall: 0.0443 - Specificity: 0.9541 - F1: 0.0090 - Loss: 0.1066\n",
      "\n",
      "Batch 6/299 ━━━━━━━━━━━━━━━━━━━━ 11:21:38\n",
      "Accuracy: 0.9416 - Precision: 0.0042 - Recall: 0.0369 - Specificity: 0.9618 - F1: 0.0075 - Loss: 0.0919\n",
      "\n",
      "Batch 7/299 ━━━━━━━━━━━━━━━━━━━━ 11:23:00\n",
      "Accuracy: 0.9456 - Precision: 0.0036 - Recall: 0.0316 - Specificity: 0.9672 - F1: 0.0064 - Loss: 0.0868\n",
      "\n",
      "Batch 8/299 ━━━━━━━━━━━━━━━━━━━━ 11:24:26\n",
      "Accuracy: 0.9484 - Precision: 0.0031 - Recall: 0.0277 - Specificity: 0.9713 - F1: 0.0056 - Loss: 0.0785\n",
      "\n",
      "Batch 9/299 ━━━━━━━━━━━━━━━━━━━━ 11:25:40\n",
      "Accuracy: 0.9511 - Precision: 0.0028 - Recall: 0.0246 - Specificity: 0.9745 - F1: 0.0050 - Loss: 0.0711\n",
      "\n",
      "Batch 10/299 ━━━━━━━━━━━━━━━━━━━━ 11:26:54\n",
      "Accuracy: 0.9533 - Precision: 0.0025 - Recall: 0.0221 - Specificity: 0.9771 - F1: 0.0045 - Loss: 0.0660\n",
      "\n",
      "Batch 11/299 ━━━━━━━━━━━━━━━━━━━━ 11:28:11\n",
      "Accuracy: 0.9547 - Precision: 0.0023 - Recall: 0.0201 - Specificity: 0.9791 - F1: 0.0041 - Loss: 0.0618\n",
      "\n",
      "Batch 12/299 ━━━━━━━━━━━━━━━━━━━━ 11:30:14\n",
      "Accuracy: 0.9567 - Precision: 0.0021 - Recall: 0.0184 - Specificity: 0.9809 - F1: 0.0037 - Loss: 0.0577\n",
      "\n",
      "Batch 13/299 ━━━━━━━━━━━━━━━━━━━━ 11:31:57\n",
      "Accuracy: 0.9587 - Precision: 0.0019 - Recall: 0.0170 - Specificity: 0.9824 - F1: 0.0035 - Loss: 0.0539\n",
      "\n",
      "Batch 14/299 ━━━━━━━━━━━━━━━━━━━━ 11:33:19\n",
      "Accuracy: 0.9600 - Precision: 0.0018 - Recall: 0.0158 - Specificity: 0.9836 - F1: 0.0032 - Loss: 0.0509\n",
      "\n",
      "Batch 15/299 ━━━━━━━━━━━━━━━━━━━━ 11:34:36\n",
      "Accuracy: 0.9605 - Precision: 0.0017 - Recall: 0.0148 - Specificity: 0.9847 - F1: 0.0030 - Loss: 0.0490\n",
      "\n",
      "Batch 16/299 ━━━━━━━━━━━━━━━━━━━━ 11:35:41\n",
      "Accuracy: 0.9616 - Precision: 0.0016 - Recall: 0.0138 - Specificity: 0.9857 - F1: 0.0028 - Loss: 0.0466\n",
      "\n",
      "Batch 17/299 ━━━━━━━━━━━━━━━━━━━━ 11:37:15\n",
      "Accuracy: 0.9626 - Precision: 0.0015 - Recall: 0.0130 - Specificity: 0.9865 - F1: 0.0026 - Loss: 0.0445\n",
      "\n",
      "Batch 18/299 ━━━━━━━━━━━━━━━━━━━━ 11:38:32\n",
      "Accuracy: 0.9630 - Precision: 0.0014 - Recall: 0.0123 - Specificity: 0.9873 - F1: 0.0025 - Loss: 0.0428\n",
      "\n",
      "Batch 19/299 ━━━━━━━━━━━━━━━━━━━━ 11:39:40\n",
      "Accuracy: 0.9634 - Precision: 0.0013 - Recall: 0.0116 - Specificity: 0.9879 - F1: 0.0024 - Loss: 0.0412\n",
      "\n",
      "Batch 20/299 ━━━━━━━━━━━━━━━━━━━━ 11:40:27\n",
      "Accuracy: 0.9638 - Precision: 0.0012 - Recall: 0.0111 - Specificity: 0.9885 - F1: 0.0022 - Loss: 0.0397\n",
      "\n",
      "Batch 21/299 ━━━━━━━━━━━━━━━━━━━━ 11:41:26\n",
      "Accuracy: 0.9644 - Precision: 0.0012 - Recall: 0.0105 - Specificity: 0.9891 - F1: 0.0021 - Loss: 0.0383\n",
      "\n",
      "Batch 22/299 ━━━━━━━━━━━━━━━━━━━━ 11:42:21\n",
      "Accuracy: 0.9650 - Precision: 0.0011 - Recall: 0.0101 - Specificity: 0.9896 - F1: 0.0020 - Loss: 0.0371\n",
      "\n",
      "Batch 23/299 ━━━━━━━━━━━━━━━━━━━━ 11:43:11\n",
      "Accuracy: 0.9655 - Precision: 0.0011 - Recall: 0.0096 - Specificity: 0.9900 - F1: 0.0020 - Loss: 0.0359\n",
      "\n",
      "Batch 24/299 ━━━━━━━━━━━━━━━━━━━━ 11:44:17\n",
      "Accuracy: 0.9660 - Precision: 0.0010 - Recall: 0.0092 - Specificity: 0.9904 - F1: 0.0019 - Loss: 0.0348\n",
      "\n",
      "Batch 25/299 ━━━━━━━━━━━━━━━━━━━━ 11:46:08\n",
      "Accuracy: 0.9662 - Precision: 0.0010 - Recall: 0.0089 - Specificity: 0.9908 - F1: 0.0018 - Loss: 0.0338\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime  # Import datetime for the custom callback\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping  # Import EarlyStopping\n",
    "\n",
    "# Directory paths\n",
    "train_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\"\n",
    "train_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\\train_mask\"\n",
    "test_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\"\n",
    "test_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\\test_mask\"\n",
    "valid_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\"\n",
    "valid_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\\valid_mask\"\n",
    "\n",
    "# Load images and masks\n",
    "def load_images_and_masks(img_dir, mask_dir, img_size=(256, 256)):\n",
    "    images = []\n",
    "    masks = []\n",
    "    \n",
    "    img_files = os.listdir(img_dir)\n",
    "    for img_file in img_files:\n",
    "        img_path = os.path.join(img_dir, img_file)\n",
    "        # Adjust mask filename to include \"_mask.png\"\n",
    "        mask_file = img_file + \"_mask.png\"\n",
    "        mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "        if os.path.exists(mask_path):\n",
    "            # Load image as RGB\n",
    "            img = load_img(img_path, color_mode='rgb', target_size=img_size)\n",
    "            img = img_to_array(img) / 255.0\n",
    "            \n",
    "            # Load mask as grayscale\n",
    "            mask = load_img(mask_path, color_mode='grayscale', target_size=img_size)\n",
    "            mask = img_to_array(mask) / 255.0\n",
    "\n",
    "            # Convert image to grayscale for edge detection\n",
    "            img_gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "            \n",
    "            # Apply Canny Edge Detection\n",
    "            canny_edges = cv2.Canny(img_gray, threshold1=100, threshold2=200)\n",
    "            canny_edges = canny_edges / 255.0  # Normalize to range 0-1\n",
    "            \n",
    "            # Stack image with Canny edges as an additional channel\n",
    "            img_stack = np.stack((img_gray, canny_edges), axis=-1)\n",
    "\n",
    "            images.append(img_stack)\n",
    "            masks.append(mask)\n",
    "        else:\n",
    "            print(f\"Mask not found for {img_file}, skipping this image.\")\n",
    "    \n",
    "    return np.array(images), np.array(masks)\n",
    "\n",
    "# Load training and validation data\n",
    "X_train, y_train = load_images_and_masks(train_img_dir, train_mask_dir)\n",
    "X_valid, y_valid = load_images_and_masks(valid_img_dir, valid_mask_dir)\n",
    "\n",
    "# Capsule Layer with Dynamic Routing\n",
    "def squash(vectors, axis=-1):\n",
    "    \"\"\"Squashing function to ensure output vectors' lengths are between 0 and 1\"\"\"\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, num_routing=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Define the weight matrix W for transformation between input and capsule dimensions\n",
    "        self.W = self.add_weight(shape=[input_shape[-1], self.num_capsules * self.dim_capsule],\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs, [-1, inputs.shape[1] * inputs.shape[2], inputs.shape[3]])\n",
    "        u_hat = tf.einsum('...ij,jk->...ik', inputs, self.W)\n",
    "        u_hat = tf.reshape(u_hat, [-1, inputs.shape[1], self.num_capsules, self.dim_capsule])\n",
    "        \n",
    "        b = tf.zeros(shape=[tf.shape(inputs)[0], inputs.shape[1], self.num_capsules])\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(b, axis=-1)\n",
    "            s = tf.reduce_sum(c[..., tf.newaxis] * u_hat, axis=1)\n",
    "            v = squash(s)\n",
    "            if i < self.num_routing - 1:\n",
    "                b += tf.reduce_sum(u_hat * v[:, tf.newaxis, :, :], axis=-1)\n",
    "        return v\n",
    "\n",
    "# Attention Gate\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    theta_x = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(x)\n",
    "    phi_g = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(g)\n",
    "    concat_xg = tf.keras.layers.add([theta_x, phi_g])\n",
    "    act_xg = tf.keras.layers.Activation('relu')(concat_xg)\n",
    "    psi = tf.keras.layers.Conv2D(1, 1, padding='same')(act_xg)\n",
    "    sigmoid_xg = tf.keras.layers.Activation('sigmoid')(psi)\n",
    "    y = tf.keras.layers.multiply([x, sigmoid_xg])\n",
    "    return y\n",
    "\n",
    "# U-Net with Capsule Network Layers, Attention Mechanisms, and Canny Edge Detection\n",
    "def unet_capsule_model(input_size=(256, 256, 2)):\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    \n",
    "    # Contracting Path with Capsules\n",
    "    c1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = CapsuleLayer(num_capsules=8, dim_capsule=16)(c1)\n",
    "    c1_flattened = tf.keras.layers.Flatten()(c1)\n",
    "    c1_reshaped = tf.keras.layers.Dense(256*256, activation='relu')(c1_flattened)\n",
    "    c1_reshaped = tf.keras.layers.Reshape((256, 256, 1))(c1_reshaped)\n",
    "    p1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c1_reshaped)\n",
    "    \n",
    "    c2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = CapsuleLayer(num_capsules=16, dim_capsule=32)(c2)\n",
    "    c2_flattened = tf.keras.layers.Flatten()(c2)\n",
    "    c2_reshaped = tf.keras.layers.Dense(128*128, activation='relu')(c2_flattened)\n",
    "    c2_reshaped = tf.keras.layers.Reshape((128, 128, 1))(c2_reshaped)\n",
    "    p2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c2_reshaped)\n",
    "    \n",
    "    c3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = CapsuleLayer(num_capsules=32, dim_capsule=64)(c3)\n",
    "    c3_flattened = tf.keras.layers.Flatten()(c3)\n",
    "    c3_reshaped = tf.keras.layers.Dense(64*64, activation='relu')(c3_flattened)\n",
    "    c3_reshaped = tf.keras.layers.Reshape((64, 64, 1))(c3_reshaped)\n",
    "    p3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c3_reshaped)\n",
    "    \n",
    "    # Bottleneck\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(b)\n",
    "    \n",
    "    # Expansive Path with Attention Gates\n",
    "    u1 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(b)\n",
    "    c3_attention = attention_gate(c3_reshaped, u1, inter_channels=128)\n",
    "    u1 = tf.keras.layers.concatenate([u1, c3_attention])\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(c4)\n",
    "    \n",
    "    u2 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(c4)\n",
    "    c2_attention = attention_gate(c2_reshaped, u2, inter_channels=64)\n",
    "    u2 = tf.keras.layers.concatenate([u2, c2_attention])\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(c5)\n",
    "    \n",
    "    u3 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(c5)\n",
    "    c1_attention = attention_gate(c1_reshaped, u3, inter_channels=32)\n",
    "    u3 = tf.keras.layers.concatenate([u3, c1_attention])\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(u3)\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(c6)\n",
    "    \n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(c6)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Define custom metrics\n",
    "def custom_precision(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    predicted_positives = K.sum(y_pred_bin)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    possible_positives = K.sum(y_true)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def custom_specificity(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_negatives = K.sum(K.round((1 - y_true) * (1 - y_pred_bin)))\n",
    "    possible_negatives = K.sum(1 - y_true)\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    precision = custom_precision(y_true, y_pred)\n",
    "    recall = custom_recall(y_true, y_pred)\n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "# Define Focal Loss\n",
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "    weight = alpha * y_true * K.pow((1 - y_pred), gamma) + (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma)\n",
    "    loss = weight * cross_entropy\n",
    "    return K.mean(loss)\n",
    "\n",
    "# Compile the model\n",
    "model = unet_capsule_model()\n",
    "model.compile(optimizer='adam', loss=focal_loss_fixed, metrics=['accuracy', custom_precision, custom_recall, custom_specificity, custom_f1])\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 16\n",
    "\n",
    "# Calculate total batches for training\n",
    "total_batches = int(np.ceil(len(X_train) / batch_size))\n",
    "\n",
    "# Custom callback to print more metrics at each batch with epoch tracking\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches):\n",
    "        super().__init__()\n",
    "        self.batch_counter = 1  # Initialize the batch counter\n",
    "        self.total_batches = total_batches  # Total number of batches per epoch\n",
    "        self.current_epoch = 1  # Initialize current epoch\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch + 1  # Epochs are zero-indexed\n",
    "        print(f\"\\nEpoch {self.current_epoch}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        \n",
    "        # Time formatting for current step\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        \n",
    "        # Print the metrics with proper formatting\n",
    "        print(f\"Batch {self.batch_counter}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Specificity: {specificity:.4f} - F1: {f1:.4f} - Loss: {loss:.4f}\\n\")\n",
    "        \n",
    "        # Increment batch counter\n",
    "        self.batch_counter += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Reset batch counter at the end of each epoch\n",
    "        self.batch_counter = 1\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',        # Metric to monitor\n",
    "    patience=5,                # Number of epochs with no improvement after which training will be stopped\n",
    "    restore_best_weights=True  # Restore model weights from the epoch with the best value of the monitored metric\n",
    ")\n",
    "\n",
    "# Initialize the custom callback\n",
    "metrics_callback = MetricsCallback(total_batches=total_batches)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=20,               # Number of epochs\n",
    "    batch_size=batch_size,   # Batch size of 16\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    callbacks=[metrics_callback, early_stopping],  # Add early_stopping to the callbacks list\n",
    "    verbose=0                # Suppress default Keras logging\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('dental_xray_unet_capsule_model_with_canny.h5')\n",
    "\n",
    "# Load test data\n",
    "X_test, y_test = load_images_and_masks(test_img_dir, test_mask_dir)\n",
    "\n",
    "# Evaluate on the training set\n",
    "y_train_pred = model.predict(X_train, batch_size=batch_size)\n",
    "y_train_pred_bin = (y_train_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Confusion Matrix for training\n",
    "conf_matrix_train = confusion_matrix(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_train, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Train\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the validation set\n",
    "y_valid_pred = model.predict(X_valid, batch_size=batch_size)\n",
    "y_valid_pred_bin = (y_valid_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Confusion Matrix for validation\n",
    "conf_matrix_valid = confusion_matrix(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_valid, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Validation\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_test_pred = model.predict(X_test, batch_size=batch_size)\n",
    "y_test_pred_bin = (y_test_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Confusion Matrix for testing\n",
    "conf_matrix_test = confusion_matrix(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Test\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Performance report for training set\n",
    "train_accuracy = accuracy_score(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "train_recall = recall_score(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "train_precision = precision_score(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "train_f1 = f1_score(y_train.flatten(), y_train_pred_bin.flatten())\n",
    "train_tn, train_fp, train_fn, train_tp = confusion_matrix(y_train.flatten(), y_train_pred_bin.flatten()).ravel()\n",
    "train_specificity = train_tn / (train_tn + train_fp)\n",
    "\n",
    "print(f'Training Set Results:')\n",
    "print(f'Accuracy: {train_accuracy:.4f}')\n",
    "print(f'Recall (Sensitivity): {train_recall:.4f}')\n",
    "print(f'Precision: {train_precision:.4f}')\n",
    "print(f'F1 Score: {train_f1:.4f}')\n",
    "print(f'Specificity: {train_specificity:.4f}')\n",
    "\n",
    "# Performance report for validation set\n",
    "valid_accuracy = accuracy_score(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "valid_recall = recall_score(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "valid_precision = precision_score(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "valid_f1 = f1_score(y_valid.flatten(), y_valid_pred_bin.flatten())\n",
    "valid_tn, valid_fp, valid_fn, valid_tp = confusion_matrix(y_valid.flatten(), y_valid_pred_bin.flatten()).ravel()\n",
    "valid_specificity = valid_tn / (valid_tn + valid_fp)\n",
    "\n",
    "print(f'Validation Set Results:')\n",
    "print(f'Accuracy: {valid_accuracy:.4f}')\n",
    "print(f'Recall (Sensitivity): {valid_recall:.4f}')\n",
    "print(f'Precision: {valid_precision:.4f}')\n",
    "print(f'F1 Score: {valid_f1:.4f}')\n",
    "print(f'Specificity: {valid_specificity:.4f}')\n",
    "\n",
    "# Performance report for testing set\n",
    "test_accuracy = accuracy_score(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "test_recall = recall_score(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "test_precision = precision_score(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "test_f1 = f1_score(y_test.flatten(), y_test_pred_bin.flatten())\n",
    "test_tn, test_fp, test_fn, test_tp = confusion_matrix(y_test.flatten(), y_test_pred_bin.flatten()).ravel()\n",
    "test_specificity = test_tn / (test_tn + test_fp)\n",
    "\n",
    "print(f'Testing Set Results:')\n",
    "print(f'Accuracy: {test_accuracy:.4f}')\n",
    "print(f'Recall (Sensitivity): {test_recall:.4f}')\n",
    "print(f'Precision: {test_precision:.4f}')\n",
    "print(f'F1 Score: {test_f1:.4f}')\n",
    "print(f'Specificity: {test_specificity:.4f}')\n",
    "\n",
    "# Visualization: Show input image, true mask, and predicted mask for a few samples\n",
    "def visualize_predictions(images, true_masks, pred_masks, title):\n",
    "    for i in range(3):  # Visualize first 3 predictions\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(true_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        \n",
    "        # Predicted mask\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        \n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions for training set\n",
    "visualize_predictions(X_train, y_train, y_train_pred_bin, \"Train Set Predictions\")\n",
    "\n",
    "# Visualize predictions for validation set\n",
    "visualize_predictions(X_valid, y_valid, y_valid_pred_bin, \"Validation Set Predictions\")\n",
    "\n",
    "# Visualize predictions for testing set\n",
    "visualize_predictions(X_test, y_test, y_test_pred_bin, \"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f66993d-666f-4ea4-b181-92ae67f72269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84c1a578-1a4d-4c28-b0ec-01fce45a74b9",
   "metadata": {},
   "source": [
    "### Full code that has all visualizations etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a576e86-2747-4982-ba87-e9f7ccd2c64e",
   "metadata": {},
   "source": [
    "* The following code is full but it has an issue with data loading ONLY when I run it in my LapTop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225fa16-66e1-4382-8c05-7bba0535a4ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Batch 1/298 ━━━━━━━━━━━━━━━━━━━━ 15:33:07\n",
      "Accuracy: 0.8367 - Precision: 0.0262 - Recall: 0.1444 - Specificity: 0.8553 - F1: 0.0444 - Loss: 0.1276\n",
      "\n",
      "Batch 2/298 ━━━━━━━━━━━━━━━━━━━━ 15:34:15\n",
      "Accuracy: 0.9036 - Precision: 0.0131 - Recall: 0.0722 - Specificity: 0.9277 - F1: 0.0222 - Loss: 0.1255\n",
      "\n",
      "Batch 3/298 ━━━━━━━━━━━━━━━━━━━━ 15:35:04\n",
      "Accuracy: 0.9254 - Precision: 0.0087 - Recall: 0.0481 - Specificity: 0.9518 - F1: 0.0148 - Loss: 0.1216\n",
      "\n",
      "Batch 4/298 ━━━━━━━━━━━━━━━━━━━━ 15:35:51\n",
      "Accuracy: 0.9399 - Precision: 0.0065 - Recall: 0.0361 - Specificity: 0.9638 - F1: 0.0111 - Loss: 0.1102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Directory paths\n",
    "train_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\"\n",
    "train_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\\train_mask\"\n",
    "test_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\"\n",
    "test_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\\test_mask\"\n",
    "valid_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\"\n",
    "valid_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\\valid_mask\"\n",
    "\n",
    "# Image generator to load data in batches, skipping those without masks\n",
    "def image_generator(img_dir, mask_dir, batch_size, img_size=(256, 256)):\n",
    "    img_files = os.listdir(img_dir)\n",
    "    while True:\n",
    "        images = []\n",
    "        masks = []\n",
    "        for img_file in img_files:\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            # Adjust mask filename to include \"_mask.png\"\n",
    "            mask_file = img_file + \"_mask.png\"\n",
    "            mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "            # Check if the corresponding mask exists\n",
    "            if os.path.exists(mask_path):\n",
    "                # Load image and mask\n",
    "                img = load_img(img_path, color_mode='rgb', target_size=img_size)\n",
    "                img = img_to_array(img) / 255.0\n",
    "                mask = load_img(mask_path, color_mode='grayscale', target_size=img_size)\n",
    "                mask = img_to_array(mask) / 255.0\n",
    "\n",
    "                # Convert image to grayscale for edge detection\n",
    "                img_gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "                canny_edges = cv2.Canny(img_gray, threshold1=100, threshold2=200)\n",
    "                canny_edges = canny_edges / 255.0  # Normalize to range 0-1\n",
    "\n",
    "                # Stack image with Canny edges as an additional channel\n",
    "                img_stack = np.stack((img_gray, canny_edges), axis=-1)\n",
    "\n",
    "                images.append(img_stack)\n",
    "                masks.append(mask)\n",
    "\n",
    "            # When the batch reaches the desired size, yield the data\n",
    "            if len(images) == batch_size:\n",
    "                yield np.array(images), np.array(masks)\n",
    "                images = []\n",
    "                masks = []\n",
    "\n",
    "# Capsule Layer with Dynamic Routing\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, num_routing=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=[input_shape[-1], self.num_capsules * self.dim_capsule],\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs, [-1, inputs.shape[1] * inputs.shape[2], inputs.shape[3]])\n",
    "        u_hat = tf.einsum('...ij,jk->...ik', inputs, self.W)\n",
    "        u_hat = tf.reshape(u_hat, [-1, inputs.shape[1], self.num_capsules, self.dim_capsule])\n",
    "        b = tf.zeros(shape=[tf.shape(inputs)[0], inputs.shape[1], self.num_capsules])\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(b, axis=-1)\n",
    "            s = tf.reduce_sum(c[..., tf.newaxis] * u_hat, axis=1)\n",
    "            v = squash(s)\n",
    "            if i < self.num_routing - 1:\n",
    "                b += tf.reduce_sum(u_hat * v[:, tf.newaxis, :, :], axis=-1)\n",
    "        return v\n",
    "\n",
    "# Attention Gate\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    theta_x = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(x)\n",
    "    phi_g = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(g)\n",
    "    concat_xg = tf.keras.layers.add([theta_x, phi_g])\n",
    "    act_xg = tf.keras.layers.Activation('relu')(concat_xg)\n",
    "    psi = tf.keras.layers.Conv2D(1, 1, padding='same')(act_xg)\n",
    "    sigmoid_xg = tf.keras.layers.Activation('sigmoid')(psi)\n",
    "    y = tf.keras.layers.multiply([x, sigmoid_xg])\n",
    "    return y\n",
    "\n",
    "# U-Net with Capsule Network Layers, Attention Mechanisms, and Canny Edge Detection\n",
    "def unet_capsule_model(input_size=(256, 256, 2)):\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    c1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = CapsuleLayer(num_capsules=8, dim_capsule=16)(c1)\n",
    "    c1_flattened = tf.keras.layers.Flatten()(c1)\n",
    "    c1_reshaped = tf.keras.layers.Dense(256*256, activation='relu')(c1_flattened)\n",
    "    c1_reshaped = tf.keras.layers.Reshape((256, 256, 1))(c1_reshaped)\n",
    "    p1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c1_reshaped)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = CapsuleLayer(num_capsules=16, dim_capsule=32)(c2)\n",
    "    c2_flattened = tf.keras.layers.Flatten()(c2)\n",
    "    c2_reshaped = tf.keras.layers.Dense(128*128, activation='relu')(c2_flattened)\n",
    "    c2_reshaped = tf.keras.layers.Reshape((128, 128, 1))(c2_reshaped)\n",
    "    p2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c2_reshaped)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = CapsuleLayer(num_capsules=32, dim_capsule=64)(c3)\n",
    "    c3_flattened = tf.keras.layers.Flatten()(c3)\n",
    "    c3_reshaped = tf.keras.layers.Dense(64*64, activation='relu')(c3_flattened)\n",
    "    c3_reshaped = tf.keras.layers.Reshape((64, 64, 1))(c3_reshaped)\n",
    "    p3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c3_reshaped)\n",
    "\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(b)\n",
    "\n",
    "    u1 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(b)\n",
    "    c3_attention = attention_gate(c3_reshaped, u1, inter_channels=128)\n",
    "    u1 = tf.keras.layers.concatenate([u1, c3_attention])\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(c4)\n",
    "    c2_attention = attention_gate(c2_reshaped, u2, inter_channels=64)\n",
    "    u2 = tf.keras.layers.concatenate([u2, c2_attention])\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(c5)\n",
    "\n",
    "    u3 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(c5)\n",
    "    c1_attention = attention_gate(c1_reshaped, u3, inter_channels=32)\n",
    "    u3 = tf.keras.layers.concatenate([u3, c1_attention])\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(u3)\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(c6)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(c6)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Custom metrics\n",
    "def custom_precision(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    predicted_positives = K.sum(y_pred_bin)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    possible_positives = K.sum(y_true)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def custom_specificity(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_negatives = K.sum(K.round((1 - y_true) * (1 - y_pred_bin)))\n",
    "    possible_negatives = K.sum(1 - y_true)\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    precision = custom_precision(y_true, y_pred)\n",
    "    recall = custom_recall(y_true, y_pred)\n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "# Focal Loss\n",
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "    weight = alpha * y_true * K.pow((1 - y_pred), gamma) + (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma)\n",
    "    loss = weight * cross_entropy\n",
    "    return K.mean(loss)\n",
    "\n",
    "# Compile the model\n",
    "model = unet_capsule_model()\n",
    "model.compile(optimizer='adam', loss=focal_loss_fixed, metrics=['accuracy', custom_precision, custom_recall, custom_specificity, custom_f1])\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 16\n",
    "\n",
    "# Create data generators\n",
    "train_gen = image_generator(train_img_dir, train_mask_dir, batch_size)\n",
    "valid_gen = image_generator(valid_img_dir, valid_mask_dir, batch_size)\n",
    "test_gen = image_generator(test_img_dir, test_mask_dir, batch_size)\n",
    "\n",
    "# Number of steps per epoch\n",
    "steps_per_epoch = len(os.listdir(train_img_dir)) // batch_size\n",
    "validation_steps = len(os.listdir(valid_img_dir)) // batch_size\n",
    "test_steps = len(os.listdir(test_img_dir)) // batch_size\n",
    "\n",
    "# Custom callback to print more metrics at each batch and epoch for training, validation, and test sets\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches, X_valid, y_valid, X_test, y_test):\n",
    "        super().__init__()\n",
    "        self.batch_counter = 1\n",
    "        self.total_batches = total_batches\n",
    "        self.current_epoch = 1\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch + 1\n",
    "        print(f\"\\nEpoch {self.current_epoch}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Batch {self.batch_counter}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Specificity: {specificity:.4f} - F1: {f1:.4f} - Loss: {loss:.4f}\\n\")\n",
    "        self.batch_counter += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        val_accuracy = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        val_precision = logs.get('val_custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        val_recall = logs.get('val_custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        val_f1 = logs.get('val_custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        val_specificity = logs.get('val_custom_specificity', 0)\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}\")\n",
    "        print(f\"Train - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Specificity: {specificity:.4f}, F1: {f1:.4f}, Loss: {loss:.4f}\")\n",
    "        print(f\"Validation - Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, Specificity: {val_specificity:.4f}, F1: {val_f1:.4f}, Loss: {val_loss:.4f}\\n\")\n",
    "        test_loss, test_accuracy, test_precision, test_recall, test_specificity, test_f1 = self.model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        print(f\"Test Set Results - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, Specificity: {test_specificity:.4f}, F1: {test_f1:.4f}, Loss: {test_loss:.4f}\\n\")\n",
    "        self.batch_counter = 1\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize the custom callback with validation and test data\n",
    "metrics_callback = MetricsCallback(total_batches=steps_per_epoch, X_valid=valid_gen, y_valid=valid_gen, X_test=test_gen, y_test=test_gen)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=20,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[metrics_callback, early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('dental_xray_unet_capsule_model_with_canny.h5')\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_gen_full = image_generator(train_img_dir, train_mask_dir, batch_size)\n",
    "y_train_pred = model.predict(train_gen_full, steps=steps_per_epoch)\n",
    "y_train_pred_bin = (y_train_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Confusion Matrix for training\n",
    "conf_matrix_train = confusion_matrix(y_train_pred_bin.flatten(), y_train_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_train, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Train\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for validation set\n",
    "y_valid_pred = model.predict(valid_gen, steps=validation_steps)\n",
    "y_valid_pred_bin = (y_valid_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "conf_matrix_valid = confusion_matrix(y_valid_pred_bin.flatten(), y_valid_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_valid, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Validation\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for test set\n",
    "y_test_pred = model.predict(test_gen, steps=test_steps)\n",
    "y_test_pred_bin = (y_test_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "conf_matrix_test = confusion_matrix(y_test_pred_bin.flatten(), y_test_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Test\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Show input image, true mask, and predicted mask for a few samples\n",
    "def visualize_predictions(images, true_masks, pred_masks, title):\n",
    "    for i in range(3):  # Visualize first 3 predictions\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(true_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions for training set\n",
    "visualize_predictions(X_train, y_train, y_train_pred_bin, \"Train Set Predictions\")\n",
    "\n",
    "# Visualize predictions for validation set\n",
    "visualize_predictions(X_valid, y_valid, y_valid_pred_bin, \"Validation Set Predictions\")\n",
    "\n",
    "# Visualize predictions for test set\n",
    "visualize_predictions(X_test, y_test, y_test_pred_bin, \"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf73e19d-b763-4fa5-8c31-dd7ec47090c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "75fa6ed0-08f5-4cd8-9c3a-c5562146db0c",
   "metadata": {},
   "source": [
    "* The issue of data loading for some mask files in LapTop is solved in the following code."
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
<<<<<<< HEAD
   "execution_count": 5,
=======
   "execution_count": null,
>>>>>>> a9b1774f (update before helene storm!)
=======
   "execution_count": 5,
>>>>>>> 09cd13ea (add)
   "id": "e7c72b34-db26-4794-a018-f67f32bb1331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
<<<<<<< HEAD
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 258\u001b[0m\n\u001b[0;32m    255\u001b[0m metrics_callback \u001b[38;5;241m=\u001b[39m MetricsCallback(total_batches\u001b[38;5;241m=\u001b[39msteps_per_epoch, X_valid\u001b[38;5;241m=\u001b[39mvalid_gen, y_valid\u001b[38;5;241m=\u001b[39mvalid_gen, X_test\u001b[38;5;241m=\u001b[39mtest_gen, y_test\u001b[38;5;241m=\u001b[39mtest_gen)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    259\u001b[0m     train_gen,\n\u001b[0;32m    260\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m    261\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m    262\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalid_gen,\n\u001b[0;32m    263\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39mvalidation_steps,\n\u001b[0;32m    264\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[metrics_callback, early_stopping],\n\u001b[0;32m    265\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    266\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m    269\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdental_xray_unet_capsule_model_with_canny.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 889\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwds, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[0;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    698\u001b[0m )\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[0;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[0;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[0;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m api\u001b[38;5;241m.\u001b[39mconverted_call(\n\u001b[0;32m     42\u001b[0m       original_func,\n\u001b[0;32m     43\u001b[0m       args,\n\u001b[0;32m     44\u001b[0m       kwargs,\n\u001b[0;32m     45\u001b[0m       options\u001b[38;5;241m=\u001b[39mconverter\u001b[38;5;241m.\u001b[39mConversionOptions(\n\u001b[0;32m     46\u001b[0m           recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m           optional_features\u001b[38;5;241m=\u001b[39mautograph_options,\n\u001b[0;32m     48\u001b[0m           user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       ))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[0;32m    338\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph artifact\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools\u001b[38;5;241m.\u001b[39mpartial):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:121\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    122\u001b[0m     one_step_on_data, args\u001b[38;5;241m=\u001b[39m(data,)\n\u001b[0;32m    123\u001b[0m )\n\u001b[0;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    125\u001b[0m     outputs,\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    127\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    128\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m   1669\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   1672\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended\u001b[38;5;241m.\u001b[39mcall_for_each_replica(fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3261\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   3262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   4059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   4060\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m-> 4061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 889\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwds, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[0;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    698\u001b[0m )\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[0;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[0;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[0;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m api\u001b[38;5;241m.\u001b[39mconverted_call(\n\u001b[0;32m     42\u001b[0m       original_func,\n\u001b[0;32m     43\u001b[0m       args,\n\u001b[0;32m     44\u001b[0m       kwargs,\n\u001b[0;32m     45\u001b[0m       options\u001b[38;5;241m=\u001b[39mconverter\u001b[38;5;241m.\u001b[39mConversionOptions(\n\u001b[0;32m     46\u001b[0m           recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m           optional_features\u001b[38;5;241m=\u001b[39mautograph_options,\n\u001b[0;32m     48\u001b[0m           user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       ))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[0;32m    338\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph artifact\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools\u001b[38;5;241m.\u001b[39mpartial):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:108\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:51\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_has_training_arg:\n\u001b[1;32m---> 51\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:882\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    880\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 882\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m         call_fn,\n\u001b[0;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:175\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m             x\u001b[38;5;241m.\u001b[39m_keras_mask \u001b[38;5;241m=\u001b[39m mask\n\u001b[1;32m--> 175\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_through_graph(\n\u001b[0;32m    176\u001b[0m     inputs, operation_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m op: operation_fn(op, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m    177\u001b[0m )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\function.py:171\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[1;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[0;32m    169\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:556\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_has_training_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m operation\u001b[38;5;241m.\u001b[39m_call_has_training_arg\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    554\u001b[0m ):\n\u001b[0;32m    555\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[1;32m--> 556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m operation(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:882\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    880\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 882\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m         call_fn,\n\u001b[0;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 81\u001b[0m, in \u001b[0;36mCapsuleLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     79\u001b[0m c \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(b, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m s \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(c[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, tf\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m u_hat, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m v \u001b[38;5;241m=\u001b[39m squash(s)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_routing \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     83\u001b[0m     b \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(u_hat \u001b[38;5;241m*\u001b[39m v[:, tf\u001b[38;5;241m.\u001b[39mnewaxis, :, :], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36msquash\u001b[1;34m(vectors, axis)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquash\u001b[39m(vectors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 58\u001b[0m     s_squared_norm \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39msquare(vectors), axis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m     scale \u001b[38;5;241m=\u001b[39m s_squared_norm \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m s_squared_norm) \u001b[38;5;241m/\u001b[39m tf\u001b[38;5;241m.\u001b[39msqrt(s_squared_norm \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mepsilon())\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scale \u001b[38;5;241m*\u001b[39m vectors\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2215\u001b[0m, in \u001b[0;36mreduce_sum\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.reduce_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduce_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m   2153\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_sum\u001b[39m(input_tensor, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the sum of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[0;32m   2156\u001b[0m \n\u001b[0;32m   2157\u001b[0m \u001b[38;5;124;03m  This is the reduction operation for the elementwise `tf.math.add` op.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[38;5;124;03m  @end_compatibility\u001b[39;00m\n\u001b[0;32m   2213\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0;32m   2216\u001b[0m                               _ReductionDims(input_tensor, axis))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2227\u001b[0m, in \u001b[0;36mreduce_sum_with_dims\u001b[1;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_sum_with_dims\u001b[39m(input_tensor,\n\u001b[0;32m   2220\u001b[0m                          axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2221\u001b[0m                          keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2222\u001b[0m                          name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2223\u001b[0m                          dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2224\u001b[0m   keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(keepdims)\n\u001b[0;32m   2225\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _may_reduce_to_scalar(\n\u001b[0;32m   2226\u001b[0m       keepdims, axis,\n\u001b[1;32m-> 2227\u001b[0m       gen_math_ops\u001b[38;5;241m.\u001b[39m_sum(input_tensor, dims, keepdims, name\u001b[38;5;241m=\u001b[39mname))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:13763\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[0;32m  13761\u001b[0m   keep_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m  13762\u001b[0m keep_dims \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39mmake_bool(keep_dims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m> 13763\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m _op_def_library\u001b[38;5;241m.\u001b[39m_apply_op_helper(\n\u001b[0;32m  13764\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, reduction_indices\u001b[38;5;241m=\u001b[39maxis, keep_dims\u001b[38;5;241m=\u001b[39mkeep_dims,\n\u001b[0;32m  13765\u001b[0m              name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m  13766\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[0;32m  13767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:778\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m g\u001b[38;5;241m.\u001b[39mas_default(), ops\u001b[38;5;241m.\u001b[39mname_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[0;32m    777\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fallback:\n\u001b[1;32m--> 778\u001b[0m     _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n\u001b[0;32m    779\u001b[0m                            keywords, default_type_attr_map, attrs, inputs,\n\u001b[0;32m    780\u001b[0m                            input_types)\n\u001b[0;32m    781\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[0;32m    782\u001b[0m                            default_type_attr_map, attrs)\n\u001b[0;32m    783\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:531\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[1;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[0;32m    529\u001b[0m inferred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 531\u001b[0m   inferred \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m    532\u001b[0m       values, name\u001b[38;5;241m=\u001b[39minput_arg\u001b[38;5;241m.\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39minput_arg\u001b[38;5;241m.\u001b[39mis_ref)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    534\u001b[0m   \u001b[38;5;66;03m# When converting a python object such as a list of Dimensions, we\u001b[39;00m\n\u001b[0;32m    535\u001b[0m   \u001b[38;5;66;03m# need a dtype to be specified, thus tensor conversion may throw\u001b[39;00m\n\u001b[0;32m    536\u001b[0m   \u001b[38;5;66;03m# an exception which we will ignore and try again below.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[0;32m    714\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[0;32m    715\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m conversion_func(value, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39mas_ref)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m constant_op\u001b[38;5;241m.\u001b[39mconstant(v, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    277\u001b[0m                         allow_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:291\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    293\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:281\u001b[0m, in \u001b[0;36m_create_graph_constant\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    279\u001b[0m dtype_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtensor_value\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    280\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensor_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value}\n\u001b[1;32m--> 281\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39m_create_op_internal(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, [], [dtype_value\u001b[38;5;241m.\u001b[39mtype], attrs\u001b[38;5;241m=\u001b[39mattrs, name\u001b[38;5;241m=\u001b[39mname)\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m    285\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m    286\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m   callback_outputs \u001b[38;5;241m=\u001b[39m op_callbacks\u001b[38;5;241m.\u001b[39minvoke_op_callbacks(\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[38;5;241m=\u001b[39mname, graph\u001b[38;5;241m=\u001b[39mg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_create_op_internal(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2682\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2682\u001b[0m   ret \u001b[38;5;241m=\u001b[39m Operation\u001b[38;5;241m.\u001b[39mfrom_node_def(\n\u001b[0;32m   2683\u001b[0m       node_def,\n\u001b[0;32m   2684\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2685\u001b[0m       inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   2686\u001b[0m       output_types\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   2687\u001b[0m       control_inputs\u001b[38;5;241m=\u001b[39mcontrol_inputs,\n\u001b[0;32m   2688\u001b[0m       input_types\u001b[38;5;241m=\u001b[39minput_types,\n\u001b[0;32m   2689\u001b[0m       original_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_original_op,\n\u001b[0;32m   2690\u001b[0m       op_def\u001b[38;5;241m=\u001b[39mop_def,\n\u001b[0;32m   2691\u001b[0m   )\n\u001b[0;32m   2692\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1177\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1177\u001b[0m c_op \u001b[38;5;241m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1007\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39m_c_graph\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mas\u001b[39;00m c_graph:\n\u001b[1;32m-> 1007\u001b[0m   op_desc \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_NewOperation(c_graph,\n\u001b[0;32m   1008\u001b[0m                                               compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mop),\n\u001b[0;32m   1009\u001b[0m                                               compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_def\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m   1011\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
=======
      "WARNING:tensorflow:From C:\\Users\\Jaber\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Batch 1/298 ━━━━━━━━━━━━━━━━━━━━ 15:55:28\n",
      "Accuracy: 0.9679 - Precision: 0.0300 - Recall: 0.0058 - Specificity: 0.9948 - F1: 0.0097 - Loss: 0.1274\n",
      "\n",
      "Batch 2/298 ━━━━━━━━━━━━━━━━━━━━ 15:56:36\n",
      "Accuracy: 0.9762 - Precision: 0.0150 - Recall: 0.0029 - Specificity: 0.9974 - F1: 0.0049 - Loss: 0.1259\n",
      "\n",
      "Batch 3/298 ━━━━━━━━━━━━━━━━━━━━ 15:57:29\n",
      "Accuracy: 0.9778 - Precision: 0.0100 - Recall: 0.0019 - Specificity: 0.9983 - F1: 0.0032 - Loss: 0.1222\n",
      "\n",
      "Batch 4/298 ━━━━━━━━━━━━━━━━━━━━ 15:58:40\n",
      "Accuracy: 0.9790 - Precision: 0.0075 - Recall: 0.0014 - Specificity: 0.9987 - F1: 0.0024 - Loss: 0.1132\n",
      "\n",
      "Batch 5/298 ━━━━━━━━━━━━━━━━━━━━ 16:00:30\n",
      "Accuracy: 0.9800 - Precision: 0.0060 - Recall: 0.0012 - Specificity: 0.9990 - F1: 0.0019 - Loss: 0.0926\n",
      "\n",
      "Batch 6/298 ━━━━━━━━━━━━━━━━━━━━ 16:01:33\n",
      "Accuracy: 0.9805 - Precision: 0.0050 - Recall: 0.0010 - Specificity: 0.9991 - F1: 0.0016 - Loss: 0.0868\n",
      "\n",
      "Batch 7/298 ━━━━━━━━━━━━━━━━━━━━ 16:02:27\n",
      "Accuracy: 0.9812 - Precision: 0.0043 - Recall: 0.0008 - Specificity: 0.9993 - F1: 0.0014 - Loss: 0.0782\n",
      "\n",
      "Batch 8/298 ━━━━━━━━━━━━━━━━━━━━ 16:03:35\n",
      "Accuracy: 0.9814 - Precision: 0.0038 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0012 - Loss: 0.0698\n",
      "\n",
      "Batch 9/298 ━━━━━━━━━━━━━━━━━━━━ 16:04:38\n",
      "Accuracy: 0.9815 - Precision: 0.0033 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0011 - Loss: 0.0629\n",
      "\n",
      "Batch 10/298 ━━━━━━━━━━━━━━━━━━━━ 16:05:30\n",
      "Accuracy: 0.9818 - Precision: 0.0030 - Recall: 0.0006 - Specificity: 0.9995 - F1: 0.0010 - Loss: 0.0580\n",
      "\n",
      "Batch 11/298 ━━━━━━━━━━━━━━━━━━━━ 16:06:44\n",
      "Accuracy: 0.9820 - Precision: 0.0027 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0009 - Loss: 0.0538\n",
      "\n",
      "Batch 12/298 ━━━━━━━━━━━━━━━━━━━━ 16:07:41\n",
      "Accuracy: 0.9821 - Precision: 0.0025 - Recall: 0.0005 - Specificity: 0.9996 - F1: 0.0008 - Loss: 0.0500\n",
      "\n",
      "Batch 13/298 ━━━━━━━━━━━━━━━━━━━━ 16:08:43\n",
      "Accuracy: 0.9818 - Precision: 0.0023 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0007 - Loss: 0.0468\n",
      "\n",
      "Batch 14/298 ━━━━━━━━━━━━━━━━━━━━ 16:09:47\n",
      "Accuracy: 0.9817 - Precision: 0.0021 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0007 - Loss: 0.0441\n",
      "\n",
      "Batch 15/298 ━━━━━━━━━━━━━━━━━━━━ 16:10:52\n",
      "Accuracy: 0.9818 - Precision: 0.0020 - Recall: 0.0004 - Specificity: 0.9997 - F1: 0.0006 - Loss: 0.0417\n",
      "\n",
      "Batch 16/298 ━━━━━━━━━━━━━━━━━━━━ 16:11:51\n",
      "Accuracy: 0.9820 - Precision: 0.0019 - Recall: 0.0004 - Specificity: 0.9997 - F1: 0.0006 - Loss: 0.0396\n",
      "\n",
      "Batch 17/298 ━━━━━━━━━━━━━━━━━━━━ 16:12:45\n",
      "Accuracy: 0.9816 - Precision: 0.0018 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0006 - Loss: 0.0379\n",
      "\n",
      "Batch 18/298 ━━━━━━━━━━━━━━━━━━━━ 16:13:40\n",
      "Accuracy: 0.9817 - Precision: 0.0017 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0005 - Loss: 0.0362\n",
      "\n",
      "Batch 19/298 ━━━━━━━━━━━━━━━━━━━━ 16:14:34\n",
      "Accuracy: 0.9817 - Precision: 0.0016 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0005 - Loss: 0.0346\n",
      "\n",
      "Batch 20/298 ━━━━━━━━━━━━━━━━━━━━ 16:15:26\n",
      "Accuracy: 0.9818 - Precision: 0.0015 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0005 - Loss: 0.0332\n",
      "\n",
      "Batch 21/298 ━━━━━━━━━━━━━━━━━━━━ 16:16:24\n",
      "Accuracy: 0.9817 - Precision: 0.0014 - Recall: 0.0003 - Specificity: 0.9998 - F1: 0.0005 - Loss: 0.0320\n",
      "\n",
      "Batch 22/298 ━━━━━━━━━━━━━━━━━━━━ 16:17:17\n",
      "Accuracy: 0.9817 - Precision: 0.0014 - Recall: 0.0003 - Specificity: 0.9998 - F1: 0.0004 - Loss: 0.0308\n",
      "\n",
      "Batch 23/298 ━━━━━━━━━━━━━━━━━━━━ 16:18:12\n",
      "Accuracy: 0.9815 - Precision: 0.0013 - Recall: 0.0003 - Specificity: 0.9998 - F1: 0.0004 - Loss: 0.0298\n",
      "\n",
      "Batch 24/298 ━━━━━━━━━━━━━━━━━━━━ 16:19:04\n",
      "Accuracy: 0.9815 - Precision: 0.0013 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0004 - Loss: 0.0288\n",
      "\n",
      "Batch 25/298 ━━━━━━━━━━━━━━━━━━━━ 16:19:58\n",
      "Accuracy: 0.9815 - Precision: 0.0012 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0004 - Loss: 0.0280\n",
      "\n",
      "Batch 26/298 ━━━━━━━━━━━━━━━━━━━━ 16:20:51\n",
      "Accuracy: 0.9814 - Precision: 0.0012 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0004 - Loss: 0.0271\n",
      "\n",
      "Batch 27/298 ━━━━━━━━━━━━━━━━━━━━ 16:21:44\n",
      "Accuracy: 0.9814 - Precision: 0.0011 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0004 - Loss: 0.0263\n",
      "\n",
      "Batch 28/298 ━━━━━━━━━━━━━━━━━━━━ 16:22:37\n",
      "Accuracy: 0.9814 - Precision: 0.0011 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0003 - Loss: 0.0256\n",
      "\n",
      "Batch 29/298 ━━━━━━━━━━━━━━━━━━━━ 16:23:28\n",
      "Accuracy: 0.9813 - Precision: 0.0010 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0003 - Loss: 0.0249\n",
      "\n",
      "Batch 30/298 ━━━━━━━━━━━━━━━━━━━━ 16:24:23\n",
      "Accuracy: 0.9812 - Precision: 0.0010 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0003 - Loss: 0.0243\n",
      "\n",
      "Batch 31/298 ━━━━━━━━━━━━━━━━━━━━ 16:25:14\n",
      "Accuracy: 0.9811 - Precision: 0.0010 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0003 - Loss: 0.0238\n",
      "\n",
      "Batch 32/298 ━━━━━━━━━━━━━━━━━━━━ 16:26:07\n",
      "Accuracy: 0.9810 - Precision: 0.0009 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0003 - Loss: 0.0233\n",
      "\n",
      "Batch 33/298 ━━━━━━━━━━━━━━━━━━━━ 16:26:57\n",
      "Accuracy: 0.9808 - Precision: 0.0009 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0003 - Loss: 0.0228\n",
      "\n",
      "Batch 34/298 ━━━━━━━━━━━━━━━━━━━━ 16:27:54\n",
      "Accuracy: 0.9806 - Precision: 0.0009 - Recall: 0.0002 - Specificity: 0.9998 - F1: 0.0003 - Loss: 0.0223\n",
      "\n",
      "Batch 35/298 ━━━━━━━━━━━━━━━━━━━━ 16:28:47\n",
      "Accuracy: 0.9806 - Precision: 0.0009 - Recall: 0.0002 - Specificity: 0.9999 - F1: 0.0003 - Loss: 0.0219\n",
      "\n",
      "Batch 36/298 ━━━━━━━━━━━━━━━━━━━━ 16:29:40\n",
      "Accuracy: 0.9806 - Precision: 0.0008 - Recall: 0.0002 - Specificity: 0.9999 - F1: 0.0003 - Loss: 0.0215\n",
      "\n",
      "Batch 37/298 ━━━━━━━━━━━━━━━━━━━━ 16:31:08\n",
      "Accuracy: 0.9806 - Precision: 0.0008 - Recall: 0.0002 - Specificity: 0.9999 - F1: 0.0003 - Loss: 0.0210\n",
      "\n",
      "Batch 38/298 ━━━━━━━━━━━━━━━━━━━━ 16:32:25\n",
      "Accuracy: 0.9805 - Precision: 0.0008 - Recall: 0.0002 - Specificity: 0.9999 - F1: 0.0003 - Loss: 0.0207\n",
      "\n",
      "Batch 39/298 ━━━━━━━━━━━━━━━━━━━━ 16:33:24\n",
      "Accuracy: 0.9805 - Precision: 0.0008 - Recall: 0.0001 - Specificity: 0.9999 - F1: 0.0002 - Loss: 0.0203\n",
      "\n",
      "Batch 40/298 ━━━━━━━━━━━━━━━━━━━━ 16:34:30\n",
      "Accuracy: 0.9803 - Precision: 0.0008 - Recall: 0.0001 - Specificity: 0.9999 - F1: 0.0002 - Loss: 0.0200\n",
      "\n",
      "Batch 41/298 ━━━━━━━━━━━━━━━━━━━━ 16:35:29\n",
      "Accuracy: 0.9802 - Precision: 0.0007 - Recall: 0.0001 - Specificity: 0.9999 - F1: 0.0002 - Loss: 0.0197\n",
      "\n",
      "Batch 42/298 ━━━━━━━━━━━━━━━━━━━━ 16:36:46\n",
      "Accuracy: 0.9801 - Precision: 0.0007 - Recall: 0.0001 - Specificity: 0.9999 - F1: 0.0002 - Loss: 0.0193\n",
      "\n",
      "Batch 43/298 ━━━━━━━━━━━━━━━━━━━━ 16:38:14\n",
      "Accuracy: 0.9801 - Precision: 0.0007 - Recall: 0.0001 - Specificity: 0.9999 - F1: 0.0002 - Loss: 0.0191\n",
      "\n",
      "Batch 44/298 ━━━━━━━━━━━━━━━━━━━━ 16:39:13\n",
      "Accuracy: 0.9801 - Precision: 0.0007 - Recall: 0.0001 - Specificity: 0.9999 - F1: 0.0002 - Loss: 0.0188\n",
      "\n"
>>>>>>> a9b1774f (update before helene storm!)
=======
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 258\u001b[0m\n\u001b[0;32m    255\u001b[0m metrics_callback \u001b[38;5;241m=\u001b[39m MetricsCallback(total_batches\u001b[38;5;241m=\u001b[39msteps_per_epoch, X_valid\u001b[38;5;241m=\u001b[39mvalid_gen, y_valid\u001b[38;5;241m=\u001b[39mvalid_gen, X_test\u001b[38;5;241m=\u001b[39mtest_gen, y_test\u001b[38;5;241m=\u001b[39mtest_gen)\n\u001b[0;32m    257\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m    259\u001b[0m     train_gen,\n\u001b[0;32m    260\u001b[0m     steps_per_epoch\u001b[38;5;241m=\u001b[39msteps_per_epoch,\n\u001b[0;32m    261\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m    262\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39mvalid_gen,\n\u001b[0;32m    263\u001b[0m     validation_steps\u001b[38;5;241m=\u001b[39mvalidation_steps,\n\u001b[0;32m    264\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[metrics_callback, early_stopping],\n\u001b[0;32m    265\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    266\u001b[0m )\n\u001b[0;32m    268\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n\u001b[0;32m    269\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdental_xray_unet_capsule_model_with_canny.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:318\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    317\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 318\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    319\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    320\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 889\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwds, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[0;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    698\u001b[0m )\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[0;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[0;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[0;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m api\u001b[38;5;241m.\u001b[39mconverted_call(\n\u001b[0;32m     42\u001b[0m       original_func,\n\u001b[0;32m     43\u001b[0m       args,\n\u001b[0;32m     44\u001b[0m       kwargs,\n\u001b[0;32m     45\u001b[0m       options\u001b[38;5;241m=\u001b[39mconverter\u001b[38;5;241m.\u001b[39mConversionOptions(\n\u001b[0;32m     46\u001b[0m           recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m           optional_features\u001b[38;5;241m=\u001b[39mautograph_options,\n\u001b[0;32m     48\u001b[0m           user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       ))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[0;32m    338\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph artifact\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools\u001b[38;5;241m.\u001b[39mpartial):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:121\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_iterator\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step given a Dataset iterator.\"\"\"\u001b[39;00m\n\u001b[0;32m    120\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[1;32m--> 121\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m    122\u001b[0m     one_step_on_data, args\u001b[38;5;241m=\u001b[39m(data,)\n\u001b[0;32m    123\u001b[0m )\n\u001b[0;32m    124\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[0;32m    125\u001b[0m     outputs,\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[0;32m    127\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    128\u001b[0m )\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1673\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m   1669\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[0;32m   1670\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[0;32m   1671\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[0;32m   1672\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m-> 1673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extended\u001b[38;5;241m.\u001b[39mcall_for_each_replica(fn, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3263\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   3261\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   3262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m-> 3263\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_for_each_replica(fn, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:4061\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[1;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[0;32m   4059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[0;32m   4060\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m-> 4061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:889\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    887\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    888\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 889\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize(args, kwds, add_initializers_to\u001b[38;5;241m=\u001b[39minitializers)\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    892\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    893\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:696\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[0;32m    692\u001b[0m     variable_capturing_scope,\n\u001b[0;32m    693\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[1;32m--> 696\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mtrace_function(\n\u001b[0;32m    697\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    698\u001b[0m )\n\u001b[0;32m    700\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    701\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[0;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m _maybe_define_function(\n\u001b[0;32m    179\u001b[0m       args, kwargs, tracing_options\n\u001b[0;32m    180\u001b[0m   )\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[0;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[1;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m _create_concrete_function(\n\u001b[0;32m    284\u001b[0m     target_func_type, lookup_func_context, func_graph, tracing_options\n\u001b[0;32m    285\u001b[0m )\n\u001b[0;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[0;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[0;32m    290\u001b[0m   )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[1;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[0;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[0;32m    304\u001b[0m       placeholder_context\n\u001b[0;32m    305\u001b[0m   )\n\u001b[0;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    309\u001b[0m )\n\u001b[1;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m func_graph_module\u001b[38;5;241m.\u001b[39mfunc_graph_from_py_func(\n\u001b[0;32m    311\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     tracing_options\u001b[38;5;241m.\u001b[39mpython_function,\n\u001b[0;32m    313\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39margs,\n\u001b[0;32m    314\u001b[0m     placeholder_bound_args\u001b[38;5;241m.\u001b[39mkwargs,\n\u001b[0;32m    315\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    316\u001b[0m     func_graph\u001b[38;5;241m=\u001b[39mfunc_graph,\n\u001b[0;32m    317\u001b[0m     add_control_dependencies\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m disable_acd,\n\u001b[0;32m    318\u001b[0m     arg_names\u001b[38;5;241m=\u001b[39mfunction_type_utils\u001b[38;5;241m.\u001b[39mto_arg_names(function_type),\n\u001b[0;32m    319\u001b[0m     create_placeholders\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    320\u001b[0m )\n\u001b[0;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[0;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[0;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:599\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    596\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    598\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 599\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    600\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m api\u001b[38;5;241m.\u001b[39mconverted_call(\n\u001b[0;32m     42\u001b[0m       original_func,\n\u001b[0;32m     43\u001b[0m       args,\n\u001b[0;32m     44\u001b[0m       kwargs,\n\u001b[0;32m     45\u001b[0m       options\u001b[38;5;241m=\u001b[39mconverter\u001b[38;5;241m.\u001b[39mConversionOptions(\n\u001b[0;32m     46\u001b[0m           recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     47\u001b[0m           optional_features\u001b[38;5;241m=\u001b[39mautograph_options,\n\u001b[0;32m     48\u001b[0m           user_requested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m       ))\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:339\u001b[0m, in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_autograph_artifact(f):\n\u001b[0;32m    338\u001b[0m   logging\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPermanently allowed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: AutoGraph artifact\u001b[39m\u001b[38;5;124m'\u001b[39m, f)\n\u001b[1;32m--> 339\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# If this is a partial, unwrap it and redo all the checks.\u001b[39;00m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, functools\u001b[38;5;241m.\u001b[39mpartial):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\impl\\api.py:643\u001b[0m, in \u001b[0;36mdo_not_convert.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    642\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mDISABLED):\n\u001b[1;32m--> 643\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:108\u001b[0m, in \u001b[0;36mTensorFlowTrainer.make_train_function.<locals>.one_step_on_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;129m@tf\u001b[39m\u001b[38;5;241m.\u001b[39mautograph\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mdo_not_convert\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_step_on_data\u001b[39m(data):\n\u001b[0;32m    107\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Runs a single training step on a batch of data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_step(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:51\u001b[0m, in \u001b[0;36mTensorFlowTrainer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mGradientTape() \u001b[38;5;28;01mas\u001b[39;00m tape:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_has_training_arg:\n\u001b[1;32m---> 51\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:882\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    880\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 882\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m         call_fn,\n\u001b[0;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:175\u001b[0m, in \u001b[0;36mFunctional.call\u001b[1;34m(self, inputs, training, mask)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    174\u001b[0m             x\u001b[38;5;241m.\u001b[39m_keras_mask \u001b[38;5;241m=\u001b[39m mask\n\u001b[1;32m--> 175\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_through_graph(\n\u001b[0;32m    176\u001b[0m     inputs, operation_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m op: operation_fn(op, training\u001b[38;5;241m=\u001b[39mtraining)\n\u001b[0;32m    177\u001b[0m )\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unpack_singleton(outputs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\function.py:171\u001b[0m, in \u001b[0;36mFunction._run_through_graph\u001b[1;34m(self, inputs, operation_fn, call_fn)\u001b[0m\n\u001b[0;32m    169\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m call_fn(op, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 171\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;66;03m# Update tensor_dict.\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(node\u001b[38;5;241m.\u001b[39moutputs, tree\u001b[38;5;241m.\u001b[39mflatten(outputs)):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\models\\functional.py:556\u001b[0m, in \u001b[0;36moperation_fn.<locals>.call\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(operation, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_has_training_arg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m operation\u001b[38;5;241m.\u001b[39m_call_has_training_arg\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m training \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    554\u001b[0m ):\n\u001b[0;32m    555\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training\n\u001b[1;32m--> 556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m operation(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\layer.py:882\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    880\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 882\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[0;32m    886\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\ops\\operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[0;32m     43\u001b[0m         call_fn,\n\u001b[0;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m     45\u001b[0m     )\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m call_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 81\u001b[0m, in \u001b[0;36mCapsuleLayer.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     79\u001b[0m c \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(b, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     80\u001b[0m s \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(c[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, tf\u001b[38;5;241m.\u001b[39mnewaxis] \u001b[38;5;241m*\u001b[39m u_hat, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 81\u001b[0m v \u001b[38;5;241m=\u001b[39m squash(s)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_routing \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     83\u001b[0m     b \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(u_hat \u001b[38;5;241m*\u001b[39m v[:, tf\u001b[38;5;241m.\u001b[39mnewaxis, :, :], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 58\u001b[0m, in \u001b[0;36msquash\u001b[1;34m(vectors, axis)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msquash\u001b[39m(vectors, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 58\u001b[0m     s_squared_norm \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreduce_sum(tf\u001b[38;5;241m.\u001b[39msquare(vectors), axis, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     59\u001b[0m     scale \u001b[38;5;241m=\u001b[39m s_squared_norm \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m s_squared_norm) \u001b[38;5;241m/\u001b[39m tf\u001b[38;5;241m.\u001b[39msqrt(s_squared_norm \u001b[38;5;241m+\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mbackend\u001b[38;5;241m.\u001b[39mepsilon())\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scale \u001b[38;5;241m*\u001b[39m vectors\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:88\u001b[0m, in \u001b[0;36mweak_tensor_unary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     89\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     90\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m dispatch_target(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[0;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[0;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2215\u001b[0m, in \u001b[0;36mreduce_sum\u001b[1;34m(input_tensor, axis, keepdims, name)\u001b[0m\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmath.reduce_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreduce_sum\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m   2153\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[0;32m   2154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_sum\u001b[39m(input_tensor, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Computes the sum of elements across dimensions of a tensor.\u001b[39;00m\n\u001b[0;32m   2156\u001b[0m \n\u001b[0;32m   2157\u001b[0m \u001b[38;5;124;03m  This is the reduction operation for the elementwise `tf.math.add` op.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2212\u001b[0m \u001b[38;5;124;03m  @end_compatibility\u001b[39;00m\n\u001b[0;32m   2213\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2215\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m reduce_sum_with_dims(input_tensor, axis, keepdims, name,\n\u001b[0;32m   2216\u001b[0m                               _ReductionDims(input_tensor, axis))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:2227\u001b[0m, in \u001b[0;36mreduce_sum_with_dims\u001b[1;34m(input_tensor, axis, keepdims, name, dims)\u001b[0m\n\u001b[0;32m   2219\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce_sum_with_dims\u001b[39m(input_tensor,\n\u001b[0;32m   2220\u001b[0m                          axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2221\u001b[0m                          keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   2222\u001b[0m                          name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   2223\u001b[0m                          dims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   2224\u001b[0m   keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(keepdims)\n\u001b[0;32m   2225\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _may_reduce_to_scalar(\n\u001b[0;32m   2226\u001b[0m       keepdims, axis,\n\u001b[1;32m-> 2227\u001b[0m       gen_math_ops\u001b[38;5;241m.\u001b[39m_sum(input_tensor, dims, keepdims, name\u001b[38;5;241m=\u001b[39mname))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py:13763\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(input, axis, keep_dims, name)\u001b[0m\n\u001b[0;32m  13761\u001b[0m   keep_dims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m  13762\u001b[0m keep_dims \u001b[38;5;241m=\u001b[39m _execute\u001b[38;5;241m.\u001b[39mmake_bool(keep_dims, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeep_dims\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m> 13763\u001b[0m _, _, _op, _outputs \u001b[38;5;241m=\u001b[39m _op_def_library\u001b[38;5;241m.\u001b[39m_apply_op_helper(\n\u001b[0;32m  13764\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, reduction_indices\u001b[38;5;241m=\u001b[39maxis, keep_dims\u001b[38;5;241m=\u001b[39mkeep_dims,\n\u001b[0;32m  13765\u001b[0m              name\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m  13766\u001b[0m _result \u001b[38;5;241m=\u001b[39m _outputs[:]\n\u001b[0;32m  13767\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _execute\u001b[38;5;241m.\u001b[39mmust_record_gradient():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:778\u001b[0m, in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m g\u001b[38;5;241m.\u001b[39mas_default(), ops\u001b[38;5;241m.\u001b[39mname_scope(name) \u001b[38;5;28;01mas\u001b[39;00m scope:\n\u001b[0;32m    777\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m fallback:\n\u001b[1;32m--> 778\u001b[0m     _ExtractInputsAndAttrs(op_type_name, op_def, allowed_list_attr_map,\n\u001b[0;32m    779\u001b[0m                            keywords, default_type_attr_map, attrs, inputs,\n\u001b[0;32m    780\u001b[0m                            input_types)\n\u001b[0;32m    781\u001b[0m     _ExtractRemainingAttrs(op_type_name, op_def, keywords,\n\u001b[0;32m    782\u001b[0m                            default_type_attr_map, attrs)\n\u001b[0;32m    783\u001b[0m     _ExtractAttrProto(op_type_name, op_def, attrs, attr_protos)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:531\u001b[0m, in \u001b[0;36m_ExtractInputsAndAttrs\u001b[1;34m(op_type_name, op_def, allowed_list_attr_map, keywords, default_type_attr_map, attrs, inputs, input_types)\u001b[0m\n\u001b[0;32m    529\u001b[0m inferred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 531\u001b[0m   inferred \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(\n\u001b[0;32m    532\u001b[0m       values, name\u001b[38;5;241m=\u001b[39minput_arg\u001b[38;5;241m.\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39minput_arg\u001b[38;5;241m.\u001b[39mis_ref)\n\u001b[0;32m    533\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    534\u001b[0m   \u001b[38;5;66;03m# When converting a python object such as a list of Dimensions, we\u001b[39;00m\n\u001b[0;32m    535\u001b[0m   \u001b[38;5;66;03m# need a dtype to be specified, thus tensor conversion may throw\u001b[39;00m\n\u001b[0;32m    536\u001b[0m   \u001b[38;5;66;03m# an exception which we will ignore and try again below.\u001b[39;00m\n\u001b[0;32m    537\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\profiler\\trace.py:183\u001b[0m, in \u001b[0;36mtrace_wrapper.<locals>.inner_wrapper.<locals>.wrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m Trace(trace_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrace_kwargs):\n\u001b[0;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:713\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m    711\u001b[0m \u001b[38;5;66;03m# TODO(b/142518781): Fix all call-sites and remove redundant arg\u001b[39;00m\n\u001b[0;32m    712\u001b[0m preferred_dtype \u001b[38;5;241m=\u001b[39m preferred_dtype \u001b[38;5;129;01mor\u001b[39;00m dtype_hint\n\u001b[1;32m--> 713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tensor_conversion_registry\u001b[38;5;241m.\u001b[39mconvert(\n\u001b[0;32m    714\u001b[0m     value, dtype, name, as_ref, preferred_dtype, accepted_result_types\n\u001b[0;32m    715\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[0;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    226\u001b[0m           _add_error_prefix(\n\u001b[0;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m conversion_func(value, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname, as_ref\u001b[38;5;241m=\u001b[39mas_ref)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[0;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m constant_op\u001b[38;5;241m.\u001b[39mconstant(v, dtype\u001b[38;5;241m=\u001b[39mdtype, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[1;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[0;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_impl(value, dtype, shape, name, verify_shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    277\u001b[0m                         allow_broadcast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:291\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m    289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[0;32m    293\u001b[0m )\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:281\u001b[0m, in \u001b[0;36m_create_graph_constant\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    279\u001b[0m dtype_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue(\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtensor_value\u001b[38;5;241m.\u001b[39mtensor\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    280\u001b[0m attrs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensor_value, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: dtype_value}\n\u001b[1;32m--> 281\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m g\u001b[38;5;241m.\u001b[39m_create_op_internal(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, [], [dtype_value\u001b[38;5;241m.\u001b[39mtype], attrs\u001b[38;5;241m=\u001b[39mattrs, name\u001b[38;5;241m=\u001b[39mname)\u001b[38;5;241m.\u001b[39moutputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m op_callbacks\u001b[38;5;241m.\u001b[39mshould_invoke_op_callbacks():\n\u001b[0;32m    285\u001b[0m   \u001b[38;5;66;03m# TODO(b/147670703): Once the special-op creation code paths\u001b[39;00m\n\u001b[0;32m    286\u001b[0m   \u001b[38;5;66;03m# are unified. Remove this `if` block.\u001b[39;00m\n\u001b[0;32m    287\u001b[0m   callback_outputs \u001b[38;5;241m=\u001b[39m op_callbacks\u001b[38;5;241m.\u001b[39minvoke_op_callbacks(\n\u001b[0;32m    288\u001b[0m       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mtuple\u001b[39m(), attrs, (const_tensor,), op_name\u001b[38;5;241m=\u001b[39mname, graph\u001b[38;5;241m=\u001b[39mg)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[38;5;241m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m_create_op_internal(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:2682\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;66;03m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;66;03m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 2682\u001b[0m   ret \u001b[38;5;241m=\u001b[39m Operation\u001b[38;5;241m.\u001b[39mfrom_node_def(\n\u001b[0;32m   2683\u001b[0m       node_def,\n\u001b[0;32m   2684\u001b[0m       \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2685\u001b[0m       inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m   2686\u001b[0m       output_types\u001b[38;5;241m=\u001b[39mdtypes,\n\u001b[0;32m   2687\u001b[0m       control_inputs\u001b[38;5;241m=\u001b[39mcontrol_inputs,\n\u001b[0;32m   2688\u001b[0m       input_types\u001b[38;5;241m=\u001b[39minput_types,\n\u001b[0;32m   2689\u001b[0m       original_op\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_original_op,\n\u001b[0;32m   2690\u001b[0m       op_def\u001b[38;5;241m=\u001b[39mop_def,\n\u001b[0;32m   2691\u001b[0m   )\n\u001b[0;32m   2692\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_op_helper(ret, compute_device\u001b[38;5;241m=\u001b[39mcompute_device)\n\u001b[0;32m   2693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1177\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1174\u001b[0m     control_input_ops\u001b[38;5;241m.\u001b[39mappend(control_op)\n\u001b[0;32m   1176\u001b[0m \u001b[38;5;66;03m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1177\u001b[0m c_op \u001b[38;5;241m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[38;5;241m=\u001b[39mop_def)\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m Operation(c_op, SymbolicTensor)\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init(g)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1007\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1005\u001b[0m \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1006\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m graph\u001b[38;5;241m.\u001b[39m_c_graph\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mas\u001b[39;00m c_graph:\n\u001b[1;32m-> 1007\u001b[0m   op_desc \u001b[38;5;241m=\u001b[39m pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_NewOperation(c_graph,\n\u001b[0;32m   1008\u001b[0m                                               compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mop),\n\u001b[0;32m   1009\u001b[0m                                               compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mname))\n\u001b[0;32m   1010\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node_def\u001b[38;5;241m.\u001b[39mdevice:\n\u001b[0;32m   1011\u001b[0m   pywrap_tf_session\u001b[38;5;241m.\u001b[39mTF_SetDevice(op_desc, compat\u001b[38;5;241m.\u001b[39mas_str(node_def\u001b[38;5;241m.\u001b[39mdevice))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
>>>>>>> 09cd13ea (add)
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Directory paths\n",
    "train_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\"\n",
    "train_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\\train_mask\"\n",
    "test_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\"\n",
    "test_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\\test_mask\"\n",
    "valid_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\"\n",
    "valid_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\\valid_mask\"\n",
    "\n",
    "# Image generator to load data in batches\n",
    "def image_generator(img_dir, mask_dir, batch_size, img_size=(256, 256)):\n",
    "    img_files = os.listdir(img_dir)\n",
    "    while True:\n",
    "        images = []\n",
    "        masks = []\n",
    "        for img_file in img_files:\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            mask_file = img_file + \"_mask.png\"\n",
    "            mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "            if os.path.exists(mask_path):\n",
    "                # Load image and mask\n",
    "                img = load_img(img_path, color_mode='rgb', target_size=img_size)\n",
    "                img = img_to_array(img) / 255.0\n",
    "                mask = load_img(mask_path, color_mode='grayscale', target_size=img_size)\n",
    "                mask = img_to_array(mask) / 255.0\n",
    "\n",
    "                # Convert image to grayscale for edge detection\n",
    "                img_gray = cv2.cvtColor((img * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "                canny_edges = cv2.Canny(img_gray, threshold1=100, threshold2=200)\n",
    "                canny_edges = canny_edges / 255.0  # Normalize to range 0-1\n",
    "\n",
    "                # Stack image with Canny edges as an additional channel\n",
    "                img_stack = np.stack((img_gray, canny_edges), axis=-1)\n",
    "\n",
    "                images.append(img_stack)\n",
    "                masks.append(mask)\n",
    "   \n",
    "            if len(images) == batch_size:\n",
    "                yield np.array(images), np.array(masks)\n",
    "                images = []\n",
    "                masks = []\n",
    "\n",
    "# Capsule Layer with Dynamic Routing\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, num_routing=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=[input_shape[-1], self.num_capsules * self.dim_capsule],\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs, [-1, inputs.shape[1] * inputs.shape[2], inputs.shape[3]])\n",
    "        u_hat = tf.einsum('...ij,jk->...ik', inputs, self.W)\n",
    "        u_hat = tf.reshape(u_hat, [-1, inputs.shape[1], self.num_capsules, self.dim_capsule])\n",
    "        b = tf.zeros(shape=[tf.shape(inputs)[0], inputs.shape[1], self.num_capsules])\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(b, axis=-1)\n",
    "            s = tf.reduce_sum(c[..., tf.newaxis] * u_hat, axis=1)\n",
    "            v = squash(s)\n",
    "            if i < self.num_routing - 1:\n",
    "                b += tf.reduce_sum(u_hat * v[:, tf.newaxis, :, :], axis=-1)\n",
    "        return v\n",
    "\n",
    "# Attention Gate\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    theta_x = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(x)\n",
    "    phi_g = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(g)\n",
    "    concat_xg = tf.keras.layers.add([theta_x, phi_g])\n",
    "    act_xg = tf.keras.layers.Activation('relu')(concat_xg)\n",
    "    psi = tf.keras.layers.Conv2D(1, 1, padding='same')(act_xg)\n",
    "    sigmoid_xg = tf.keras.layers.Activation('sigmoid')(psi)\n",
    "    y = tf.keras.layers.multiply([x, sigmoid_xg])\n",
    "    return y\n",
    "\n",
    "# U-Net with Capsule Network Layers, Attention Mechanisms, and Canny Edge Detection\n",
    "def unet_capsule_model(input_size=(256, 256, 2)):\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    c1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = CapsuleLayer(num_capsules=8, dim_capsule=16)(c1)\n",
    "    c1_flattened = tf.keras.layers.Flatten()(c1)\n",
    "    c1_reshaped = tf.keras.layers.Dense(256*256, activation='relu')(c1_flattened)\n",
    "    c1_reshaped = tf.keras.layers.Reshape((256, 256, 1))(c1_reshaped)\n",
    "    p1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c1_reshaped)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = CapsuleLayer(num_capsules=16, dim_capsule=32)(c2)\n",
    "    c2_flattened = tf.keras.layers.Flatten()(c2)\n",
    "    c2_reshaped = tf.keras.layers.Dense(128*128, activation='relu')(c2_flattened)\n",
    "    c2_reshaped = tf.keras.layers.Reshape((128, 128, 1))(c2_reshaped)\n",
    "    p2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c2_reshaped)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = CapsuleLayer(num_capsules=32, dim_capsule=64)(c3)\n",
    "    c3_flattened = tf.keras.layers.Flatten()(c3)\n",
    "    c3_reshaped = tf.keras.layers.Dense(64*64, activation='relu')(c3_flattened)\n",
    "    c3_reshaped = tf.keras.layers.Reshape((64, 64, 1))(c3_reshaped)\n",
    "    p3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c3_reshaped)\n",
    "\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(b)\n",
    "\n",
    "    u1 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(b)\n",
    "    c3_attention = attention_gate(c3_reshaped, u1, inter_channels=128)\n",
    "    u1 = tf.keras.layers.concatenate([u1, c3_attention])\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(c4)\n",
    "    c2_attention = attention_gate(c2_reshaped, u2, inter_channels=64)\n",
    "    u2 = tf.keras.layers.concatenate([u2, c2_attention])\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(c5)\n",
    "\n",
    "    u3 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(c5)\n",
    "    c1_attention = attention_gate(c1_reshaped, u3, inter_channels=32)\n",
    "    u3 = tf.keras.layers.concatenate([u3, c1_attention])\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(u3)\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(c6)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(c6)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Define custom metrics\n",
    "def custom_precision(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    predicted_positives = K.sum(y_pred_bin)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    possible_positives = K.sum(y_true)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def custom_specificity(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_negatives = K.sum(K.round((1 - y_true) * (1 - y_pred_bin)))\n",
    "    possible_negatives = K.sum(1 - y_true)\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    precision = custom_precision(y_true, y_pred)\n",
    "    recall = custom_recall(y_true, y_pred)\n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "# Define Focal Loss\n",
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "    weight = alpha * y_true * K.pow((1 - y_pred), gamma) + (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma)\n",
    "    loss = weight * cross_entropy\n",
    "    return K.mean(loss)\n",
    "\n",
    "# Compile the model\n",
    "model = unet_capsule_model()\n",
    "model.compile(optimizer='adam', loss=focal_loss_fixed, metrics=['accuracy', custom_precision, custom_recall, custom_specificity, custom_f1])\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 16\n",
    "\n",
    "# Create data generators\n",
    "train_gen = image_generator(train_img_dir, train_mask_dir, batch_size)\n",
    "valid_gen = image_generator(valid_img_dir, valid_mask_dir, batch_size)\n",
    "test_gen = image_generator(test_img_dir, test_mask_dir, batch_size)\n",
    "\n",
    "# Number of steps per epoch\n",
    "steps_per_epoch = len(os.listdir(train_img_dir)) // batch_size\n",
    "validation_steps = len(os.listdir(valid_img_dir)) // batch_size\n",
    "test_steps = len(os.listdir(test_img_dir)) // batch_size\n",
    "\n",
    "# Custom callback to print more metrics at each batch and epoch for training, validation, and test sets\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches, X_valid, y_valid, X_test, y_test):\n",
    "        super().__init__()\n",
    "        self.batch_counter = 1\n",
    "        self.total_batches = total_batches\n",
    "        self.current_epoch = 1\n",
    "        self.X_valid = X_valid\n",
    "        self.y_valid = y_valid\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch + 1\n",
    "        print(f\"\\nEpoch {self.current_epoch}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Batch {self.batch_counter}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Specificity: {specificity:.4f} - F1: {f1:.4f} - Loss: {loss:.4f}\\n\")\n",
    "        self.batch_counter += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        val_accuracy = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        val_precision = logs.get('val_custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        val_recall = logs.get('val_custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        val_f1 = logs.get('val_custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        val_specificity = logs.get('val_custom_specificity', 0)\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}\")\n",
    "        print(f\"Train - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, Specificity: {specificity:.4f}, F1: {f1:.4f}, Loss: {loss:.4f}\")\n",
    "        print(f\"Validation - Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, Specificity: {val_specificity:.4f}, F1: {val_f1:.4f}, Loss: {val_loss:.4f}\\n\")\n",
    "        test_loss, test_accuracy, test_precision, test_recall, test_specificity, test_f1 = self.model.evaluate(self.X_test, self.y_test, verbose=0)\n",
    "        print(f\"Test Set Results - Accuracy: {test_accuracy:.4f}, Precision: {test_precision:.4f}, Recall: {test_recall:.4f}, Specificity: {test_specificity:.4f}, F1: {test_f1:.4f}, Loss: {test_loss:.4f}\\n\")\n",
    "        self.batch_counter = 1\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize the custom callback with validation and test data\n",
    "metrics_callback = MetricsCallback(total_batches=steps_per_epoch, X_valid=valid_gen, y_valid=valid_gen, X_test=test_gen, y_test=test_gen)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=20,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[metrics_callback, early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('dental_xray_unet_capsule_model_with_canny.h5')\n",
    "\n",
    "# Evaluate on the training set\n",
    "train_gen_full = image_generator(train_img_dir, train_mask_dir, batch_size)\n",
    "y_train_pred = model.predict(train_gen_full, steps=steps_per_epoch)\n",
    "y_train_pred_bin = (y_train_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Confusion Matrix for training\n",
    "conf_matrix_train = confusion_matrix(y_train_pred_bin.flatten(), y_train_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_train, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Train\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for validation set\n",
    "y_valid_pred = model.predict(valid_gen, steps=validation_steps)\n",
    "y_valid_pred_bin = (y_valid_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "conf_matrix_valid = confusion_matrix(y_valid_pred_bin.flatten(), y_valid_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_valid, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Validation\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix for test set\n",
    "y_test_pred = model.predict(test_gen, steps=test_steps)\n",
    "y_test_pred_bin = (y_test_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "conf_matrix_test = confusion_matrix(y_test_pred_bin.flatten(), y_test_pred_bin.flatten())\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_test, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix for Test\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.show()\n",
    "\n",
    "# Visualization: Show input image, true mask, and predicted mask for a few samples\n",
    "def visualize_predictions(images, true_masks, pred_masks, title):\n",
    "    for i in range(3):\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(true_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions for training set\n",
    "visualize_predictions(X_train, y_train, y_train_pred_bin, \"Train Set Predictions\")\n",
    "\n",
    "# Visualize predictions for validation set\n",
    "visualize_predictions(X_valid, y_valid, y_valid_pred_bin, \"Validation Set Predictions\")\n",
    "\n",
    "# Visualize predictions for test set\n",
    "visualize_predictions(X_test, y_test, y_test_pred_bin, \"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd2aa2c-790b-4da0-a507-bd0df903964c",
   "metadata": {},
   "outputs": [],
   "source": []
<<<<<<< HEAD
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f870c64f-87d2-4515-ae24-1ec137a6e2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Jaber\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:192: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "Batch 1/298 ━━━━━━━━━━━━━━━━━━━━ 13:36:41\n",
      "Accuracy: 0.8815 - Precision: 0.0274 - Recall: 0.0974 - Specificity: 0.9034 - F1: 0.0428 - Loss: 0.1275\n",
      "\n",
      "Batch 2/298 ━━━━━━━━━━━━━━━━━━━━ 13:37:59\n",
      "Accuracy: 0.9330 - Precision: 0.0137 - Recall: 0.0487 - Specificity: 0.9517 - F1: 0.0214 - Loss: 0.1262\n",
      "\n",
      "Batch 3/298 ━━━━━━━━━━━━━━━━━━━━ 13:38:55\n",
      "Accuracy: 0.9490 - Precision: 0.0091 - Recall: 0.0325 - Specificity: 0.9678 - F1: 0.0143 - Loss: 0.1230\n",
      "\n",
      "Batch 4/298 ━━━━━━━━━━━━━━━━━━━━ 13:39:50\n",
      "Accuracy: 0.9574 - Precision: 0.0069 - Recall: 0.0243 - Specificity: 0.9759 - F1: 0.0107 - Loss: 0.1149\n",
      "\n",
      "Batch 5/298 ━━━━━━━━━━━━━━━━━━━━ 13:40:47\n",
      "Accuracy: 0.9627 - Precision: 0.0055 - Recall: 0.0195 - Specificity: 0.9807 - F1: 0.0086 - Loss: 0.0943\n",
      "\n",
      "Batch 6/298 ━━━━━━━━━━━━━━━━━━━━ 13:42:01\n",
      "Accuracy: 0.9661 - Precision: 0.0046 - Recall: 0.0162 - Specificity: 0.9839 - F1: 0.0071 - Loss: 0.0889\n",
      "\n",
      "Batch 7/298 ━━━━━━━━━━━━━━━━━━━━ 13:43:07\n",
      "Accuracy: 0.9689 - Precision: 0.0039 - Recall: 0.0139 - Specificity: 0.9862 - F1: 0.0061 - Loss: 0.0826\n",
      "\n",
      "Batch 8/298 ━━━━━━━━━━━━━━━━━━━━ 13:44:18\n",
      "Accuracy: 0.9706 - Precision: 0.0034 - Recall: 0.0122 - Specificity: 0.9879 - F1: 0.0053 - Loss: 0.0757\n",
      "\n",
      "Batch 9/298 ━━━━━━━━━━━━━━━━━━━━ 13:45:15\n",
      "Accuracy: 0.9719 - Precision: 0.0030 - Recall: 0.0108 - Specificity: 0.9893 - F1: 0.0048 - Loss: 0.0684\n",
      "\n",
      "Batch 10/298 ━━━━━━━━━━━━━━━━━━━━ 13:46:09\n",
      "Accuracy: 0.9731 - Precision: 0.0027 - Recall: 0.0097 - Specificity: 0.9903 - F1: 0.0043 - Loss: 0.0626\n",
      "\n",
      "Batch 11/298 ━━━━━━━━━━━━━━━━━━━━ 13:47:06\n",
      "Accuracy: 0.9741 - Precision: 0.0025 - Recall: 0.0089 - Specificity: 0.9912 - F1: 0.0039 - Loss: 0.0585\n",
      "\n",
      "Batch 12/298 ━━━━━━━━━━━━━━━━━━━━ 13:47:59\n",
      "Accuracy: 0.9749 - Precision: 0.0023 - Recall: 0.0081 - Specificity: 0.9920 - F1: 0.0036 - Loss: 0.0545\n",
      "\n",
      "Batch 13/298 ━━━━━━━━━━━━━━━━━━━━ 13:48:54\n",
      "Accuracy: 0.9752 - Precision: 0.0021 - Recall: 0.0075 - Specificity: 0.9926 - F1: 0.0033 - Loss: 0.0508\n",
      "\n",
      "Batch 14/298 ━━━━━━━━━━━━━━━━━━━━ 13:49:45\n",
      "Accuracy: 0.9755 - Precision: 0.0020 - Recall: 0.0070 - Specificity: 0.9931 - F1: 0.0031 - Loss: 0.0478\n",
      "\n",
      "Batch 15/298 ━━━━━━━━━━━━━━━━━━━━ 13:50:37\n",
      "Accuracy: 0.9761 - Precision: 0.0018 - Recall: 0.0065 - Specificity: 0.9936 - F1: 0.0029 - Loss: 0.0452\n",
      "\n",
      "Batch 16/298 ━━━━━━━━━━━━━━━━━━━━ 13:51:31\n",
      "Accuracy: 0.9766 - Precision: 0.0017 - Recall: 0.0061 - Specificity: 0.9940 - F1: 0.0027 - Loss: 0.0429\n",
      "\n",
      "Batch 17/298 ━━━━━━━━━━━━━━━━━━━━ 13:52:22\n",
      "Accuracy: 0.9765 - Precision: 0.0016 - Recall: 0.0057 - Specificity: 0.9943 - F1: 0.0025 - Loss: 0.0411\n",
      "\n",
      "Batch 18/298 ━━━━━━━━━━━━━━━━━━━━ 13:53:17\n",
      "Accuracy: 0.9769 - Precision: 0.0015 - Recall: 0.0054 - Specificity: 0.9946 - F1: 0.0024 - Loss: 0.0393\n",
      "\n",
      "Batch 19/298 ━━━━━━━━━━━━━━━━━━━━ 13:54:08\n",
      "Accuracy: 0.9772 - Precision: 0.0014 - Recall: 0.0051 - Specificity: 0.9949 - F1: 0.0023 - Loss: 0.0375\n",
      "\n",
      "Batch 20/298 ━━━━━━━━━━━━━━━━━━━━ 13:55:03\n",
      "Accuracy: 0.9775 - Precision: 0.0014 - Recall: 0.0049 - Specificity: 0.9952 - F1: 0.0021 - Loss: 0.0359\n",
      "\n",
      "Batch 21/298 ━━━━━━━━━━━━━━━━━━━━ 13:55:54\n",
      "Accuracy: 0.9776 - Precision: 0.0013 - Recall: 0.0046 - Specificity: 0.9954 - F1: 0.0020 - Loss: 0.0346\n",
      "\n",
      "Batch 22/298 ━━━━━━━━━━━━━━━━━━━━ 13:56:47\n",
      "Accuracy: 0.9778 - Precision: 0.0012 - Recall: 0.0044 - Specificity: 0.9956 - F1: 0.0019 - Loss: 0.0333\n",
      "\n",
      "Batch 23/298 ━━━━━━━━━━━━━━━━━━━━ 13:57:38\n",
      "Accuracy: 0.9777 - Precision: 0.0012 - Recall: 0.0042 - Specificity: 0.9958 - F1: 0.0019 - Loss: 0.0322\n",
      "\n",
      "Batch 24/298 ━━━━━━━━━━━━━━━━━━━━ 13:58:36\n",
      "Accuracy: 0.9779 - Precision: 0.0011 - Recall: 0.0041 - Specificity: 0.9960 - F1: 0.0018 - Loss: 0.0311\n",
      "\n",
      "Batch 25/298 ━━━━━━━━━━━━━━━━━━━━ 13:59:32\n",
      "Accuracy: 0.9781 - Precision: 0.0011 - Recall: 0.0039 - Specificity: 0.9961 - F1: 0.0017 - Loss: 0.0302\n",
      "\n",
      "Batch 26/298 ━━━━━━━━━━━━━━━━━━━━ 14:00:23\n",
      "Accuracy: 0.9781 - Precision: 0.0011 - Recall: 0.0037 - Specificity: 0.9963 - F1: 0.0016 - Loss: 0.0293\n",
      "\n",
      "Batch 27/298 ━━━━━━━━━━━━━━━━━━━━ 14:01:19\n",
      "Accuracy: 0.9782 - Precision: 0.0010 - Recall: 0.0036 - Specificity: 0.9964 - F1: 0.0016 - Loss: 0.0284\n",
      "\n",
      "Batch 28/298 ━━━━━━━━━━━━━━━━━━━━ 14:02:12\n",
      "Accuracy: 0.9783 - Precision: 0.0010 - Recall: 0.0035 - Specificity: 0.9966 - F1: 0.0015 - Loss: 0.0276\n",
      "\n",
      "Batch 29/298 ━━━━━━━━━━━━━━━━━━━━ 14:03:06\n",
      "Accuracy: 0.9783 - Precision: 0.0009 - Recall: 0.0034 - Specificity: 0.9967 - F1: 0.0015 - Loss: 0.0269\n",
      "\n",
      "Batch 30/298 ━━━━━━━━━━━━━━━━━━━━ 14:03:59\n",
      "Accuracy: 0.9784 - Precision: 0.0009 - Recall: 0.0032 - Specificity: 0.9968 - F1: 0.0014 - Loss: 0.0262\n",
      "\n",
      "Batch 31/298 ━━━━━━━━━━━━━━━━━━━━ 14:04:57\n",
      "Accuracy: 0.9783 - Precision: 0.0009 - Recall: 0.0031 - Specificity: 0.9969 - F1: 0.0014 - Loss: 0.0256\n",
      "\n",
      "Batch 32/298 ━━━━━━━━━━━━━━━━━━━━ 14:05:59\n",
      "Accuracy: 0.9783 - Precision: 0.0009 - Recall: 0.0030 - Specificity: 0.9970 - F1: 0.0013 - Loss: 0.0250\n",
      "\n",
      "Batch 33/298 ━━━━━━━━━━━━━━━━━━━━ 14:06:53\n",
      "Accuracy: 0.9781 - Precision: 0.0008 - Recall: 0.0030 - Specificity: 0.9971 - F1: 0.0013 - Loss: 0.0245\n",
      "\n",
      "Batch 34/298 ━━━━━━━━━━━━━━━━━━━━ 14:07:52\n",
      "Accuracy: 0.9780 - Precision: 0.0008 - Recall: 0.0029 - Specificity: 0.9972 - F1: 0.0013 - Loss: 0.0240\n",
      "\n",
      "Batch 35/298 ━━━━━━━━━━━━━━━━━━━━ 14:08:45\n",
      "Accuracy: 0.9781 - Precision: 0.0008 - Recall: 0.0028 - Specificity: 0.9972 - F1: 0.0012 - Loss: 0.0235\n",
      "\n",
      "Batch 36/298 ━━━━━━━━━━━━━━━━━━━━ 14:09:38\n",
      "Accuracy: 0.9782 - Precision: 0.0008 - Recall: 0.0027 - Specificity: 0.9973 - F1: 0.0012 - Loss: 0.0230\n",
      "\n",
      "Batch 37/298 ━━━━━━━━━━━━━━━━━━━━ 14:10:29\n",
      "Accuracy: 0.9783 - Precision: 0.0007 - Recall: 0.0026 - Specificity: 0.9974 - F1: 0.0012 - Loss: 0.0226\n",
      "\n",
      "Batch 38/298 ━━━━━━━━━━━━━━━━━━━━ 14:11:22\n",
      "Accuracy: 0.9782 - Precision: 0.0007 - Recall: 0.0026 - Specificity: 0.9975 - F1: 0.0011 - Loss: 0.0222\n",
      "\n",
      "Batch 39/298 ━━━━━━━━━━━━━━━━━━━━ 14:12:15\n",
      "Accuracy: 0.9782 - Precision: 0.0007 - Recall: 0.0025 - Specificity: 0.9975 - F1: 0.0011 - Loss: 0.0218\n",
      "\n",
      "Batch 40/298 ━━━━━━━━━━━━━━━━━━━━ 14:13:11\n",
      "Accuracy: 0.9781 - Precision: 0.0007 - Recall: 0.0024 - Specificity: 0.9976 - F1: 0.0011 - Loss: 0.0214\n",
      "\n",
      "Batch 41/298 ━━━━━━━━━━━━━━━━━━━━ 14:14:18\n",
      "Accuracy: 0.9781 - Precision: 0.0007 - Recall: 0.0024 - Specificity: 0.9976 - F1: 0.0010 - Loss: 0.0211\n",
      "\n",
      "Batch 42/298 ━━━━━━━━━━━━━━━━━━━━ 14:15:24\n",
      "Accuracy: 0.9781 - Precision: 0.0007 - Recall: 0.0023 - Specificity: 0.9977 - F1: 0.0010 - Loss: 0.0207\n",
      "\n",
      "Batch 43/298 ━━━━━━━━━━━━━━━━━━━━ 14:16:17\n",
      "Accuracy: 0.9781 - Precision: 0.0006 - Recall: 0.0023 - Specificity: 0.9978 - F1: 0.0010 - Loss: 0.0204\n",
      "\n",
      "Batch 44/298 ━━━━━━━━━━━━━━━━━━━━ 14:17:13\n",
      "Accuracy: 0.9781 - Precision: 0.0006 - Recall: 0.0022 - Specificity: 0.9978 - F1: 0.0010 - Loss: 0.0201\n",
      "\n",
      "Batch 45/298 ━━━━━━━━━━━━━━━━━━━━ 14:18:13\n",
      "Accuracy: 0.9782 - Precision: 0.0006 - Recall: 0.0022 - Specificity: 0.9979 - F1: 0.0010 - Loss: 0.0198\n",
      "\n",
      "Batch 46/298 ━━━━━━━━━━━━━━━━━━━━ 14:19:09\n",
      "Accuracy: 0.9782 - Precision: 0.0006 - Recall: 0.0021 - Specificity: 0.9979 - F1: 0.0009 - Loss: 0.0195\n",
      "\n",
      "Batch 47/298 ━━━━━━━━━━━━━━━━━━━━ 14:20:00\n",
      "Accuracy: 0.9783 - Precision: 0.0006 - Recall: 0.0021 - Specificity: 0.9979 - F1: 0.0009 - Loss: 0.0192\n",
      "\n",
      "Batch 48/298 ━━━━━━━━━━━━━━━━━━━━ 14:20:53\n",
      "Accuracy: 0.9783 - Precision: 0.0006 - Recall: 0.0020 - Specificity: 0.9980 - F1: 0.0009 - Loss: 0.0189\n",
      "\n",
      "Batch 49/298 ━━━━━━━━━━━━━━━━━━━━ 14:21:49\n",
      "Accuracy: 0.9784 - Precision: 0.0006 - Recall: 0.0020 - Specificity: 0.9980 - F1: 0.0009 - Loss: 0.0187\n",
      "\n",
      "Batch 50/298 ━━━━━━━━━━━━━━━━━━━━ 14:22:43\n",
      "Accuracy: 0.9785 - Precision: 0.0005 - Recall: 0.0019 - Specificity: 0.9981 - F1: 0.0009 - Loss: 0.0184\n",
      "\n",
      "Batch 51/298 ━━━━━━━━━━━━━━━━━━━━ 14:23:40\n",
      "Accuracy: 0.9784 - Precision: 0.0005 - Recall: 0.0019 - Specificity: 0.9981 - F1: 0.0008 - Loss: 0.0182\n",
      "\n",
      "Batch 52/298 ━━━━━━━━━━━━━━━━━━━━ 14:24:38\n",
      "Accuracy: 0.9785 - Precision: 0.0005 - Recall: 0.0019 - Specificity: 0.9981 - F1: 0.0008 - Loss: 0.0179\n",
      "\n",
      "Batch 53/298 ━━━━━━━━━━━━━━━━━━━━ 14:25:43\n",
      "Accuracy: 0.9785 - Precision: 0.0005 - Recall: 0.0018 - Specificity: 0.9982 - F1: 0.0008 - Loss: 0.0177\n",
      "\n",
      "Batch 54/298 ━━━━━━━━━━━━━━━━━━━━ 14:26:39\n",
      "Accuracy: 0.9786 - Precision: 0.0005 - Recall: 0.0018 - Specificity: 0.9982 - F1: 0.0008 - Loss: 0.0175\n",
      "\n",
      "Batch 55/298 ━━━━━━━━━━━━━━━━━━━━ 14:27:36\n",
      "Accuracy: 0.9786 - Precision: 0.0005 - Recall: 0.0018 - Specificity: 0.9982 - F1: 0.0008 - Loss: 0.0173\n",
      "\n",
      "Batch 56/298 ━━━━━━━━━━━━━━━━━━━━ 14:28:28\n",
      "Accuracy: 0.9786 - Precision: 0.0005 - Recall: 0.0017 - Specificity: 0.9983 - F1: 0.0008 - Loss: 0.0171\n",
      "\n",
      "Batch 57/298 ━━━━━━━━━━━━━━━━━━━━ 14:29:29\n",
      "Accuracy: 0.9785 - Precision: 0.0005 - Recall: 0.0017 - Specificity: 0.9983 - F1: 0.0008 - Loss: 0.0169\n",
      "\n",
      "Batch 58/298 ━━━━━━━━━━━━━━━━━━━━ 14:30:21\n",
      "Accuracy: 0.9785 - Precision: 0.0005 - Recall: 0.0017 - Specificity: 0.9983 - F1: 0.0007 - Loss: 0.0167\n",
      "\n",
      "Batch 59/298 ━━━━━━━━━━━━━━━━━━━━ 14:31:15\n",
      "Accuracy: 0.9785 - Precision: 0.0005 - Recall: 0.0017 - Specificity: 0.9984 - F1: 0.0007 - Loss: 0.0166\n",
      "\n",
      "Batch 60/298 ━━━━━━━━━━━━━━━━━━━━ 14:32:07\n",
      "Accuracy: 0.9785 - Precision: 0.0005 - Recall: 0.0016 - Specificity: 0.9984 - F1: 0.0007 - Loss: 0.0164\n",
      "\n",
      "Batch 61/298 ━━━━━━━━━━━━━━━━━━━━ 14:33:04\n",
      "Accuracy: 0.9785 - Precision: 0.0004 - Recall: 0.0016 - Specificity: 0.9984 - F1: 0.0007 - Loss: 0.0162\n",
      "\n",
      "Batch 62/298 ━━━━━━━━━━━━━━━━━━━━ 14:33:59\n",
      "Accuracy: 0.9786 - Precision: 0.0004 - Recall: 0.0016 - Specificity: 0.9984 - F1: 0.0007 - Loss: 0.0161\n",
      "\n",
      "Batch 63/298 ━━━━━━━━━━━━━━━━━━━━ 14:35:01\n",
      "Accuracy: 0.9787 - Precision: 0.0004 - Recall: 0.0015 - Specificity: 0.9985 - F1: 0.0007 - Loss: 0.0159\n",
      "\n",
      "Batch 64/298 ━━━━━━━━━━━━━━━━━━━━ 14:36:06\n",
      "Accuracy: 0.9785 - Precision: 0.0004 - Recall: 0.0015 - Specificity: 0.9985 - F1: 0.0007 - Loss: 0.0159\n",
      "\n",
      "Batch 65/298 ━━━━━━━━━━━━━━━━━━━━ 14:37:00\n",
      "Accuracy: 0.9785 - Precision: 0.0004 - Recall: 0.0015 - Specificity: 0.9985 - F1: 0.0007 - Loss: 0.0158\n",
      "\n",
      "Batch 66/298 ━━━━━━━━━━━━━━━━━━━━ 14:37:54\n",
      "Accuracy: 0.9784 - Precision: 0.0004 - Recall: 0.0015 - Specificity: 0.9985 - F1: 0.0006 - Loss: 0.0156\n",
      "\n",
      "Batch 67/298 ━━━━━━━━━━━━━━━━━━━━ 14:38:50\n",
      "Accuracy: 0.9785 - Precision: 0.0004 - Recall: 0.0015 - Specificity: 0.9986 - F1: 0.0006 - Loss: 0.0155\n",
      "\n",
      "Batch 68/298 ━━━━━━━━━━━━━━━━━━━━ 14:39:45\n",
      "Accuracy: 0.9784 - Precision: 0.0004 - Recall: 0.0014 - Specificity: 0.9986 - F1: 0.0006 - Loss: 0.0154\n",
      "\n",
      "Batch 69/298 ━━━━━━━━━━━━━━━━━━━━ 14:40:38\n",
      "Accuracy: 0.9785 - Precision: 0.0004 - Recall: 0.0014 - Specificity: 0.9986 - F1: 0.0006 - Loss: 0.0153\n",
      "\n",
      "Batch 70/298 ━━━━━━━━━━━━━━━━━━━━ 14:41:35\n",
      "Accuracy: 0.9784 - Precision: 0.0004 - Recall: 0.0014 - Specificity: 0.9986 - F1: 0.0006 - Loss: 0.0152\n",
      "\n",
      "Batch 71/298 ━━━━━━━━━━━━━━━━━━━━ 14:42:28\n",
      "Accuracy: 0.9783 - Precision: 0.0004 - Recall: 0.0014 - Specificity: 0.9986 - F1: 0.0006 - Loss: 0.0151\n",
      "\n",
      "Batch 72/298 ━━━━━━━━━━━━━━━━━━━━ 14:43:24\n",
      "Accuracy: 0.9783 - Precision: 0.0004 - Recall: 0.0014 - Specificity: 0.9987 - F1: 0.0006 - Loss: 0.0149\n",
      "\n",
      "Batch 73/298 ━━━━━━━━━━━━━━━━━━━━ 14:44:17\n",
      "Accuracy: 0.9784 - Precision: 0.0004 - Recall: 0.0013 - Specificity: 0.9987 - F1: 0.0006 - Loss: 0.0148\n",
      "\n",
      "Batch 74/298 ━━━━━━━━━━━━━━━━━━━━ 14:45:12\n",
      "Accuracy: 0.9785 - Precision: 0.0004 - Recall: 0.0013 - Specificity: 0.9987 - F1: 0.0006 - Loss: 0.0147\n",
      "\n",
      "Batch 75/298 ━━━━━━━━━━━━━━━━━━━━ 14:46:03\n",
      "Accuracy: 0.9785 - Precision: 0.0004 - Recall: 0.0013 - Specificity: 0.9987 - F1: 0.0006 - Loss: 0.0146\n",
      "\n",
      "Batch 76/298 ━━━━━━━━━━━━━━━━━━━━ 14:46:55\n",
      "Accuracy: 0.9786 - Precision: 0.0004 - Recall: 0.0013 - Specificity: 0.9987 - F1: 0.0006 - Loss: 0.0144\n",
      "\n",
      "Batch 77/298 ━━━━━━━━━━━━━━━━━━━━ 14:47:48\n",
      "Accuracy: 0.9786 - Precision: 0.0004 - Recall: 0.0013 - Specificity: 0.9987 - F1: 0.0006 - Loss: 0.0143\n",
      "\n",
      "Batch 78/298 ━━━━━━━━━━━━━━━━━━━━ 14:48:39\n",
      "Accuracy: 0.9785 - Precision: 0.0004 - Recall: 0.0012 - Specificity: 0.9988 - F1: 0.0005 - Loss: 0.0142\n",
      "\n",
      "Batch 79/298 ━━━━━━━━━━━━━━━━━━━━ 14:49:56\n",
      "Accuracy: 0.9786 - Precision: 0.0003 - Recall: 0.0012 - Specificity: 0.9988 - F1: 0.0005 - Loss: 0.0141\n",
      "\n",
      "Batch 80/298 ━━━━━━━━━━━━━━━━━━━━ 14:50:57\n",
      "Accuracy: 0.9785 - Precision: 0.0003 - Recall: 0.0012 - Specificity: 0.9988 - F1: 0.0005 - Loss: 0.0140\n",
      "\n",
      "Batch 81/298 ━━━━━━━━━━━━━━━━━━━━ 14:51:58\n",
      "Accuracy: 0.9785 - Precision: 0.0003 - Recall: 0.0012 - Specificity: 0.9988 - F1: 0.0005 - Loss: 0.0139\n",
      "\n",
      "Batch 82/298 ━━━━━━━━━━━━━━━━━━━━ 14:52:53\n",
      "Accuracy: 0.9785 - Precision: 0.0003 - Recall: 0.0012 - Specificity: 0.9988 - F1: 0.0005 - Loss: 0.0139\n",
      "\n",
      "Batch 83/298 ━━━━━━━━━━━━━━━━━━━━ 14:53:51\n",
      "Accuracy: 0.9785 - Precision: 0.0003 - Recall: 0.0012 - Specificity: 0.9988 - F1: 0.0005 - Loss: 0.0137\n",
      "\n",
      "Batch 84/298 ━━━━━━━━━━━━━━━━━━━━ 14:54:45\n",
      "Accuracy: 0.9785 - Precision: 0.0003 - Recall: 0.0012 - Specificity: 0.9989 - F1: 0.0005 - Loss: 0.0137\n",
      "\n",
      "Batch 85/298 ━━━━━━━━━━━━━━━━━━━━ 14:55:39\n",
      "Accuracy: 0.9786 - Precision: 0.0003 - Recall: 0.0011 - Specificity: 0.9989 - F1: 0.0005 - Loss: 0.0136\n",
      "\n",
      "Batch 86/298 ━━━━━━━━━━━━━━━━━━━━ 14:56:30\n",
      "Accuracy: 0.9786 - Precision: 0.0003 - Recall: 0.0011 - Specificity: 0.9989 - F1: 0.0005 - Loss: 0.0135\n",
      "\n",
      "Batch 87/298 ━━━━━━━━━━━━━━━━━━━━ 14:57:37\n",
      "Accuracy: 0.9785 - Precision: 0.0003 - Recall: 0.0011 - Specificity: 0.9989 - F1: 0.0005 - Loss: 0.0134\n",
      "\n",
      "Batch 88/298 ━━━━━━━━━━━━━━━━━━━━ 14:58:28\n",
      "Accuracy: 0.9786 - Precision: 0.0003 - Recall: 0.0011 - Specificity: 0.9989 - F1: 0.0005 - Loss: 0.0133\n",
      "\n",
      "Batch 89/298 ━━━━━━━━━━━━━━━━━━━━ 14:59:26\n",
      "Accuracy: 0.9786 - Precision: 0.0003 - Recall: 0.0011 - Specificity: 0.9989 - F1: 0.0005 - Loss: 0.0132\n",
      "\n",
      "Batch 90/298 ━━━━━━━━━━━━━━━━━━━━ 15:00:19\n",
      "Accuracy: 0.9786 - Precision: 0.0003 - Recall: 0.0011 - Specificity: 0.9989 - F1: 0.0005 - Loss: 0.0131\n",
      "\n",
      "Batch 91/298 ━━━━━━━━━━━━━━━━━━━━ 15:01:11\n",
      "Accuracy: 0.9787 - Precision: 0.0003 - Recall: 0.0011 - Specificity: 0.9989 - F1: 0.0005 - Loss: 0.0130\n",
      "\n",
      "Batch 92/298 ━━━━━━━━━━━━━━━━━━━━ 15:02:05\n",
      "Accuracy: 0.9788 - Precision: 0.0003 - Recall: 0.0011 - Specificity: 0.9990 - F1: 0.0005 - Loss: 0.0129\n",
      "\n",
      "Batch 93/298 ━━━━━━━━━━━━━━━━━━━━ 15:03:08\n",
      "Accuracy: 0.9788 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0005 - Loss: 0.0128\n",
      "\n",
      "Batch 94/298 ━━━━━━━━━━━━━━━━━━━━ 15:04:03\n",
      "Accuracy: 0.9789 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0005 - Loss: 0.0128\n",
      "\n",
      "Batch 95/298 ━━━━━━━━━━━━━━━━━━━━ 15:05:08\n",
      "Accuracy: 0.9789 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0005 - Loss: 0.0127\n",
      "\n",
      "Batch 96/298 ━━━━━━━━━━━━━━━━━━━━ 15:06:12\n",
      "Accuracy: 0.9789 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0004 - Loss: 0.0126\n",
      "\n",
      "Batch 97/298 ━━━━━━━━━━━━━━━━━━━━ 15:07:16\n",
      "Accuracy: 0.9790 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0004 - Loss: 0.0125\n",
      "\n",
      "Batch 98/298 ━━━━━━━━━━━━━━━━━━━━ 15:08:12\n",
      "Accuracy: 0.9790 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0004 - Loss: 0.0124\n",
      "\n",
      "Batch 99/298 ━━━━━━━━━━━━━━━━━━━━ 15:09:13\n",
      "Accuracy: 0.9791 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0004 - Loss: 0.0124\n",
      "\n",
      "Batch 100/298 ━━━━━━━━━━━━━━━━━━━━ 15:10:08\n",
      "Accuracy: 0.9791 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0004 - Loss: 0.0123\n",
      "\n",
      "Batch 101/298 ━━━━━━━━━━━━━━━━━━━━ 15:11:00\n",
      "Accuracy: 0.9791 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9990 - F1: 0.0004 - Loss: 0.0122\n",
      "\n",
      "Batch 102/298 ━━━━━━━━━━━━━━━━━━━━ 15:11:53\n",
      "Accuracy: 0.9791 - Precision: 0.0003 - Recall: 0.0010 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0121\n",
      "\n",
      "Batch 103/298 ━━━━━━━━━━━━━━━━━━━━ 15:12:45\n",
      "Accuracy: 0.9792 - Precision: 0.0003 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0121\n",
      "\n",
      "Batch 104/298 ━━━━━━━━━━━━━━━━━━━━ 15:13:41\n",
      "Accuracy: 0.9792 - Precision: 0.0003 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0120\n",
      "\n",
      "Batch 105/298 ━━━━━━━━━━━━━━━━━━━━ 15:14:36\n",
      "Accuracy: 0.9792 - Precision: 0.0003 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0119\n",
      "\n",
      "Batch 106/298 ━━━━━━━━━━━━━━━━━━━━ 15:15:31\n",
      "Accuracy: 0.9792 - Precision: 0.0003 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0119\n",
      "\n",
      "Batch 107/298 ━━━━━━━━━━━━━━━━━━━━ 15:16:21\n",
      "Accuracy: 0.9792 - Precision: 0.0003 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0118\n",
      "\n",
      "Batch 108/298 ━━━━━━━━━━━━━━━━━━━━ 15:17:13\n",
      "Accuracy: 0.9792 - Precision: 0.0003 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0118\n",
      "\n",
      "Batch 109/298 ━━━━━━━━━━━━━━━━━━━━ 15:18:04\n",
      "Accuracy: 0.9792 - Precision: 0.0003 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0117\n",
      "\n",
      "Batch 110/298 ━━━━━━━━━━━━━━━━━━━━ 15:18:53\n",
      "Accuracy: 0.9792 - Precision: 0.0002 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0117\n",
      "\n",
      "Batch 111/298 ━━━━━━━━━━━━━━━━━━━━ 15:19:46\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0116\n",
      "\n",
      "Batch 112/298 ━━━━━━━━━━━━━━━━━━━━ 15:20:35\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0115\n",
      "\n",
      "Batch 113/298 ━━━━━━━━━━━━━━━━━━━━ 15:21:39\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0009 - Specificity: 0.9991 - F1: 0.0004 - Loss: 0.0115\n",
      "\n",
      "Batch 114/298 ━━━━━━━━━━━━━━━━━━━━ 15:22:41\n",
      "Accuracy: 0.9792 - Precision: 0.0002 - Recall: 0.0009 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0115\n",
      "\n",
      "Batch 115/298 ━━━━━━━━━━━━━━━━━━━━ 15:23:39\n",
      "Accuracy: 0.9792 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0114\n",
      "\n",
      "Batch 116/298 ━━━━━━━━━━━━━━━━━━━━ 15:24:51\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0113\n",
      "\n",
      "Batch 117/298 ━━━━━━━━━━━━━━━━━━━━ 15:26:01\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0113\n",
      "\n",
      "Batch 118/298 ━━━━━━━━━━━━━━━━━━━━ 15:26:56\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0112\n",
      "\n",
      "Batch 119/298 ━━━━━━━━━━━━━━━━━━━━ 15:27:56\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0112\n",
      "\n",
      "Batch 120/298 ━━━━━━━━━━━━━━━━━━━━ 15:28:51\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0112\n",
      "\n",
      "Batch 121/298 ━━━━━━━━━━━━━━━━━━━━ 15:30:02\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0111\n",
      "\n",
      "Batch 122/298 ━━━━━━━━━━━━━━━━━━━━ 15:30:58\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0004 - Loss: 0.0111\n",
      "\n",
      "Batch 123/298 ━━━━━━━━━━━━━━━━━━━━ 15:31:54\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0003 - Loss: 0.0110\n",
      "\n",
      "Batch 124/298 ━━━━━━━━━━━━━━━━━━━━ 15:32:47\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0003 - Loss: 0.0110\n",
      "\n",
      "Batch 125/298 ━━━━━━━━━━━━━━━━━━━━ 15:33:41\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0003 - Loss: 0.0109\n",
      "\n",
      "Batch 126/298 ━━━━━━━━━━━━━━━━━━━━ 15:34:37\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0003 - Loss: 0.0109\n",
      "\n",
      "Batch 127/298 ━━━━━━━━━━━━━━━━━━━━ 15:35:39\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0003 - Loss: 0.0109\n",
      "\n",
      "Batch 128/298 ━━━━━━━━━━━━━━━━━━━━ 15:36:32\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9992 - F1: 0.0003 - Loss: 0.0108\n",
      "\n",
      "Batch 129/298 ━━━━━━━━━━━━━━━━━━━━ 15:37:27\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0008 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0108\n",
      "\n",
      "Batch 130/298 ━━━━━━━━━━━━━━━━━━━━ 15:38:19\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0107\n",
      "\n",
      "Batch 131/298 ━━━━━━━━━━━━━━━━━━━━ 15:39:12\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0107\n",
      "\n",
      "Batch 132/298 ━━━━━━━━━━━━━━━━━━━━ 15:40:05\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0106\n",
      "\n",
      "Batch 133/298 ━━━━━━━━━━━━━━━━━━━━ 15:40:58\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0106\n",
      "\n",
      "Batch 134/298 ━━━━━━━━━━━━━━━━━━━━ 15:41:53\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0106\n",
      "\n",
      "Batch 135/298 ━━━━━━━━━━━━━━━━━━━━ 15:42:45\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0105\n",
      "\n",
      "Batch 136/298 ━━━━━━━━━━━━━━━━━━━━ 15:43:38\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0105\n",
      "\n",
      "Batch 137/298 ━━━━━━━━━━━━━━━━━━━━ 15:44:32\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0105\n",
      "\n",
      "Batch 138/298 ━━━━━━━━━━━━━━━━━━━━ 15:45:25\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0104\n",
      "\n",
      "Batch 139/298 ━━━━━━━━━━━━━━━━━━━━ 15:46:18\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0104\n",
      "\n",
      "Batch 140/298 ━━━━━━━━━━━━━━━━━━━━ 15:47:10\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0104\n",
      "\n",
      "Batch 141/298 ━━━━━━━━━━━━━━━━━━━━ 15:48:04\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0104\n",
      "\n",
      "Batch 142/298 ━━━━━━━━━━━━━━━━━━━━ 15:48:55\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0103\n",
      "\n",
      "Batch 143/298 ━━━━━━━━━━━━━━━━━━━━ 15:49:51\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0103\n",
      "\n",
      "Batch 144/298 ━━━━━━━━━━━━━━━━━━━━ 15:50:42\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0103\n",
      "\n",
      "Batch 145/298 ━━━━━━━━━━━━━━━━━━━━ 15:51:46\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0102\n",
      "\n",
      "Batch 146/298 ━━━━━━━━━━━━━━━━━━━━ 15:52:40\n",
      "Accuracy: 0.9792 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0102\n",
      "\n",
      "Batch 147/298 ━━━━━━━━━━━━━━━━━━━━ 15:53:34\n",
      "Accuracy: 0.9792 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0102\n",
      "\n",
      "Batch 148/298 ━━━━━━━━━━━━━━━━━━━━ 15:54:26\n",
      "Accuracy: 0.9792 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9993 - F1: 0.0003 - Loss: 0.0101\n",
      "\n",
      "Batch 149/298 ━━━━━━━━━━━━━━━━━━━━ 15:55:18\n",
      "Accuracy: 0.9792 - Precision: 0.0002 - Recall: 0.0007 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0101\n",
      "\n",
      "Batch 150/298 ━━━━━━━━━━━━━━━━━━━━ 15:56:11\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0101\n",
      "\n",
      "Batch 151/298 ━━━━━━━━━━━━━━━━━━━━ 15:57:03\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0100\n",
      "\n",
      "Batch 152/298 ━━━━━━━━━━━━━━━━━━━━ 15:57:58\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0100\n",
      "\n",
      "Batch 153/298 ━━━━━━━━━━━━━━━━━━━━ 15:58:51\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0100\n",
      "\n",
      "Batch 154/298 ━━━━━━━━━━━━━━━━━━━━ 15:59:46\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0099\n",
      "\n",
      "Batch 155/298 ━━━━━━━━━━━━━━━━━━━━ 16:00:40\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0099\n",
      "\n",
      "Batch 156/298 ━━━━━━━━━━━━━━━━━━━━ 16:01:35\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0099\n",
      "\n",
      "Batch 157/298 ━━━━━━━━━━━━━━━━━━━━ 16:02:27\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0099\n",
      "\n",
      "Batch 158/298 ━━━━━━━━━━━━━━━━━━━━ 16:03:37\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0098\n",
      "\n",
      "Batch 159/298 ━━━━━━━━━━━━━━━━━━━━ 16:04:41\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0098\n",
      "\n",
      "Batch 160/298 ━━━━━━━━━━━━━━━━━━━━ 16:05:36\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0098\n",
      "\n",
      "Batch 161/298 ━━━━━━━━━━━━━━━━━━━━ 16:06:29\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0098\n",
      "\n",
      "Batch 162/298 ━━━━━━━━━━━━━━━━━━━━ 16:07:24\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0097\n",
      "\n",
      "Batch 163/298 ━━━━━━━━━━━━━━━━━━━━ 16:08:16\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0097\n",
      "\n",
      "Batch 164/298 ━━━━━━━━━━━━━━━━━━━━ 16:09:08\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0097\n",
      "\n",
      "Batch 165/298 ━━━━━━━━━━━━━━━━━━━━ 16:10:02\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0097\n",
      "\n",
      "Batch 166/298 ━━━━━━━━━━━━━━━━━━━━ 16:10:57\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0096\n",
      "\n",
      "Batch 167/298 ━━━━━━━━━━━━━━━━━━━━ 16:11:53\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0096\n",
      "\n",
      "Batch 168/298 ━━━━━━━━━━━━━━━━━━━━ 16:12:45\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0096\n",
      "\n",
      "Batch 169/298 ━━━━━━━━━━━━━━━━━━━━ 16:13:39\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0096\n",
      "\n",
      "Batch 170/298 ━━━━━━━━━━━━━━━━━━━━ 16:14:32\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0095\n",
      "\n",
      "Batch 171/298 ━━━━━━━━━━━━━━━━━━━━ 16:15:26\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0003 - Loss: 0.0095\n",
      "\n",
      "Batch 172/298 ━━━━━━━━━━━━━━━━━━━━ 16:16:18\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0002 - Loss: 0.0095\n",
      "\n",
      "Batch 173/298 ━━━━━━━━━━━━━━━━━━━━ 16:17:12\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0002 - Loss: 0.0095\n",
      "\n",
      "Batch 174/298 ━━━━━━━━━━━━━━━━━━━━ 16:18:06\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0002 - Loss: 0.0094\n",
      "\n",
      "Batch 175/298 ━━━━━━━━━━━━━━━━━━━━ 16:18:57\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9994 - F1: 0.0002 - Loss: 0.0094\n",
      "\n",
      "Batch 176/298 ━━━━━━━━━━━━━━━━━━━━ 16:19:51\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0094\n",
      "\n",
      "Batch 177/298 ━━━━━━━━━━━━━━━━━━━━ 16:20:44\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0006 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0094\n",
      "\n",
      "Batch 178/298 ━━━━━━━━━━━━━━━━━━━━ 16:21:49\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0094\n",
      "\n",
      "Batch 179/298 ━━━━━━━━━━━━━━━━━━━━ 16:22:42\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0093\n",
      "\n",
      "Batch 180/298 ━━━━━━━━━━━━━━━━━━━━ 16:23:37\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0093\n",
      "\n",
      "Batch 181/298 ━━━━━━━━━━━━━━━━━━━━ 16:24:32\n",
      "Accuracy: 0.9794 - Precision: 0.0002 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0093\n",
      "\n",
      "Batch 182/298 ━━━━━━━━━━━━━━━━━━━━ 16:25:26\n",
      "Accuracy: 0.9793 - Precision: 0.0002 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0093\n",
      "\n",
      "Batch 183/298 ━━━━━━━━━━━━━━━━━━━━ 16:26:18\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0093\n",
      "\n",
      "Batch 184/298 ━━━━━━━━━━━━━━━━━━━━ 16:27:10\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0092\n",
      "\n",
      "Batch 185/298 ━━━━━━━━━━━━━━━━━━━━ 16:28:06\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0092\n",
      "\n",
      "Batch 186/298 ━━━━━━━━━━━━━━━━━━━━ 16:28:57\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0092\n",
      "\n",
      "Batch 187/298 ━━━━━━━━━━━━━━━━━━━━ 16:29:51\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0092\n",
      "\n",
      "Batch 188/298 ━━━━━━━━━━━━━━━━━━━━ 16:30:43\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0092\n",
      "\n",
      "Batch 189/298 ━━━━━━━━━━━━━━━━━━━━ 16:31:36\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0092\n",
      "\n",
      "Batch 190/298 ━━━━━━━━━━━━━━━━━━━━ 16:32:29\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 191/298 ━━━━━━━━━━━━━━━━━━━━ 16:33:25\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 192/298 ━━━━━━━━━━━━━━━━━━━━ 16:34:18\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 193/298 ━━━━━━━━━━━━━━━━━━━━ 16:35:13\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 194/298 ━━━━━━━━━━━━━━━━━━━━ 16:36:14\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 195/298 ━━━━━━━━━━━━━━━━━━━━ 16:37:06\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 196/298 ━━━━━━━━━━━━━━━━━━━━ 16:38:02\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 197/298 ━━━━━━━━━━━━━━━━━━━━ 16:38:54\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 198/298 ━━━━━━━━━━━━━━━━━━━━ 16:39:48\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 199/298 ━━━━━━━━━━━━━━━━━━━━ 16:40:41\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 200/298 ━━━━━━━━━━━━━━━━━━━━ 16:41:37\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 201/298 ━━━━━━━━━━━━━━━━━━━━ 16:42:29\n",
      "Accuracy: 0.9796 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 202/298 ━━━━━━━━━━━━━━━━━━━━ 16:43:25\n",
      "Accuracy: 0.9796 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 203/298 ━━━━━━━━━━━━━━━━━━━━ 16:44:21\n",
      "Accuracy: 0.9796 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 204/298 ━━━━━━━━━━━━━━━━━━━━ 16:45:13\n",
      "Accuracy: 0.9796 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 205/298 ━━━━━━━━━━━━━━━━━━━━ 16:46:07\n",
      "Accuracy: 0.9796 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 206/298 ━━━━━━━━━━━━━━━━━━━━ 16:46:59\n",
      "Accuracy: 0.9796 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 207/298 ━━━━━━━━━━━━━━━━━━━━ 16:47:56\n",
      "Accuracy: 0.9796 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 208/298 ━━━━━━━━━━━━━━━━━━━━ 16:48:48\n",
      "Accuracy: 0.9797 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 209/298 ━━━━━━━━━━━━━━━━━━━━ 16:49:43\n",
      "Accuracy: 0.9797 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 210/298 ━━━━━━━━━━━━━━━━━━━━ 16:50:34\n",
      "Accuracy: 0.9796 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 211/298 ━━━━━━━━━━━━━━━━━━━━ 16:51:27\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 212/298 ━━━━━━━━━━━━━━━━━━━━ 16:52:20\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 213/298 ━━━━━━━━━━━━━━━━━━━━ 16:53:12\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 214/298 ━━━━━━━━━━━━━━━━━━━━ 16:54:06\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9995 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 215/298 ━━━━━━━━━━━━━━━━━━━━ 16:54:59\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 216/298 ━━━━━━━━━━━━━━━━━━━━ 16:55:53\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0005 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 217/298 ━━━━━━━━━━━━━━━━━━━━ 16:56:46\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 218/298 ━━━━━━━━━━━━━━━━━━━━ 16:57:40\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 219/298 ━━━━━━━━━━━━━━━━━━━━ 16:58:31\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 220/298 ━━━━━━━━━━━━━━━━━━━━ 16:59:27\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0091\n",
      "\n",
      "Batch 221/298 ━━━━━━━━━━━━━━━━━━━━ 17:00:18\n",
      "Accuracy: 0.9792 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 222/298 ━━━━━━━━━━━━━━━━━━━━ 17:01:09\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 223/298 ━━━━━━━━━━━━━━━━━━━━ 17:02:04\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 224/298 ━━━━━━━━━━━━━━━━━━━━ 17:02:55\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 225/298 ━━━━━━━━━━━━━━━━━━━━ 17:03:48\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 226/298 ━━━━━━━━━━━━━━━━━━━━ 17:04:41\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 227/298 ━━━━━━━━━━━━━━━━━━━━ 17:05:34\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0090\n",
      "\n",
      "Batch 228/298 ━━━━━━━━━━━━━━━━━━━━ 17:06:30\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 229/298 ━━━━━━━━━━━━━━━━━━━━ 17:07:21\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 230/298 ━━━━━━━━━━━━━━━━━━━━ 17:08:14\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 231/298 ━━━━━━━━━━━━━━━━━━━━ 17:09:06\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 232/298 ━━━━━━━━━━━━━━━━━━━━ 17:10:13\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 233/298 ━━━━━━━━━━━━━━━━━━━━ 17:11:05\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 234/298 ━━━━━━━━━━━━━━━━━━━━ 17:12:03\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 235/298 ━━━━━━━━━━━━━━━━━━━━ 17:12:57\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0089\n",
      "\n",
      "Batch 236/298 ━━━━━━━━━━━━━━━━━━━━ 17:13:53\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 237/298 ━━━━━━━━━━━━━━━━━━━━ 17:14:49\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 238/298 ━━━━━━━━━━━━━━━━━━━━ 17:15:47\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 239/298 ━━━━━━━━━━━━━━━━━━━━ 17:16:39\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 240/298 ━━━━━━━━━━━━━━━━━━━━ 17:17:36\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 241/298 ━━━━━━━━━━━━━━━━━━━━ 17:18:50\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 242/298 ━━━━━━━━━━━━━━━━━━━━ 17:19:46\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 243/298 ━━━━━━━━━━━━━━━━━━━━ 17:20:39\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0088\n",
      "\n",
      "Batch 244/298 ━━━━━━━━━━━━━━━━━━━━ 17:21:34\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 245/298 ━━━━━━━━━━━━━━━━━━━━ 17:22:26\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 246/298 ━━━━━━━━━━━━━━━━━━━━ 17:23:23\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 247/298 ━━━━━━━━━━━━━━━━━━━━ 17:24:21\n",
      "Accuracy: 0.9795 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 248/298 ━━━━━━━━━━━━━━━━━━━━ 17:25:14\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 249/298 ━━━━━━━━━━━━━━━━━━━━ 17:26:10\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 250/298 ━━━━━━━━━━━━━━━━━━━━ 17:27:01\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 251/298 ━━━━━━━━━━━━━━━━━━━━ 17:27:55\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 252/298 ━━━━━━━━━━━━━━━━━━━━ 17:28:46\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0087\n",
      "\n",
      "Batch 253/298 ━━━━━━━━━━━━━━━━━━━━ 17:29:43\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 254/298 ━━━━━━━━━━━━━━━━━━━━ 17:30:34\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 255/298 ━━━━━━━━━━━━━━━━━━━━ 17:31:27\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 256/298 ━━━━━━━━━━━━━━━━━━━━ 17:32:19\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 257/298 ━━━━━━━━━━━━━━━━━━━━ 17:33:12\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 258/298 ━━━━━━━━━━━━━━━━━━━━ 17:34:06\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 259/298 ━━━━━━━━━━━━━━━━━━━━ 17:34:57\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 260/298 ━━━━━━━━━━━━━━━━━━━━ 17:35:52\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 261/298 ━━━━━━━━━━━━━━━━━━━━ 17:36:45\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 262/298 ━━━━━━━━━━━━━━━━━━━━ 17:37:41\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 263/298 ━━━━━━━━━━━━━━━━━━━━ 17:38:33\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0086\n",
      "\n",
      "Batch 264/298 ━━━━━━━━━━━━━━━━━━━━ 17:39:26\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 265/298 ━━━━━━━━━━━━━━━━━━━━ 17:40:19\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 266/298 ━━━━━━━━━━━━━━━━━━━━ 17:41:11\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 267/298 ━━━━━━━━━━━━━━━━━━━━ 17:42:05\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 268/298 ━━━━━━━━━━━━━━━━━━━━ 17:43:08\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 269/298 ━━━━━━━━━━━━━━━━━━━━ 17:44:12\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 270/298 ━━━━━━━━━━━━━━━━━━━━ 17:45:06\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 271/298 ━━━━━━━━━━━━━━━━━━━━ 17:46:20\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 272/298 ━━━━━━━━━━━━━━━━━━━━ 17:47:11\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 273/298 ━━━━━━━━━━━━━━━━━━━━ 17:48:05\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 274/298 ━━━━━━━━━━━━━━━━━━━━ 17:48:59\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0085\n",
      "\n",
      "Batch 275/298 ━━━━━━━━━━━━━━━━━━━━ 17:49:53\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9996 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 276/298 ━━━━━━━━━━━━━━━━━━━━ 17:50:44\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 277/298 ━━━━━━━━━━━━━━━━━━━━ 17:51:38\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 278/298 ━━━━━━━━━━━━━━━━━━━━ 17:52:29\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0004 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 279/298 ━━━━━━━━━━━━━━━━━━━━ 17:53:22\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 280/298 ━━━━━━━━━━━━━━━━━━━━ 17:54:15\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 281/298 ━━━━━━━━━━━━━━━━━━━━ 17:55:08\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 282/298 ━━━━━━━━━━━━━━━━━━━━ 17:56:04\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 283/298 ━━━━━━━━━━━━━━━━━━━━ 17:56:55\n",
      "Accuracy: 0.9794 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 284/298 ━━━━━━━━━━━━━━━━━━━━ 17:57:49\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 285/298 ━━━━━━━━━━━━━━━━━━━━ 17:58:40\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0002 - Loss: 0.0084\n",
      "\n",
      "Batch 286/298 ━━━━━━━━━━━━━━━━━━━━ 17:59:34\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0084\n",
      "\n",
      "Batch 287/298 ━━━━━━━━━━━━━━━━━━━━ 18:00:28\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0084\n",
      "\n",
      "Batch 288/298 ━━━━━━━━━━━━━━━━━━━━ 18:01:22\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0084\n",
      "\n",
      "Batch 289/298 ━━━━━━━━━━━━━━━━━━━━ 18:02:21\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0084\n",
      "\n",
      "Batch 290/298 ━━━━━━━━━━━━━━━━━━━━ 18:03:14\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0084\n",
      "\n",
      "Batch 291/298 ━━━━━━━━━━━━━━━━━━━━ 18:04:10\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0084\n",
      "\n",
      "Batch 292/298 ━━━━━━━━━━━━━━━━━━━━ 18:05:03\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0083\n",
      "\n",
      "Batch 293/298 ━━━━━━━━━━━━━━━━━━━━ 18:06:02\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0083\n",
      "\n",
      "Batch 294/298 ━━━━━━━━━━━━━━━━━━━━ 18:06:55\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0083\n",
      "\n",
      "Batch 295/298 ━━━━━━━━━━━━━━━━━━━━ 18:07:49\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0083\n",
      "\n",
      "Batch 296/298 ━━━━━━━━━━━━━━━━━━━━ 18:08:42\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0083\n",
      "\n",
      "Batch 297/298 ━━━━━━━━━━━━━━━━━━━━ 18:09:36\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0083\n",
      "\n",
      "Batch 298/298 ━━━━━━━━━━━━━━━━━━━━ 18:10:29\n",
      "Accuracy: 0.9793 - Precision: 0.0001 - Recall: 0.0003 - Specificity: 0.9997 - F1: 0.0001 - Loss: 0.0083\n",
      "\n",
      "Epoch 1/20\n",
      "Validation - Accuracy: 0.9808, Precision: 0.0000, Recall: 0.0000, Specificity: 1.0000, F1: 0.0000, Loss: 0.0060\n",
      "\n",
      "\n",
      "Epoch 2/20\n",
      "Batch 1/298 ━━━━━━━━━━━━━━━━━━━━ 18:34:41\n",
      "Accuracy: 0.9841 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 2/298 ━━━━━━━━━━━━━━━━━━━━ 18:35:35\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 3/298 ━━━━━━━━━━━━━━━━━━━━ 18:36:34\n",
      "Accuracy: 0.9815 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 4/298 ━━━━━━━━━━━━━━━━━━━━ 18:37:27\n",
      "Accuracy: 0.9813 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 5/298 ━━━━━━━━━━━━━━━━━━━━ 18:38:22\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 6/298 ━━━━━━━━━━━━━━━━━━━━ 18:39:13\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 7/298 ━━━━━━━━━━━━━━━━━━━━ 18:40:06\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 8/298 ━━━━━━━━━━━━━━━━━━━━ 18:40:56\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 9/298 ━━━━━━━━━━━━━━━━━━━━ 18:41:49\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 10/298 ━━━━━━━━━━━━━━━━━━━━ 18:42:38\n",
      "Accuracy: 0.9813 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 11/298 ━━━━━━━━━━━━━━━━━━━━ 18:43:34\n",
      "Accuracy: 0.9815 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 12/298 ━━━━━━━━━━━━━━━━━━━━ 18:44:28\n",
      "Accuracy: 0.9820 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 13/298 ━━━━━━━━━━━━━━━━━━━━ 18:45:20\n",
      "Accuracy: 0.9818 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 14/298 ━━━━━━━━━━━━━━━━━━━━ 18:46:14\n",
      "Accuracy: 0.9816 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 15/298 ━━━━━━━━━━━━━━━━━━━━ 18:47:06\n",
      "Accuracy: 0.9818 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 16/298 ━━━━━━━━━━━━━━━━━━━━ 18:48:01\n",
      "Accuracy: 0.9818 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 17/298 ━━━━━━━━━━━━━━━━━━━━ 18:48:54\n",
      "Accuracy: 0.9819 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 18/298 ━━━━━━━━━━━━━━━━━━━━ 18:49:46\n",
      "Accuracy: 0.9818 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 19/298 ━━━━━━━━━━━━━━━━━━━━ 18:50:37\n",
      "Accuracy: 0.9819 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 20/298 ━━━━━━━━━━━━━━━━━━━━ 18:51:29\n",
      "Accuracy: 0.9819 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 21/298 ━━━━━━━━━━━━━━━━━━━━ 18:52:21\n",
      "Accuracy: 0.9819 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 22/298 ━━━━━━━━━━━━━━━━━━━━ 18:53:13\n",
      "Accuracy: 0.9820 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0051\n",
      "\n",
      "Batch 23/298 ━━━━━━━━━━━━━━━━━━━━ 18:54:06\n",
      "Accuracy: 0.9818 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 24/298 ━━━━━━━━━━━━━━━━━━━━ 18:54:57\n",
      "Accuracy: 0.9817 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 25/298 ━━━━━━━━━━━━━━━━━━━━ 18:55:49\n",
      "Accuracy: 0.9818 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 26/298 ━━━━━━━━━━━━━━━━━━━━ 18:56:41\n",
      "Accuracy: 0.9816 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 27/298 ━━━━━━━━━━━━━━━━━━━━ 18:57:33\n",
      "Accuracy: 0.9816 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 28/298 ━━━━━━━━━━━━━━━━━━━━ 18:58:27\n",
      "Accuracy: 0.9814 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 29/298 ━━━━━━━━━━━━━━━━━━━━ 18:59:18\n",
      "Accuracy: 0.9816 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 30/298 ━━━━━━━━━━━━━━━━━━━━ 19:00:12\n",
      "Accuracy: 0.9816 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 31/298 ━━━━━━━━━━━━━━━━━━━━ 19:01:04\n",
      "Accuracy: 0.9815 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0052\n",
      "\n",
      "Batch 32/298 ━━━━━━━━━━━━━━━━━━━━ 19:02:00\n",
      "Accuracy: 0.9812 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 33/298 ━━━━━━━━━━━━━━━━━━━━ 19:02:52\n",
      "Accuracy: 0.9812 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 34/298 ━━━━━━━━━━━━━━━━━━━━ 19:03:45\n",
      "Accuracy: 0.9813 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 35/298 ━━━━━━━━━━━━━━━━━━━━ 19:04:40\n",
      "Accuracy: 0.9812 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 36/298 ━━━━━━━━━━━━━━━━━━━━ 19:05:33\n",
      "Accuracy: 0.9812 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 37/298 ━━━━━━━━━━━━━━━━━━━━ 19:06:30\n",
      "Accuracy: 0.9812 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 38/298 ━━━━━━━━━━━━━━━━━━━━ 19:07:21\n",
      "Accuracy: 0.9812 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0053\n",
      "\n",
      "Batch 39/298 ━━━━━━━━━━━━━━━━━━━━ 19:08:14\n",
      "Accuracy: 0.9812 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 40/298 ━━━━━━━━━━━━━━━━━━━━ 19:09:06\n",
      "Accuracy: 0.9810 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 41/298 ━━━━━━━━━━━━━━━━━━━━ 19:09:59\n",
      "Accuracy: 0.9810 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 42/298 ━━━━━━━━━━━━━━━━━━━━ 19:10:51\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 43/298 ━━━━━━━━━━━━━━━━━━━━ 19:11:42\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 44/298 ━━━━━━━━━━━━━━━━━━━━ 19:12:36\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 45/298 ━━━━━━━━━━━━━━━━━━━━ 19:13:27\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 46/298 ━━━━━━━━━━━━━━━━━━━━ 19:14:22\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 47/298 ━━━━━━━━━━━━━━━━━━━━ 19:15:15\n",
      "Accuracy: 0.9807 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 48/298 ━━━━━━━━━━━━━━━━━━━━ 19:16:08\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 49/298 ━━━━━━━━━━━━━━━━━━━━ 19:16:59\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 50/298 ━━━━━━━━━━━━━━━━━━━━ 19:17:55\n",
      "Accuracy: 0.9807 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 51/298 ━━━━━━━━━━━━━━━━━━━━ 19:18:47\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 52/298 ━━━━━━━━━━━━━━━━━━━━ 19:19:39\n",
      "Accuracy: 0.9805 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 53/298 ━━━━━━━━━━━━━━━━━━━━ 19:20:34\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 54/298 ━━━━━━━━━━━━━━━━━━━━ 19:21:25\n",
      "Accuracy: 0.9805 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 55/298 ━━━━━━━━━━━━━━━━━━━━ 19:22:18\n",
      "Accuracy: 0.9805 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 56/298 ━━━━━━━━━━━━━━━━━━━━ 19:23:08\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 57/298 ━━━━━━━━━━━━━━━━━━━━ 19:24:06\n",
      "Accuracy: 0.9805 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 58/298 ━━━━━━━━━━━━━━━━━━━━ 19:25:01\n",
      "Accuracy: 0.9805 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 59/298 ━━━━━━━━━━━━━━━━━━━━ 19:25:53\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 60/298 ━━━━━━━━━━━━━━━━━━━━ 19:26:46\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 61/298 ━━━━━━━━━━━━━━━━━━━━ 19:27:37\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 62/298 ━━━━━━━━━━━━━━━━━━━━ 19:28:30\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 63/298 ━━━━━━━━━━━━━━━━━━━━ 19:29:21\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 64/298 ━━━━━━━━━━━━━━━━━━━━ 19:30:14\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 65/298 ━━━━━━━━━━━━━━━━━━━━ 19:31:05\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 66/298 ━━━━━━━━━━━━━━━━━━━━ 19:31:56\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 67/298 ━━━━━━━━━━━━━━━━━━━━ 19:32:48\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 68/298 ━━━━━━━━━━━━━━━━━━━━ 19:33:40\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 69/298 ━━━━━━━━━━━━━━━━━━━━ 19:34:36\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 70/298 ━━━━━━━━━━━━━━━━━━━━ 19:35:27\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 71/298 ━━━━━━━━━━━━━━━━━━━━ 19:36:21\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 72/298 ━━━━━━━━━━━━━━━━━━━━ 19:37:13\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 73/298 ━━━━━━━━━━━━━━━━━━━━ 19:38:06\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 74/298 ━━━━━━━━━━━━━━━━━━━━ 19:38:58\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 75/298 ━━━━━━━━━━━━━━━━━━━━ 19:39:51\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 76/298 ━━━━━━━━━━━━━━━━━━━━ 19:40:53\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 77/298 ━━━━━━━━━━━━━━━━━━━━ 19:41:48\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 78/298 ━━━━━━━━━━━━━━━━━━━━ 19:42:43\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 79/298 ━━━━━━━━━━━━━━━━━━━━ 19:43:36\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 80/298 ━━━━━━━━━━━━━━━━━━━━ 19:44:32\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 81/298 ━━━━━━━━━━━━━━━━━━━━ 19:45:25\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 82/298 ━━━━━━━━━━━━━━━━━━━━ 19:46:20\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 83/298 ━━━━━━━━━━━━━━━━━━━━ 19:47:11\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 84/298 ━━━━━━━━━━━━━━━━━━━━ 19:48:03\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 85/298 ━━━━━━━━━━━━━━━━━━━━ 19:48:54\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 86/298 ━━━━━━━━━━━━━━━━━━━━ 19:49:45\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 87/298 ━━━━━━━━━━━━━━━━━━━━ 19:50:38\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 88/298 ━━━━━━━━━━━━━━━━━━━━ 19:51:31\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 89/298 ━━━━━━━━━━━━━━━━━━━━ 19:52:25\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 90/298 ━━━━━━━━━━━━━━━━━━━━ 19:53:17\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 91/298 ━━━━━━━━━━━━━━━━━━━━ 19:54:10\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 92/298 ━━━━━━━━━━━━━━━━━━━━ 19:55:03\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 93/298 ━━━━━━━━━━━━━━━━━━━━ 19:55:58\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 94/298 ━━━━━━━━━━━━━━━━━━━━ 19:56:52\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 95/298 ━━━━━━━━━━━━━━━━━━━━ 19:57:44\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 96/298 ━━━━━━━━━━━━━━━━━━━━ 19:58:40\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 97/298 ━━━━━━━━━━━━━━━━━━━━ 19:59:33\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 98/298 ━━━━━━━━━━━━━━━━━━━━ 20:00:27\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 99/298 ━━━━━━━━━━━━━━━━━━━━ 20:01:19\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 100/298 ━━━━━━━━━━━━━━━━━━━━ 20:02:11\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 101/298 ━━━━━━━━━━━━━━━━━━━━ 20:03:04\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 102/298 ━━━━━━━━━━━━━━━━━━━━ 20:03:56\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 103/298 ━━━━━━━━━━━━━━━━━━━━ 20:04:51\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 104/298 ━━━━━━━━━━━━━━━━━━━━ 20:05:42\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 105/298 ━━━━━━━━━━━━━━━━━━━━ 20:06:37\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 106/298 ━━━━━━━━━━━━━━━━━━━━ 20:07:29\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 107/298 ━━━━━━━━━━━━━━━━━━━━ 20:08:23\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 108/298 ━━━━━━━━━━━━━━━━━━━━ 20:09:13\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 109/298 ━━━━━━━━━━━━━━━━━━━━ 20:10:04\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 110/298 ━━━━━━━━━━━━━━━━━━━━ 20:10:58\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 111/298 ━━━━━━━━━━━━━━━━━━━━ 20:11:49\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 112/298 ━━━━━━━━━━━━━━━━━━━━ 20:12:45\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 113/298 ━━━━━━━━━━━━━━━━━━━━ 20:13:37\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 114/298 ━━━━━━━━━━━━━━━━━━━━ 20:14:31\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 115/298 ━━━━━━━━━━━━━━━━━━━━ 20:15:22\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 116/298 ━━━━━━━━━━━━━━━━━━━━ 20:16:13\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 117/298 ━━━━━━━━━━━━━━━━━━━━ 20:17:06\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 118/298 ━━━━━━━━━━━━━━━━━━━━ 20:18:15\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 119/298 ━━━━━━━━━━━━━━━━━━━━ 20:19:12\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 120/298 ━━━━━━━━━━━━━━━━━━━━ 20:20:04\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 121/298 ━━━━━━━━━━━━━━━━━━━━ 20:21:05\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 122/298 ━━━━━━━━━━━━━━━━━━━━ 20:21:58\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 123/298 ━━━━━━━━━━━━━━━━━━━━ 20:22:57\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 124/298 ━━━━━━━━━━━━━━━━━━━━ 20:23:52\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 125/298 ━━━━━━━━━━━━━━━━━━━━ 20:24:55\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 126/298 ━━━━━━━━━━━━━━━━━━━━ 20:25:49\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 127/298 ━━━━━━━━━━━━━━━━━━━━ 20:26:47\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 128/298 ━━━━━━━━━━━━━━━━━━━━ 20:27:40\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 129/298 ━━━━━━━━━━━━━━━━━━━━ 20:28:35\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 130/298 ━━━━━━━━━━━━━━━━━━━━ 20:29:27\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 131/298 ━━━━━━━━━━━━━━━━━━━━ 20:30:19\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 132/298 ━━━━━━━━━━━━━━━━━━━━ 20:31:12\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 133/298 ━━━━━━━━━━━━━━━━━━━━ 20:32:04\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 134/298 ━━━━━━━━━━━━━━━━━━━━ 20:33:01\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 135/298 ━━━━━━━━━━━━━━━━━━━━ 20:33:54\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 136/298 ━━━━━━━━━━━━━━━━━━━━ 20:35:00\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 137/298 ━━━━━━━━━━━━━━━━━━━━ 20:35:54\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 138/298 ━━━━━━━━━━━━━━━━━━━━ 20:36:52\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 139/298 ━━━━━━━━━━━━━━━━━━━━ 20:37:43\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 140/298 ━━━━━━━━━━━━━━━━━━━━ 20:38:38\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 141/298 ━━━━━━━━━━━━━━━━━━━━ 20:39:31\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 142/298 ━━━━━━━━━━━━━━━━━━━━ 20:40:27\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 143/298 ━━━━━━━━━━━━━━━━━━━━ 20:41:20\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 144/298 ━━━━━━━━━━━━━━━━━━━━ 20:42:14\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 145/298 ━━━━━━━━━━━━━━━━━━━━ 20:43:10\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 146/298 ━━━━━━━━━━━━━━━━━━━━ 20:44:01\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 147/298 ━━━━━━━━━━━━━━━━━━━━ 20:44:57\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 148/298 ━━━━━━━━━━━━━━━━━━━━ 20:45:48\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 149/298 ━━━━━━━━━━━━━━━━━━━━ 20:46:41\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 150/298 ━━━━━━━━━━━━━━━━━━━━ 20:47:33\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 151/298 ━━━━━━━━━━━━━━━━━━━━ 20:48:27\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 152/298 ━━━━━━━━━━━━━━━━━━━━ 20:49:19\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 153/298 ━━━━━━━━━━━━━━━━━━━━ 20:50:11\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 154/298 ━━━━━━━━━━━━━━━━━━━━ 20:51:07\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 155/298 ━━━━━━━━━━━━━━━━━━━━ 20:51:58\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 156/298 ━━━━━━━━━━━━━━━━━━━━ 20:52:52\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 157/298 ━━━━━━━━━━━━━━━━━━━━ 20:53:43\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 158/298 ━━━━━━━━━━━━━━━━━━━━ 20:54:38\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 159/298 ━━━━━━━━━━━━━━━━━━━━ 20:55:38\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 160/298 ━━━━━━━━━━━━━━━━━━━━ 20:56:33\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 161/298 ━━━━━━━━━━━━━━━━━━━━ 20:57:28\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 162/298 ━━━━━━━━━━━━━━━━━━━━ 20:58:23\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 163/298 ━━━━━━━━━━━━━━━━━━━━ 20:59:21\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 164/298 ━━━━━━━━━━━━━━━━━━━━ 21:00:18\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 165/298 ━━━━━━━━━━━━━━━━━━━━ 21:01:18\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 166/298 ━━━━━━━━━━━━━━━━━━━━ 21:02:12\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 167/298 ━━━━━━━━━━━━━━━━━━━━ 21:03:11\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 168/298 ━━━━━━━━━━━━━━━━━━━━ 21:04:03\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 169/298 ━━━━━━━━━━━━━━━━━━━━ 21:04:58\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 170/298 ━━━━━━━━━━━━━━━━━━━━ 21:05:50\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 171/298 ━━━━━━━━━━━━━━━━━━━━ 21:06:52\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 172/298 ━━━━━━━━━━━━━━━━━━━━ 21:07:45\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 173/298 ━━━━━━━━━━━━━━━━━━━━ 21:08:42\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 174/298 ━━━━━━━━━━━━━━━━━━━━ 21:09:36\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 175/298 ━━━━━━━━━━━━━━━━━━━━ 21:10:32\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 176/298 ━━━━━━━━━━━━━━━━━━━━ 21:11:30\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 177/298 ━━━━━━━━━━━━━━━━━━━━ 21:12:26\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 178/298 ━━━━━━━━━━━━━━━━━━━━ 21:13:26\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 179/298 ━━━━━━━━━━━━━━━━━━━━ 21:14:29\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 180/298 ━━━━━━━━━━━━━━━━━━━━ 21:15:36\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 181/298 ━━━━━━━━━━━━━━━━━━━━ 21:16:29\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 182/298 ━━━━━━━━━━━━━━━━━━━━ 21:17:23\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 183/298 ━━━━━━━━━━━━━━━━━━━━ 21:18:27\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 184/298 ━━━━━━━━━━━━━━━━━━━━ 21:19:25\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 185/298 ━━━━━━━━━━━━━━━━━━━━ 21:20:20\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 186/298 ━━━━━━━━━━━━━━━━━━━━ 21:21:20\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 187/298 ━━━━━━━━━━━━━━━━━━━━ 21:22:15\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 188/298 ━━━━━━━━━━━━━━━━━━━━ 21:23:12\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 189/298 ━━━━━━━━━━━━━━━━━━━━ 21:24:08\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 190/298 ━━━━━━━━━━━━━━━━━━━━ 21:25:04\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 191/298 ━━━━━━━━━━━━━━━━━━━━ 21:26:08\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 192/298 ━━━━━━━━━━━━━━━━━━━━ 21:27:04\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 193/298 ━━━━━━━━━━━━━━━━━━━━ 21:27:58\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 194/298 ━━━━━━━━━━━━━━━━━━━━ 21:28:58\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 195/298 ━━━━━━━━━━━━━━━━━━━━ 21:29:51\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 196/298 ━━━━━━━━━━━━━━━━━━━━ 21:30:50\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 197/298 ━━━━━━━━━━━━━━━━━━━━ 21:31:46\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 198/298 ━━━━━━━━━━━━━━━━━━━━ 21:32:48\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 199/298 ━━━━━━━━━━━━━━━━━━━━ 21:33:56\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 200/298 ━━━━━━━━━━━━━━━━━━━━ 21:34:51\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 201/298 ━━━━━━━━━━━━━━━━━━━━ 21:35:42\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 202/298 ━━━━━━━━━━━━━━━━━━━━ 21:36:38\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 203/298 ━━━━━━━━━━━━━━━━━━━━ 21:37:30\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 204/298 ━━━━━━━━━━━━━━━━━━━━ 21:38:22\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 205/298 ━━━━━━━━━━━━━━━━━━━━ 21:39:18\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 206/298 ━━━━━━━━━━━━━━━━━━━━ 21:40:11\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 207/298 ━━━━━━━━━━━━━━━━━━━━ 21:41:07\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 208/298 ━━━━━━━━━━━━━━━━━━━━ 21:42:16\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 209/298 ━━━━━━━━━━━━━━━━━━━━ 21:43:19\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 210/298 ━━━━━━━━━━━━━━━━━━━━ 21:44:10\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 211/298 ━━━━━━━━━━━━━━━━━━━━ 21:45:06\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 212/298 ━━━━━━━━━━━━━━━━━━━━ 21:45:57\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 213/298 ━━━━━━━━━━━━━━━━━━━━ 21:46:51\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 214/298 ━━━━━━━━━━━━━━━━━━━━ 21:47:43\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 215/298 ━━━━━━━━━━━━━━━━━━━━ 21:48:40\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 216/298 ━━━━━━━━━━━━━━━━━━━━ 21:49:35\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 217/298 ━━━━━━━━━━━━━━━━━━━━ 21:50:31\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 218/298 ━━━━━━━━━━━━━━━━━━━━ 21:51:27\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 219/298 ━━━━━━━━━━━━━━━━━━━━ 21:52:19\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 220/298 ━━━━━━━━━━━━━━━━━━━━ 21:53:15\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 221/298 ━━━━━━━━━━━━━━━━━━━━ 21:54:07\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 222/298 ━━━━━━━━━━━━━━━━━━━━ 21:55:03\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 223/298 ━━━━━━━━━━━━━━━━━━━━ 21:55:54\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 224/298 ━━━━━━━━━━━━━━━━━━━━ 21:56:50\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 225/298 ━━━━━━━━━━━━━━━━━━━━ 21:57:45\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 226/298 ━━━━━━━━━━━━━━━━━━━━ 21:58:38\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 227/298 ━━━━━━━━━━━━━━━━━━━━ 21:59:34\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 228/298 ━━━━━━━━━━━━━━━━━━━━ 22:00:25\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 229/298 ━━━━━━━━━━━━━━━━━━━━ 22:01:27\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 230/298 ━━━━━━━━━━━━━━━━━━━━ 22:02:20\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 231/298 ━━━━━━━━━━━━━━━━━━━━ 22:03:19\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 232/298 ━━━━━━━━━━━━━━━━━━━━ 22:04:11\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 233/298 ━━━━━━━━━━━━━━━━━━━━ 22:05:25\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 234/298 ━━━━━━━━━━━━━━━━━━━━ 22:06:27\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 235/298 ━━━━━━━━━━━━━━━━━━━━ 22:07:26\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 236/298 ━━━━━━━━━━━━━━━━━━━━ 22:08:25\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 237/298 ━━━━━━━━━━━━━━━━━━━━ 22:09:28\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 238/298 ━━━━━━━━━━━━━━━━━━━━ 22:10:22\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 239/298 ━━━━━━━━━━━━━━━━━━━━ 22:11:23\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 240/298 ━━━━━━━━━━━━━━━━━━━━ 22:12:17\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 241/298 ━━━━━━━━━━━━━━━━━━━━ 22:13:12\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 242/298 ━━━━━━━━━━━━━━━━━━━━ 22:14:05\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 243/298 ━━━━━━━━━━━━━━━━━━━━ 22:15:01\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 244/298 ━━━━━━━━━━━━━━━━━━━━ 22:15:57\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 245/298 ━━━━━━━━━━━━━━━━━━━━ 22:17:00\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 246/298 ━━━━━━━━━━━━━━━━━━━━ 22:18:01\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 247/298 ━━━━━━━━━━━━━━━━━━━━ 22:18:57\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 248/298 ━━━━━━━━━━━━━━━━━━━━ 22:19:50\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 249/298 ━━━━━━━━━━━━━━━━━━━━ 22:20:45\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 250/298 ━━━━━━━━━━━━━━━━━━━━ 22:21:42\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 251/298 ━━━━━━━━━━━━━━━━━━━━ 22:22:37\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 252/298 ━━━━━━━━━━━━━━━━━━━━ 22:23:32\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 253/298 ━━━━━━━━━━━━━━━━━━━━ 22:24:24\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 254/298 ━━━━━━━━━━━━━━━━━━━━ 22:25:28\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 255/298 ━━━━━━━━━━━━━━━━━━━━ 22:26:22\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 256/298 ━━━━━━━━━━━━━━━━━━━━ 22:27:21\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 257/298 ━━━━━━━━━━━━━━━━━━━━ 22:28:14\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 258/298 ━━━━━━━━━━━━━━━━━━━━ 22:29:10\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 259/298 ━━━━━━━━━━━━━━━━━━━━ 22:30:02\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 260/298 ━━━━━━━━━━━━━━━━━━━━ 22:30:57\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 261/298 ━━━━━━━━━━━━━━━━━━━━ 22:31:58\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 262/298 ━━━━━━━━━━━━━━━━━━━━ 22:32:56\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 263/298 ━━━━━━━━━━━━━━━━━━━━ 22:33:49\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 264/298 ━━━━━━━━━━━━━━━━━━━━ 22:34:42\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 265/298 ━━━━━━━━━━━━━━━━━━━━ 22:35:35\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 266/298 ━━━━━━━━━━━━━━━━━━━━ 22:36:29\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 267/298 ━━━━━━━━━━━━━━━━━━━━ 22:37:28\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 268/298 ━━━━━━━━━━━━━━━━━━━━ 22:38:20\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 269/298 ━━━━━━━━━━━━━━━━━━━━ 22:39:15\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 270/298 ━━━━━━━━━━━━━━━━━━━━ 22:40:04\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 271/298 ━━━━━━━━━━━━━━━━━━━━ 22:40:57\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 272/298 ━━━━━━━━━━━━━━━━━━━━ 22:41:48\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 273/298 ━━━━━━━━━━━━━━━━━━━━ 22:42:40\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 274/298 ━━━━━━━━━━━━━━━━━━━━ 22:43:33\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 275/298 ━━━━━━━━━━━━━━━━━━━━ 22:44:24\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 276/298 ━━━━━━━━━━━━━━━━━━━━ 22:45:18\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 277/298 ━━━━━━━━━━━━━━━━━━━━ 22:46:11\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 278/298 ━━━━━━━━━━━━━━━━━━━━ 22:47:04\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 279/298 ━━━━━━━━━━━━━━━━━━━━ 22:47:55\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 280/298 ━━━━━━━━━━━━━━━━━━━━ 22:48:48\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 281/298 ━━━━━━━━━━━━━━━━━━━━ 22:49:41\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 282/298 ━━━━━━━━━━━━━━━━━━━━ 22:50:32\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 283/298 ━━━━━━━━━━━━━━━━━━━━ 22:51:26\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 284/298 ━━━━━━━━━━━━━━━━━━━━ 22:52:17\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 285/298 ━━━━━━━━━━━━━━━━━━━━ 22:53:13\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 286/298 ━━━━━━━━━━━━━━━━━━━━ 22:54:05\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 287/298 ━━━━━━━━━━━━━━━━━━━━ 22:54:59\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 288/298 ━━━━━━━━━━━━━━━━━━━━ 22:55:52\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 289/298 ━━━━━━━━━━━━━━━━━━━━ 22:56:42\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 290/298 ━━━━━━━━━━━━━━━━━━━━ 22:57:35\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 291/298 ━━━━━━━━━━━━━━━━━━━━ 22:58:25\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 292/298 ━━━━━━━━━━━━━━━━━━━━ 22:59:17\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 293/298 ━━━━━━━━━━━━━━━━━━━━ 23:00:07\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 294/298 ━━━━━━━━━━━━━━━━━━━━ 23:00:58\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 295/298 ━━━━━━━━━━━━━━━━━━━━ 23:01:54\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 296/298 ━━━━━━━━━━━━━━━━━━━━ 23:02:46\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 297/298 ━━━━━━━━━━━━━━━━━━━━ 23:03:40\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 298/298 ━━━━━━━━━━━━━━━━━━━━ 23:04:32\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Epoch 2/20\n",
      "Validation - Accuracy: 0.9807, Precision: 0.0000, Recall: 0.0000, Specificity: 1.0000, F1: 0.0000, Loss: 0.0057\n",
      "\n",
      "\n",
      "Epoch 3/20\n",
      "Batch 1/298 ━━━━━━━━━━━━━━━━━━━━ 23:28:00\n",
      "Accuracy: 0.9772 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 2/298 ━━━━━━━━━━━━━━━━━━━━ 23:28:52\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 3/298 ━━━━━━━━━━━━━━━━━━━━ 23:29:47\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 4/298 ━━━━━━━━━━━━━━━━━━━━ 23:30:36\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 5/298 ━━━━━━━━━━━━━━━━━━━━ 23:31:29\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 6/298 ━━━━━━━━━━━━━━━━━━━━ 23:32:19\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 7/298 ━━━━━━━━━━━━━━━━━━━━ 23:33:11\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 8/298 ━━━━━━━━━━━━━━━━━━━━ 23:34:01\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 9/298 ━━━━━━━━━━━━━━━━━━━━ 23:34:52\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 10/298 ━━━━━━━━━━━━━━━━━━━━ 23:35:46\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 11/298 ━━━━━━━━━━━━━━━━━━━━ 23:36:37\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 12/298 ━━━━━━━━━━━━━━━━━━━━ 23:37:30\n",
      "Accuracy: 0.9804 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 13/298 ━━━━━━━━━━━━━━━━━━━━ 23:38:20\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 14/298 ━━━━━━━━━━━━━━━━━━━━ 23:39:12\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 15/298 ━━━━━━━━━━━━━━━━━━━━ 23:40:04\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 16/298 ━━━━━━━━━━━━━━━━━━━━ 23:40:56\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 17/298 ━━━━━━━━━━━━━━━━━━━━ 23:41:51\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 18/298 ━━━━━━━━━━━━━━━━━━━━ 23:42:42\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 19/298 ━━━━━━━━━━━━━━━━━━━━ 23:43:35\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 20/298 ━━━━━━━━━━━━━━━━━━━━ 23:44:28\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 21/298 ━━━━━━━━━━━━━━━━━━━━ 23:45:23\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 22/298 ━━━━━━━━━━━━━━━━━━━━ 23:46:13\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 23/298 ━━━━━━━━━━━━━━━━━━━━ 23:47:04\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 24/298 ━━━━━━━━━━━━━━━━━━━━ 23:47:57\n",
      "Accuracy: 0.9800 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 25/298 ━━━━━━━━━━━━━━━━━━━━ 23:48:53\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 26/298 ━━━━━━━━━━━━━━━━━━━━ 23:50:01\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 27/298 ━━━━━━━━━━━━━━━━━━━━ 23:50:53\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 28/298 ━━━━━━━━━━━━━━━━━━━━ 23:51:48\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 29/298 ━━━━━━━━━━━━━━━━━━━━ 23:52:39\n",
      "Accuracy: 0.9801 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 30/298 ━━━━━━━━━━━━━━━━━━━━ 23:53:33\n",
      "Accuracy: 0.9802 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 31/298 ━━━━━━━━━━━━━━━━━━━━ 23:54:23\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 32/298 ━━━━━━━━━━━━━━━━━━━━ 23:55:14\n",
      "Accuracy: 0.9803 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 33/298 ━━━━━━━━━━━━━━━━━━━━ 23:56:06\n",
      "Accuracy: 0.9805 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 34/298 ━━━━━━━━━━━━━━━━━━━━ 23:56:57\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 35/298 ━━━━━━━━━━━━━━━━━━━━ 23:57:49\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 36/298 ━━━━━━━━━━━━━━━━━━━━ 23:58:39\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 37/298 ━━━━━━━━━━━━━━━━━━━━ 23:59:31\n",
      "Accuracy: 0.9806 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 38/298 ━━━━━━━━━━━━━━━━━━━━ 00:00:22\n",
      "Accuracy: 0.9807 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 39/298 ━━━━━━━━━━━━━━━━━━━━ 00:01:15\n",
      "Accuracy: 0.9807 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 40/298 ━━━━━━━━━━━━━━━━━━━━ 00:02:14\n",
      "Accuracy: 0.9807 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 41/298 ━━━━━━━━━━━━━━━━━━━━ 00:03:05\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 42/298 ━━━━━━━━━━━━━━━━━━━━ 00:04:00\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 43/298 ━━━━━━━━━━━━━━━━━━━━ 00:04:50\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 44/298 ━━━━━━━━━━━━━━━━━━━━ 00:05:45\n",
      "Accuracy: 0.9809 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0054\n",
      "\n",
      "Batch 45/298 ━━━━━━━━━━━━━━━━━━━━ 00:06:36\n",
      "Accuracy: 0.9808 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0055\n",
      "\n",
      "Batch 46/298 ━━━━━━━━━━━━━━━━━━━━ 00:07:28\n",
      "Accuracy: 0.9807 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0056\n",
      "\n",
      "Batch 47/298 ━━━━━━━━━━━━━━━━━━━━ 00:08:21\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 48/298 ━━━━━━━━━━━━━━━━━━━━ 00:09:12\n",
      "Accuracy: 0.9789 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 49/298 ━━━━━━━━━━━━━━━━━━━━ 00:10:06\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 50/298 ━━━━━━━━━━━━━━━━━━━━ 00:10:57\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 51/298 ━━━━━━━━━━━━━━━━━━━━ 00:11:50\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 52/298 ━━━━━━━━━━━━━━━━━━━━ 00:12:40\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 53/298 ━━━━━━━━━━━━━━━━━━━━ 00:13:33\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 54/298 ━━━━━━━━━━━━━━━━━━━━ 00:14:23\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 55/298 ━━━━━━━━━━━━━━━━━━━━ 00:15:14\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 56/298 ━━━━━━━━━━━━━━━━━━━━ 00:16:08\n",
      "Accuracy: 0.9789 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 57/298 ━━━━━━━━━━━━━━━━━━━━ 00:17:00\n",
      "Accuracy: 0.9790 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 58/298 ━━━━━━━━━━━━━━━━━━━━ 00:17:53\n",
      "Accuracy: 0.9791 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 59/298 ━━━━━━━━━━━━━━━━━━━━ 00:18:44\n",
      "Accuracy: 0.9792 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 60/298 ━━━━━━━━━━━━━━━━━━━━ 00:19:36\n",
      "Accuracy: 0.9792 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 61/298 ━━━━━━━━━━━━━━━━━━━━ 00:20:26\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 62/298 ━━━━━━━━━━━━━━━━━━━━ 00:21:16\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0067\n",
      "\n",
      "Batch 63/298 ━━━━━━━━━━━━━━━━━━━━ 00:22:08\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 64/298 ━━━━━━━━━━━━━━━━━━━━ 00:23:01\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 65/298 ━━━━━━━━━━━━━━━━━━━━ 00:23:54\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 66/298 ━━━━━━━━━━━━━━━━━━━━ 00:24:44\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 67/298 ━━━━━━━━━━━━━━━━━━━━ 00:25:38\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 68/298 ━━━━━━━━━━━━━━━━━━━━ 00:26:29\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 69/298 ━━━━━━━━━━━━━━━━━━━━ 00:27:20\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 70/298 ━━━━━━━━━━━━━━━━━━━━ 00:28:13\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 71/298 ━━━━━━━━━━━━━━━━━━━━ 00:29:04\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0066\n",
      "\n",
      "Batch 72/298 ━━━━━━━━━━━━━━━━━━━━ 00:29:57\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 73/298 ━━━━━━━━━━━━━━━━━━━━ 00:30:48\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 74/298 ━━━━━━━━━━━━━━━━━━━━ 00:31:41\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 75/298 ━━━━━━━━━━━━━━━━━━━━ 00:32:32\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 76/298 ━━━━━━━━━━━━━━━━━━━━ 00:33:23\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 77/298 ━━━━━━━━━━━━━━━━━━━━ 00:34:16\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 78/298 ━━━━━━━━━━━━━━━━━━━━ 00:35:08\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 79/298 ━━━━━━━━━━━━━━━━━━━━ 00:36:05\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 80/298 ━━━━━━━━━━━━━━━━━━━━ 00:36:56\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 81/298 ━━━━━━━━━━━━━━━━━━━━ 00:37:48\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 82/298 ━━━━━━━━━━━━━━━━━━━━ 00:38:44\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 83/298 ━━━━━━━━━━━━━━━━━━━━ 00:39:35\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 84/298 ━━━━━━━━━━━━━━━━━━━━ 00:40:29\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 85/298 ━━━━━━━━━━━━━━━━━━━━ 00:41:22\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 86/298 ━━━━━━━━━━━━━━━━━━━━ 00:42:16\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 87/298 ━━━━━━━━━━━━━━━━━━━━ 00:43:06\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 88/298 ━━━━━━━━━━━━━━━━━━━━ 00:43:59\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 89/298 ━━━━━━━━━━━━━━━━━━━━ 00:44:48\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 90/298 ━━━━━━━━━━━━━━━━━━━━ 00:45:40\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 91/298 ━━━━━━━━━━━━━━━━━━━━ 00:46:31\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 92/298 ━━━━━━━━━━━━━━━━━━━━ 00:47:21\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 93/298 ━━━━━━━━━━━━━━━━━━━━ 00:48:13\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 94/298 ━━━━━━━━━━━━━━━━━━━━ 00:49:03\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 95/298 ━━━━━━━━━━━━━━━━━━━━ 00:49:55\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 96/298 ━━━━━━━━━━━━━━━━━━━━ 00:50:46\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 97/298 ━━━━━━━━━━━━━━━━━━━━ 00:51:37\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 98/298 ━━━━━━━━━━━━━━━━━━━━ 00:52:28\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 99/298 ━━━━━━━━━━━━━━━━━━━━ 00:53:18\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 100/298 ━━━━━━━━━━━━━━━━━━━━ 00:54:10\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 101/298 ━━━━━━━━━━━━━━━━━━━━ 00:55:01\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 102/298 ━━━━━━━━━━━━━━━━━━━━ 00:55:53\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 103/298 ━━━━━━━━━━━━━━━━━━━━ 00:56:43\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 104/298 ━━━━━━━━━━━━━━━━━━━━ 00:57:34\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 105/298 ━━━━━━━━━━━━━━━━━━━━ 00:58:26\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 106/298 ━━━━━━━━━━━━━━━━━━━━ 00:59:17\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 107/298 ━━━━━━━━━━━━━━━━━━━━ 01:00:09\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 108/298 ━━━━━━━━━━━━━━━━━━━━ 01:00:59\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 109/298 ━━━━━━━━━━━━━━━━━━━━ 01:01:59\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 110/298 ━━━━━━━━━━━━━━━━━━━━ 01:02:50\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 111/298 ━━━━━━━━━━━━━━━━━━━━ 01:03:41\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 112/298 ━━━━━━━━━━━━━━━━━━━━ 01:04:34\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 113/298 ━━━━━━━━━━━━━━━━━━━━ 01:05:26\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 114/298 ━━━━━━━━━━━━━━━━━━━━ 01:06:21\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 115/298 ━━━━━━━━━━━━━━━━━━━━ 01:07:12\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 116/298 ━━━━━━━━━━━━━━━━━━━━ 01:08:05\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 117/298 ━━━━━━━━━━━━━━━━━━━━ 01:08:55\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 118/298 ━━━━━━━━━━━━━━━━━━━━ 01:09:45\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 119/298 ━━━━━━━━━━━━━━━━━━━━ 01:10:37\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 120/298 ━━━━━━━━━━━━━━━━━━━━ 01:11:29\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 121/298 ━━━━━━━━━━━━━━━━━━━━ 01:12:21\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 122/298 ━━━━━━━━━━━━━━━━━━━━ 01:13:12\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 123/298 ━━━━━━━━━━━━━━━━━━━━ 01:14:06\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 124/298 ━━━━━━━━━━━━━━━━━━━━ 01:14:57\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 125/298 ━━━━━━━━━━━━━━━━━━━━ 01:15:49\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 126/298 ━━━━━━━━━━━━━━━━━━━━ 01:16:43\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 127/298 ━━━━━━━━━━━━━━━━━━━━ 01:17:33\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 128/298 ━━━━━━━━━━━━━━━━━━━━ 01:18:26\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 129/298 ━━━━━━━━━━━━━━━━━━━━ 01:19:17\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 130/298 ━━━━━━━━━━━━━━━━━━━━ 01:20:11\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 131/298 ━━━━━━━━━━━━━━━━━━━━ 01:21:02\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 132/298 ━━━━━━━━━━━━━━━━━━━━ 01:21:53\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 133/298 ━━━━━━━━━━━━━━━━━━━━ 01:22:49\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 134/298 ━━━━━━━━━━━━━━━━━━━━ 01:23:40\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 135/298 ━━━━━━━━━━━━━━━━━━━━ 01:24:33\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 136/298 ━━━━━━━━━━━━━━━━━━━━ 01:25:25\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 137/298 ━━━━━━━━━━━━━━━━━━━━ 01:26:19\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 138/298 ━━━━━━━━━━━━━━━━━━━━ 01:27:10\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 139/298 ━━━━━━━━━━━━━━━━━━━━ 01:28:03\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 140/298 ━━━━━━━━━━━━━━━━━━━━ 01:28:55\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 141/298 ━━━━━━━━━━━━━━━━━━━━ 01:29:45\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 142/298 ━━━━━━━━━━━━━━━━━━━━ 01:30:39\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 143/298 ━━━━━━━━━━━━━━━━━━━━ 01:31:30\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 144/298 ━━━━━━━━━━━━━━━━━━━━ 01:32:23\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 145/298 ━━━━━━━━━━━━━━━━━━━━ 01:33:15\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 146/298 ━━━━━━━━━━━━━━━━━━━━ 01:34:09\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 147/298 ━━━━━━━━━━━━━━━━━━━━ 01:35:01\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 148/298 ━━━━━━━━━━━━━━━━━━━━ 01:35:52\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 149/298 ━━━━━━━━━━━━━━━━━━━━ 01:36:45\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 150/298 ━━━━━━━━━━━━━━━━━━━━ 01:37:36\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 151/298 ━━━━━━━━━━━━━━━━━━━━ 01:38:28\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 152/298 ━━━━━━━━━━━━━━━━━━━━ 01:39:19\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 153/298 ━━━━━━━━━━━━━━━━━━━━ 01:40:09\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 154/298 ━━━━━━━━━━━━━━━━━━━━ 01:41:02\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 155/298 ━━━━━━━━━━━━━━━━━━━━ 01:41:52\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 156/298 ━━━━━━━━━━━━━━━━━━━━ 01:42:48\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 157/298 ━━━━━━━━━━━━━━━━━━━━ 01:43:38\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 158/298 ━━━━━━━━━━━━━━━━━━━━ 01:44:31\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 159/298 ━━━━━━━━━━━━━━━━━━━━ 01:45:23\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 160/298 ━━━━━━━━━━━━━━━━━━━━ 01:46:14\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 161/298 ━━━━━━━━━━━━━━━━━━━━ 01:47:05\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 162/298 ━━━━━━━━━━━━━━━━━━━━ 01:47:55\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 163/298 ━━━━━━━━━━━━━━━━━━━━ 01:48:48\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 164/298 ━━━━━━━━━━━━━━━━━━━━ 01:49:39\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 165/298 ━━━━━━━━━━━━━━━━━━━━ 01:50:30\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 166/298 ━━━━━━━━━━━━━━━━━━━━ 01:51:21\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 167/298 ━━━━━━━━━━━━━━━━━━━━ 01:52:12\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 168/298 ━━━━━━━━━━━━━━━━━━━━ 01:53:04\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 169/298 ━━━━━━━━━━━━━━━━━━━━ 01:53:55\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 170/298 ━━━━━━━━━━━━━━━━━━━━ 01:54:50\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 171/298 ━━━━━━━━━━━━━━━━━━━━ 01:55:41\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 172/298 ━━━━━━━━━━━━━━━━━━━━ 01:56:34\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 173/298 ━━━━━━━━━━━━━━━━━━━━ 01:57:24\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 174/298 ━━━━━━━━━━━━━━━━━━━━ 01:58:17\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 175/298 ━━━━━━━━━━━━━━━━━━━━ 01:59:10\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 176/298 ━━━━━━━━━━━━━━━━━━━━ 02:00:01\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 177/298 ━━━━━━━━━━━━━━━━━━━━ 02:00:54\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 178/298 ━━━━━━━━━━━━━━━━━━━━ 02:01:49\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 179/298 ━━━━━━━━━━━━━━━━━━━━ 02:02:43\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 180/298 ━━━━━━━━━━━━━━━━━━━━ 02:03:34\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 181/298 ━━━━━━━━━━━━━━━━━━━━ 02:04:26\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 182/298 ━━━━━━━━━━━━━━━━━━━━ 02:05:17\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 183/298 ━━━━━━━━━━━━━━━━━━━━ 02:06:08\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 184/298 ━━━━━━━━━━━━━━━━━━━━ 02:07:02\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 185/298 ━━━━━━━━━━━━━━━━━━━━ 02:07:53\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 186/298 ━━━━━━━━━━━━━━━━━━━━ 02:08:47\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 187/298 ━━━━━━━━━━━━━━━━━━━━ 02:09:39\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 188/298 ━━━━━━━━━━━━━━━━━━━━ 02:10:34\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 189/298 ━━━━━━━━━━━━━━━━━━━━ 02:11:25\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 190/298 ━━━━━━━━━━━━━━━━━━━━ 02:12:17\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 191/298 ━━━━━━━━━━━━━━━━━━━━ 02:13:12\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 192/298 ━━━━━━━━━━━━━━━━━━━━ 02:14:03\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 193/298 ━━━━━━━━━━━━━━━━━━━━ 02:14:58\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 194/298 ━━━━━━━━━━━━━━━━━━━━ 02:15:50\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 195/298 ━━━━━━━━━━━━━━━━━━━━ 02:16:45\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 196/298 ━━━━━━━━━━━━━━━━━━━━ 02:17:37\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 197/298 ━━━━━━━━━━━━━━━━━━━━ 02:18:29\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 198/298 ━━━━━━━━━━━━━━━━━━━━ 02:19:22\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 199/298 ━━━━━━━━━━━━━━━━━━━━ 02:20:12\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 200/298 ━━━━━━━━━━━━━━━━━━━━ 02:21:06\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 201/298 ━━━━━━━━━━━━━━━━━━━━ 02:21:57\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 202/298 ━━━━━━━━━━━━━━━━━━━━ 02:22:52\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 203/298 ━━━━━━━━━━━━━━━━━━━━ 02:23:43\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 204/298 ━━━━━━━━━━━━━━━━━━━━ 02:24:36\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 205/298 ━━━━━━━━━━━━━━━━━━━━ 02:25:32\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 206/298 ━━━━━━━━━━━━━━━━━━━━ 02:26:23\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 207/298 ━━━━━━━━━━━━━━━━━━━━ 02:27:17\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 208/298 ━━━━━━━━━━━━━━━━━━━━ 02:28:08\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 209/298 ━━━━━━━━━━━━━━━━━━━━ 02:29:05\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 210/298 ━━━━━━━━━━━━━━━━━━━━ 02:30:01\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 211/298 ━━━━━━━━━━━━━━━━━━━━ 02:30:56\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 212/298 ━━━━━━━━━━━━━━━━━━━━ 02:31:48\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 213/298 ━━━━━━━━━━━━━━━━━━━━ 02:32:44\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 214/298 ━━━━━━━━━━━━━━━━━━━━ 02:33:40\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 215/298 ━━━━━━━━━━━━━━━━━━━━ 02:34:34\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 216/298 ━━━━━━━━━━━━━━━━━━━━ 02:35:26\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 217/298 ━━━━━━━━━━━━━━━━━━━━ 02:36:21\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 218/298 ━━━━━━━━━━━━━━━━━━━━ 02:37:18\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 219/298 ━━━━━━━━━━━━━━━━━━━━ 02:38:08\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 220/298 ━━━━━━━━━━━━━━━━━━━━ 02:39:01\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 221/298 ━━━━━━━━━━━━━━━━━━━━ 02:39:53\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 222/298 ━━━━━━━━━━━━━━━━━━━━ 02:40:45\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 223/298 ━━━━━━━━━━━━━━━━━━━━ 02:41:36\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 224/298 ━━━━━━━━━━━━━━━━━━━━ 02:42:28\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 225/298 ━━━━━━━━━━━━━━━━━━━━ 02:43:21\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 226/298 ━━━━━━━━━━━━━━━━━━━━ 02:44:16\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 227/298 ━━━━━━━━━━━━━━━━━━━━ 02:45:10\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 228/298 ━━━━━━━━━━━━━━━━━━━━ 02:46:02\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 229/298 ━━━━━━━━━━━━━━━━━━━━ 02:46:56\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 230/298 ━━━━━━━━━━━━━━━━━━━━ 02:47:47\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 231/298 ━━━━━━━━━━━━━━━━━━━━ 02:48:41\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 232/298 ━━━━━━━━━━━━━━━━━━━━ 02:49:33\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 233/298 ━━━━━━━━━━━━━━━━━━━━ 02:50:23\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 234/298 ━━━━━━━━━━━━━━━━━━━━ 02:51:18\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 235/298 ━━━━━━━━━━━━━━━━━━━━ 02:52:10\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 236/298 ━━━━━━━━━━━━━━━━━━━━ 02:53:03\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 237/298 ━━━━━━━━━━━━━━━━━━━━ 02:53:55\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 238/298 ━━━━━━━━━━━━━━━━━━━━ 02:54:49\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 239/298 ━━━━━━━━━━━━━━━━━━━━ 02:55:42\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 240/298 ━━━━━━━━━━━━━━━━━━━━ 02:56:32\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 241/298 ━━━━━━━━━━━━━━━━━━━━ 02:57:25\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 242/298 ━━━━━━━━━━━━━━━━━━━━ 02:58:16\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 243/298 ━━━━━━━━━━━━━━━━━━━━ 02:59:09\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 244/298 ━━━━━━━━━━━━━━━━━━━━ 02:59:59\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 245/298 ━━━━━━━━━━━━━━━━━━━━ 03:00:51\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 246/298 ━━━━━━━━━━━━━━━━━━━━ 03:01:45\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 247/298 ━━━━━━━━━━━━━━━━━━━━ 03:02:39\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 248/298 ━━━━━━━━━━━━━━━━━━━━ 03:03:37\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 249/298 ━━━━━━━━━━━━━━━━━━━━ 03:04:28\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 250/298 ━━━━━━━━━━━━━━━━━━━━ 03:05:23\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 251/298 ━━━━━━━━━━━━━━━━━━━━ 03:06:14\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 252/298 ━━━━━━━━━━━━━━━━━━━━ 03:07:08\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 253/298 ━━━━━━━━━━━━━━━━━━━━ 03:08:00\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 254/298 ━━━━━━━━━━━━━━━━━━━━ 03:08:53\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 255/298 ━━━━━━━━━━━━━━━━━━━━ 03:09:45\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 256/298 ━━━━━━━━━━━━━━━━━━━━ 03:10:37\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 257/298 ━━━━━━━━━━━━━━━━━━━━ 03:11:33\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 258/298 ━━━━━━━━━━━━━━━━━━━━ 03:12:25\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 259/298 ━━━━━━━━━━━━━━━━━━━━ 03:13:22\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 260/298 ━━━━━━━━━━━━━━━━━━━━ 03:14:14\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 261/298 ━━━━━━━━━━━━━━━━━━━━ 03:15:08\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 262/298 ━━━━━━━━━━━━━━━━━━━━ 03:16:00\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 263/298 ━━━━━━━━━━━━━━━━━━━━ 03:16:51\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 264/298 ━━━━━━━━━━━━━━━━━━━━ 03:17:44\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 265/298 ━━━━━━━━━━━━━━━━━━━━ 03:18:35\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 266/298 ━━━━━━━━━━━━━━━━━━━━ 03:19:29\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 267/298 ━━━━━━━━━━━━━━━━━━━━ 03:20:21\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 268/298 ━━━━━━━━━━━━━━━━━━━━ 03:21:15\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 269/298 ━━━━━━━━━━━━━━━━━━━━ 03:22:07\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 270/298 ━━━━━━━━━━━━━━━━━━━━ 03:23:01\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 271/298 ━━━━━━━━━━━━━━━━━━━━ 03:23:54\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 272/298 ━━━━━━━━━━━━━━━━━━━━ 03:24:46\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 273/298 ━━━━━━━━━━━━━━━━━━━━ 03:25:42\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 274/298 ━━━━━━━━━━━━━━━━━━━━ 03:26:34\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 275/298 ━━━━━━━━━━━━━━━━━━━━ 03:27:29\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 276/298 ━━━━━━━━━━━━━━━━━━━━ 03:28:20\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 277/298 ━━━━━━━━━━━━━━━━━━━━ 03:29:13\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 278/298 ━━━━━━━━━━━━━━━━━━━━ 03:30:05\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 279/298 ━━━━━━━━━━━━━━━━━━━━ 03:30:57\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 280/298 ━━━━━━━━━━━━━━━━━━━━ 03:31:52\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 281/298 ━━━━━━━━━━━━━━━━━━━━ 03:32:43\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 282/298 ━━━━━━━━━━━━━━━━━━━━ 03:33:37\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 283/298 ━━━━━━━━━━━━━━━━━━━━ 03:34:29\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 284/298 ━━━━━━━━━━━━━━━━━━━━ 03:35:25\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 285/298 ━━━━━━━━━━━━━━━━━━━━ 03:36:15\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 286/298 ━━━━━━━━━━━━━━━━━━━━ 03:37:07\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 287/298 ━━━━━━━━━━━━━━━━━━━━ 03:37:58\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 288/298 ━━━━━━━━━━━━━━━━━━━━ 03:38:49\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 289/298 ━━━━━━━━━━━━━━━━━━━━ 03:39:42\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 290/298 ━━━━━━━━━━━━━━━━━━━━ 03:40:33\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 291/298 ━━━━━━━━━━━━━━━━━━━━ 03:41:26\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 292/298 ━━━━━━━━━━━━━━━━━━━━ 03:42:17\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 293/298 ━━━━━━━━━━━━━━━━━━━━ 03:43:15\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 294/298 ━━━━━━━━━━━━━━━━━━━━ 03:44:08\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 295/298 ━━━━━━━━━━━━━━━━━━━━ 03:45:00\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 296/298 ━━━━━━━━━━━━━━━━━━━━ 03:45:56\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 297/298 ━━━━━━━━━━━━━━━━━━━━ 03:46:47\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 298/298 ━━━━━━━━━━━━━━━━━━━━ 03:47:40\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Epoch 3/20\n",
      "Validation - Accuracy: 0.9803, Precision: 0.0000, Recall: 0.0000, Specificity: 1.0000, F1: 0.0000, Loss: 0.0057\n",
      "\n",
      "\n",
      "Epoch 4/20\n",
      "Batch 1/298 ━━━━━━━━━━━━━━━━━━━━ 04:14:18\n",
      "Accuracy: 0.9756 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0068\n",
      "\n",
      "Batch 2/298 ━━━━━━━━━━━━━━━━━━━━ 04:15:30\n",
      "Accuracy: 0.9766 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 3/298 ━━━━━━━━━━━━━━━━━━━━ 04:16:25\n",
      "Accuracy: 0.9760 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0068\n",
      "\n",
      "Batch 4/298 ━━━━━━━━━━━━━━━━━━━━ 04:17:20\n",
      "Accuracy: 0.9755 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0068\n",
      "\n",
      "Batch 5/298 ━━━━━━━━━━━━━━━━━━━━ 04:18:17\n",
      "Accuracy: 0.9767 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 6/298 ━━━━━━━━━━━━━━━━━━━━ 04:19:15\n",
      "Accuracy: 0.9773 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 7/298 ━━━━━━━━━━━━━━━━━━━━ 04:20:11\n",
      "Accuracy: 0.9779 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 8/298 ━━━━━━━━━━━━━━━━━━━━ 04:21:05\n",
      "Accuracy: 0.9776 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 9/298 ━━━━━━━━━━━━━━━━━━━━ 04:21:59\n",
      "Accuracy: 0.9778 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 10/298 ━━━━━━━━━━━━━━━━━━━━ 04:22:51\n",
      "Accuracy: 0.9773 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 11/298 ━━━━━━━━━━━━━━━━━━━━ 04:23:47\n",
      "Accuracy: 0.9772 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 12/298 ━━━━━━━━━━━━━━━━━━━━ 04:24:39\n",
      "Accuracy: 0.9774 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 13/298 ━━━━━━━━━━━━━━━━━━━━ 04:25:39\n",
      "Accuracy: 0.9774 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 14/298 ━━━━━━━━━━━━━━━━━━━━ 04:26:33\n",
      "Accuracy: 0.9776 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 15/298 ━━━━━━━━━━━━━━━━━━━━ 04:27:26\n",
      "Accuracy: 0.9779 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 16/298 ━━━━━━━━━━━━━━━━━━━━ 04:28:23\n",
      "Accuracy: 0.9779 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 17/298 ━━━━━━━━━━━━━━━━━━━━ 04:29:15\n",
      "Accuracy: 0.9781 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 18/298 ━━━━━━━━━━━━━━━━━━━━ 04:30:10\n",
      "Accuracy: 0.9782 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 19/298 ━━━━━━━━━━━━━━━━━━━━ 04:31:15\n",
      "Accuracy: 0.9783 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 20/298 ━━━━━━━━━━━━━━━━━━━━ 04:32:13\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 21/298 ━━━━━━━━━━━━━━━━━━━━ 04:33:06\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 22/298 ━━━━━━━━━━━━━━━━━━━━ 04:34:04\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 23/298 ━━━━━━━━━━━━━━━━━━━━ 04:34:56\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 24/298 ━━━━━━━━━━━━━━━━━━━━ 04:35:52\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 25/298 ━━━━━━━━━━━━━━━━━━━━ 04:36:45\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 26/298 ━━━━━━━━━━━━━━━━━━━━ 04:37:41\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 27/298 ━━━━━━━━━━━━━━━━━━━━ 04:38:35\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 28/298 ━━━━━━━━━━━━━━━━━━━━ 04:39:29\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 29/298 ━━━━━━━━━━━━━━━━━━━━ 04:40:26\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 30/298 ━━━━━━━━━━━━━━━━━━━━ 04:41:18\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 31/298 ━━━━━━━━━━━━━━━━━━━━ 04:42:16\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 32/298 ━━━━━━━━━━━━━━━━━━━━ 04:43:07\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 33/298 ━━━━━━━━━━━━━━━━━━━━ 04:44:05\n",
      "Accuracy: 0.9790 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 34/298 ━━━━━━━━━━━━━━━━━━━━ 04:44:57\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 35/298 ━━━━━━━━━━━━━━━━━━━━ 04:45:51\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 36/298 ━━━━━━━━━━━━━━━━━━━━ 04:46:48\n",
      "Accuracy: 0.9785 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 37/298 ━━━━━━━━━━━━━━━━━━━━ 04:47:53\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 38/298 ━━━━━━━━━━━━━━━━━━━━ 04:48:46\n",
      "Accuracy: 0.9785 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 39/298 ━━━━━━━━━━━━━━━━━━━━ 04:49:39\n",
      "Accuracy: 0.9785 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 40/298 ━━━━━━━━━━━━━━━━━━━━ 04:50:33\n",
      "Accuracy: 0.9784 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 41/298 ━━━━━━━━━━━━━━━━━━━━ 04:51:28\n",
      "Accuracy: 0.9783 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 42/298 ━━━━━━━━━━━━━━━━━━━━ 04:52:28\n",
      "Accuracy: 0.9783 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0065\n",
      "\n",
      "Batch 43/298 ━━━━━━━━━━━━━━━━━━━━ 04:53:21\n",
      "Accuracy: 0.9784 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 44/298 ━━━━━━━━━━━━━━━━━━━━ 04:54:15\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 45/298 ━━━━━━━━━━━━━━━━━━━━ 04:55:07\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 46/298 ━━━━━━━━━━━━━━━━━━━━ 04:56:01\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 47/298 ━━━━━━━━━━━━━━━━━━━━ 04:56:54\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 48/298 ━━━━━━━━━━━━━━━━━━━━ 04:57:48\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0064\n",
      "\n",
      "Batch 49/298 ━━━━━━━━━━━━━━━━━━━━ 04:58:43\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 50/298 ━━━━━━━━━━━━━━━━━━━━ 04:59:37\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 51/298 ━━━━━━━━━━━━━━━━━━━━ 05:00:31\n",
      "Accuracy: 0.9785 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 52/298 ━━━━━━━━━━━━━━━━━━━━ 05:01:23\n",
      "Accuracy: 0.9785 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 53/298 ━━━━━━━━━━━━━━━━━━━━ 05:02:19\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 54/298 ━━━━━━━━━━━━━━━━━━━━ 05:03:27\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 55/298 ━━━━━━━━━━━━━━━━━━━━ 05:04:26\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 56/298 ━━━━━━━━━━━━━━━━━━━━ 05:05:21\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 57/298 ━━━━━━━━━━━━━━━━━━━━ 05:06:20\n",
      "Accuracy: 0.9786 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0063\n",
      "\n",
      "Batch 58/298 ━━━━━━━━━━━━━━━━━━━━ 05:07:12\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 59/298 ━━━━━━━━━━━━━━━━━━━━ 05:08:07\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 60/298 ━━━━━━━━━━━━━━━━━━━━ 05:08:59\n",
      "Accuracy: 0.9787 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 61/298 ━━━━━━━━━━━━━━━━━━━━ 05:09:52\n",
      "Accuracy: 0.9788 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 62/298 ━━━━━━━━━━━━━━━━━━━━ 05:10:47\n",
      "Accuracy: 0.9790 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0062\n",
      "\n",
      "Batch 63/298 ━━━━━━━━━━━━━━━━━━━━ 05:11:40\n",
      "Accuracy: 0.9790 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 64/298 ━━━━━━━━━━━━━━━━━━━━ 05:12:38\n",
      "Accuracy: 0.9792 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 65/298 ━━━━━━━━━━━━━━━━━━━━ 05:13:32\n",
      "Accuracy: 0.9792 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 66/298 ━━━━━━━━━━━━━━━━━━━━ 05:14:26\n",
      "Accuracy: 0.9792 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 67/298 ━━━━━━━━━━━━━━━━━━━━ 05:15:19\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 68/298 ━━━━━━━━━━━━━━━━━━━━ 05:16:17\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 69/298 ━━━━━━━━━━━━━━━━━━━━ 05:17:07\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 70/298 ━━━━━━━━━━━━━━━━━━━━ 05:18:03\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 71/298 ━━━━━━━━━━━━━━━━━━━━ 05:18:58\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 72/298 ━━━━━━━━━━━━━━━━━━━━ 05:20:00\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 73/298 ━━━━━━━━━━━━━━━━━━━━ 05:20:53\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 74/298 ━━━━━━━━━━━━━━━━━━━━ 05:21:45\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 75/298 ━━━━━━━━━━━━━━━━━━━━ 05:22:39\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 76/298 ━━━━━━━━━━━━━━━━━━━━ 05:23:34\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 77/298 ━━━━━━━━━━━━━━━━━━━━ 05:24:29\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 78/298 ━━━━━━━━━━━━━━━━━━━━ 05:25:21\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 79/298 ━━━━━━━━━━━━━━━━━━━━ 05:26:15\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 80/298 ━━━━━━━━━━━━━━━━━━━━ 05:27:07\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 81/298 ━━━━━━━━━━━━━━━━━━━━ 05:27:59\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 82/298 ━━━━━━━━━━━━━━━━━━━━ 05:28:55\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 83/298 ━━━━━━━━━━━━━━━━━━━━ 05:29:47\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 84/298 ━━━━━━━━━━━━━━━━━━━━ 05:30:44\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 85/298 ━━━━━━━━━━━━━━━━━━━━ 05:31:35\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 86/298 ━━━━━━━━━━━━━━━━━━━━ 05:32:31\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 87/298 ━━━━━━━━━━━━━━━━━━━━ 05:33:23\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 88/298 ━━━━━━━━━━━━━━━━━━━━ 05:34:16\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 89/298 ━━━━━━━━━━━━━━━━━━━━ 05:35:09\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 90/298 ━━━━━━━━━━━━━━━━━━━━ 05:36:15\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 91/298 ━━━━━━━━━━━━━━━━━━━━ 05:37:07\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 92/298 ━━━━━━━━━━━━━━━━━━━━ 05:37:59\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 93/298 ━━━━━━━━━━━━━━━━━━━━ 05:38:53\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 94/298 ━━━━━━━━━━━━━━━━━━━━ 05:39:44\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 95/298 ━━━━━━━━━━━━━━━━━━━━ 05:40:38\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 96/298 ━━━━━━━━━━━━━━━━━━━━ 05:41:29\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 97/298 ━━━━━━━━━━━━━━━━━━━━ 05:42:26\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 98/298 ━━━━━━━━━━━━━━━━━━━━ 05:43:19\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 99/298 ━━━━━━━━━━━━━━━━━━━━ 05:44:13\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 100/298 ━━━━━━━━━━━━━━━━━━━━ 05:45:07\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 101/298 ━━━━━━━━━━━━━━━━━━━━ 05:45:59\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 102/298 ━━━━━━━━━━━━━━━━━━━━ 05:46:53\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 103/298 ━━━━━━━━━━━━━━━━━━━━ 05:47:43\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 104/298 ━━━━━━━━━━━━━━━━━━━━ 05:48:36\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 105/298 ━━━━━━━━━━━━━━━━━━━━ 05:49:27\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 106/298 ━━━━━━━━━━━━━━━━━━━━ 05:50:20\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 107/298 ━━━━━━━━━━━━━━━━━━━━ 05:51:13\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 108/298 ━━━━━━━━━━━━━━━━━━━━ 05:52:20\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 109/298 ━━━━━━━━━━━━━━━━━━━━ 05:53:14\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 110/298 ━━━━━━━━━━━━━━━━━━━━ 05:54:10\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 111/298 ━━━━━━━━━━━━━━━━━━━━ 05:55:05\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 112/298 ━━━━━━━━━━━━━━━━━━━━ 05:55:57\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 113/298 ━━━━━━━━━━━━━━━━━━━━ 05:56:52\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 114/298 ━━━━━━━━━━━━━━━━━━━━ 05:57:44\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 115/298 ━━━━━━━━━━━━━━━━━━━━ 05:58:38\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 116/298 ━━━━━━━━━━━━━━━━━━━━ 05:59:29\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 117/298 ━━━━━━━━━━━━━━━━━━━━ 06:00:22\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 118/298 ━━━━━━━━━━━━━━━━━━━━ 06:01:16\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 119/298 ━━━━━━━━━━━━━━━━━━━━ 06:02:10\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 120/298 ━━━━━━━━━━━━━━━━━━━━ 06:03:10\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 121/298 ━━━━━━━━━━━━━━━━━━━━ 06:04:02\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 122/298 ━━━━━━━━━━━━━━━━━━━━ 06:04:58\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 123/298 ━━━━━━━━━━━━━━━━━━━━ 06:05:50\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 124/298 ━━━━━━━━━━━━━━━━━━━━ 06:06:48\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 125/298 ━━━━━━━━━━━━━━━━━━━━ 06:07:45\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 126/298 ━━━━━━━━━━━━━━━━━━━━ 06:08:45\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 127/298 ━━━━━━━━━━━━━━━━━━━━ 06:09:37\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 128/298 ━━━━━━━━━━━━━━━━━━━━ 06:10:31\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 129/298 ━━━━━━━━━━━━━━━━━━━━ 06:11:24\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 130/298 ━━━━━━━━━━━━━━━━━━━━ 06:12:17\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 131/298 ━━━━━━━━━━━━━━━━━━━━ 06:13:11\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 132/298 ━━━━━━━━━━━━━━━━━━━━ 06:14:03\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 133/298 ━━━━━━━━━━━━━━━━━━━━ 06:14:57\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 134/298 ━━━━━━━━━━━━━━━━━━━━ 06:15:48\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 135/298 ━━━━━━━━━━━━━━━━━━━━ 06:16:43\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 136/298 ━━━━━━━━━━━━━━━━━━━━ 06:17:34\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 137/298 ━━━━━━━━━━━━━━━━━━━━ 06:18:28\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 138/298 ━━━━━━━━━━━━━━━━━━━━ 06:19:21\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 139/298 ━━━━━━━━━━━━━━━━━━━━ 06:20:13\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 140/298 ━━━━━━━━━━━━━━━━━━━━ 06:21:08\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 141/298 ━━━━━━━━━━━━━━━━━━━━ 06:21:58\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 142/298 ━━━━━━━━━━━━━━━━━━━━ 06:22:52\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 143/298 ━━━━━━━━━━━━━━━━━━━━ 06:23:50\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 144/298 ━━━━━━━━━━━━━━━━━━━━ 06:24:49\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 145/298 ━━━━━━━━━━━━━━━━━━━━ 06:25:41\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 146/298 ━━━━━━━━━━━━━━━━━━━━ 06:26:34\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 147/298 ━━━━━━━━━━━━━━━━━━━━ 06:27:26\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 148/298 ━━━━━━━━━━━━━━━━━━━━ 06:28:19\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 149/298 ━━━━━━━━━━━━━━━━━━━━ 06:29:15\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 150/298 ━━━━━━━━━━━━━━━━━━━━ 06:30:07\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 151/298 ━━━━━━━━━━━━━━━━━━━━ 06:31:05\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 152/298 ━━━━━━━━━━━━━━━━━━━━ 06:32:00\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 153/298 ━━━━━━━━━━━━━━━━━━━━ 06:32:55\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 154/298 ━━━━━━━━━━━━━━━━━━━━ 06:33:47\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 155/298 ━━━━━━━━━━━━━━━━━━━━ 06:34:40\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 156/298 ━━━━━━━━━━━━━━━━━━━━ 06:35:32\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 157/298 ━━━━━━━━━━━━━━━━━━━━ 06:36:26\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 158/298 ━━━━━━━━━━━━━━━━━━━━ 06:37:20\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 159/298 ━━━━━━━━━━━━━━━━━━━━ 06:38:11\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 160/298 ━━━━━━━━━━━━━━━━━━━━ 06:39:04\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 161/298 ━━━━━━━━━━━━━━━━━━━━ 06:40:03\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 162/298 ━━━━━━━━━━━━━━━━━━━━ 06:41:03\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 163/298 ━━━━━━━━━━━━━━━━━━━━ 06:41:54\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 164/298 ━━━━━━━━━━━━━━━━━━━━ 06:42:47\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 165/298 ━━━━━━━━━━━━━━━━━━━━ 06:43:38\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 166/298 ━━━━━━━━━━━━━━━━━━━━ 06:44:30\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 167/298 ━━━━━━━━━━━━━━━━━━━━ 06:45:23\n",
      "Accuracy: 0.9797 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 168/298 ━━━━━━━━━━━━━━━━━━━━ 06:46:16\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 169/298 ━━━━━━━━━━━━━━━━━━━━ 06:47:09\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 170/298 ━━━━━━━━━━━━━━━━━━━━ 06:48:00\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 171/298 ━━━━━━━━━━━━━━━━━━━━ 06:48:53\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 172/298 ━━━━━━━━━━━━━━━━━━━━ 06:49:44\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 173/298 ━━━━━━━━━━━━━━━━━━━━ 06:50:37\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 174/298 ━━━━━━━━━━━━━━━━━━━━ 06:51:34\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 175/298 ━━━━━━━━━━━━━━━━━━━━ 06:52:26\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 176/298 ━━━━━━━━━━━━━━━━━━━━ 06:53:19\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 177/298 ━━━━━━━━━━━━━━━━━━━━ 06:54:10\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 178/298 ━━━━━━━━━━━━━━━━━━━━ 06:55:03\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 179/298 ━━━━━━━━━━━━━━━━━━━━ 06:56:02\n",
      "Accuracy: 0.9799 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0057\n",
      "\n",
      "Batch 180/298 ━━━━━━━━━━━━━━━━━━━━ 06:57:02\n",
      "Accuracy: 0.9798 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0058\n",
      "\n",
      "Batch 181/298 ━━━━━━━━━━━━━━━━━━━━ 06:57:53\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 182/298 ━━━━━━━━━━━━━━━━━━━━ 06:58:46\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0059\n",
      "\n",
      "Batch 183/298 ━━━━━━━━━━━━━━━━━━━━ 06:59:39\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 184/298 ━━━━━━━━━━━━━━━━━━━━ 07:00:31\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 185/298 ━━━━━━━━━━━━━━━━━━━━ 07:01:29\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 186/298 ━━━━━━━━━━━━━━━━━━━━ 07:02:20\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 187/298 ━━━━━━━━━━━━━━━━━━━━ 07:03:18\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 188/298 ━━━━━━━━━━━━━━━━━━━━ 07:04:09\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 189/298 ━━━━━━━━━━━━━━━━━━━━ 07:05:04\n",
      "Accuracy: 0.9793 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 190/298 ━━━━━━━━━━━━━━━━━━━━ 07:05:58\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 191/298 ━━━━━━━━━━━━━━━━━━━━ 07:06:51\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 192/298 ━━━━━━━━━━━━━━━━━━━━ 07:07:47\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 193/298 ━━━━━━━━━━━━━━━━━━━━ 07:08:39\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 194/298 ━━━━━━━━━━━━━━━━━━━━ 07:09:35\n",
      "Accuracy: 0.9794 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 195/298 ━━━━━━━━━━━━━━━━━━━━ 07:10:27\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 196/298 ━━━━━━━━━━━━━━━━━━━━ 07:11:22\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 197/298 ━━━━━━━━━━━━━━━━━━━━ 07:12:22\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 198/298 ━━━━━━━━━━━━━━━━━━━━ 07:13:23\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 199/298 ━━━━━━━━━━━━━━━━━━━━ 07:14:15\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 200/298 ━━━━━━━━━━━━━━━━━━━━ 07:15:08\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 201/298 ━━━━━━━━━━━━━━━━━━━━ 07:16:00\n",
      "Accuracy: 0.9795 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 202/298 ━━━━━━━━━━━━━━━━━━━━ 07:16:52\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 203/298 ━━━━━━━━━━━━━━━━━━━━ 07:17:47\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 204/298 ━━━━━━━━━━━━━━━━━━━━ 07:18:38\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 205/298 ━━━━━━━━━━━━━━━━━━━━ 07:19:32\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 206/298 ━━━━━━━━━━━━━━━━━━━━ 07:20:24\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 207/298 ━━━━━━━━━━━━━━━━━━━━ 07:21:18\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 208/298 ━━━━━━━━━━━━━━━━━━━━ 07:22:10\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 209/298 ━━━━━━━━━━━━━━━━━━━━ 07:23:05\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 210/298 ━━━━━━━━━━━━━━━━━━━━ 07:23:58\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 211/298 ━━━━━━━━━━━━━━━━━━━━ 07:24:49\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 212/298 ━━━━━━━━━━━━━━━━━━━━ 07:25:47\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0061\n",
      "\n",
      "Batch 213/298 ━━━━━━━━━━━━━━━━━━━━ 07:26:40\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 214/298 ━━━━━━━━━━━━━━━━━━━━ 07:27:38\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 215/298 ━━━━━━━━━━━━━━━━━━━━ 07:28:54\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n",
      "Batch 216/298 ━━━━━━━━━━━━━━━━━━━━ 07:30:03\n",
      "Accuracy: 0.9796 - Precision: 0.0000 - Recall: 0.0000 - Specificity: 1.0000 - F1: 0.0000 - Loss: 0.0060\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import plotly.figure_factory as ff  # For confusion matrix plot\n",
    "\n",
    "# Directory paths\n",
    "train_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\"\n",
    "train_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\train\\train_mask\"\n",
    "test_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\"\n",
    "test_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\test\\test_mask\"\n",
    "valid_img_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\Datasets\\ImageSegmentation\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\"\n",
    "valid_mask_dir = r\"C:\\Users\\Jaber\\OneDrive - University of Florida\\Educational\\GitHub\\ImageSegmentation\\Datasets\\Dental_XRay_Computacional_Vision_Segmentation\\Dental X_Ray\\valid\\valid_mask\"\n",
    "\n",
    "# Image generator to load data in batches\n",
    "def image_generator(img_dir, mask_dir, batch_size, img_size=(256, 256)):\n",
    "    img_files = os.listdir(img_dir)\n",
    "    while True:\n",
    "        images = []\n",
    "        masks = []\n",
    "        for img_file in img_files:\n",
    "            img_path = os.path.join(img_dir, img_file)\n",
    "            mask_file = img_file + \"_mask.png\"\n",
    "            mask_path = os.path.join(mask_dir, mask_file)\n",
    "\n",
    "            if os.path.exists(mask_path):\n",
    "                # Load image and mask in grayscale mode to match the input shape (256, 256, 1)\n",
    "                img = load_img(img_path, color_mode='grayscale', target_size=img_size)\n",
    "                img = img_to_array(img) / 255.0\n",
    "                mask = load_img(mask_path, color_mode='grayscale', target_size=img_size)\n",
    "                mask = img_to_array(mask) / 255.0\n",
    "\n",
    "                images.append(img)\n",
    "                masks.append(mask)\n",
    "\n",
    "            if len(images) == batch_size:\n",
    "                yield np.array(images), np.array(masks)\n",
    "                images = []\n",
    "                masks = []\n",
    "\n",
    "# Capsule Layer with Dynamic Routing\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = tf.reduce_sum(tf.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / tf.sqrt(s_squared_norm + tf.keras.backend.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, num_routing=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.num_routing = num_routing\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=[input_shape[-1], self.num_capsules * self.dim_capsule],\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = tf.reshape(inputs, [-1, inputs.shape[1] * inputs.shape[2], inputs.shape[3]])\n",
    "        u_hat = tf.einsum('...ij,jk->...ik', inputs, self.W)\n",
    "        u_hat = tf.reshape(u_hat, [-1, inputs.shape[1], self.num_capsules, self.dim_capsule])\n",
    "        b = tf.zeros(shape=[tf.shape(inputs)[0], inputs.shape[1], self.num_capsules])\n",
    "        for i in range(self.num_routing):\n",
    "            c = tf.nn.softmax(b, axis=-1)\n",
    "            s = tf.reduce_sum(c[..., tf.newaxis] * u_hat, axis=1)\n",
    "            v = squash(s)\n",
    "            if i < self.num_routing - 1:\n",
    "                b += tf.reduce_sum(u_hat * v[:, tf.newaxis, :, :], axis=-1)\n",
    "        return v\n",
    "\n",
    "# Attention Gate\n",
    "def attention_gate(x, g, inter_channels):\n",
    "    theta_x = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(x)\n",
    "    phi_g = tf.keras.layers.Conv2D(inter_channels, 1, padding='same')(g)\n",
    "    concat_xg = tf.keras.layers.add([theta_x, phi_g])\n",
    "    act_xg = tf.keras.layers.Activation('relu')(concat_xg)\n",
    "    psi = tf.keras.layers.Conv2D(1, 1, padding='same')(act_xg)\n",
    "    sigmoid_xg = tf.keras.layers.Activation('sigmoid')(psi)\n",
    "    y = tf.keras.layers.multiply([x, sigmoid_xg])\n",
    "    return y\n",
    "\n",
    "# U-Net with Capsule Network Layers, Attention Mechanisms, and Canny Edge Detection\n",
    "def unet_capsule_model(input_size=(256, 256, 1)):\n",
    "    inputs = tf.keras.layers.Input(input_size)\n",
    "    c1 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    c1 = CapsuleLayer(num_capsules=8, dim_capsule=16)(c1)\n",
    "    c1_flattened = tf.keras.layers.Flatten()(c1)\n",
    "    c1_reshaped = tf.keras.layers.Dense(256*256, activation='relu')(c1_flattened)\n",
    "    c1_reshaped = tf.keras.layers.Reshape((256, 256, 1))(c1_reshaped)\n",
    "    p1 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c1_reshaped)\n",
    "\n",
    "    c2 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(p1)\n",
    "    c2 = CapsuleLayer(num_capsules=16, dim_capsule=32)(c2)\n",
    "    c2_flattened = tf.keras.layers.Flatten()(c2)\n",
    "    c2_reshaped = tf.keras.layers.Dense(128*128, activation='relu')(c2_flattened)\n",
    "    c2_reshaped = tf.keras.layers.Reshape((128, 128, 1))(c2_reshaped)\n",
    "    p2 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c2_reshaped)\n",
    "\n",
    "    c3 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n",
    "    c3 = CapsuleLayer(num_capsules=32, dim_capsule=64)(c3)\n",
    "    c3_flattened = tf.keras.layers.Flatten()(c3)\n",
    "    c3_reshaped = tf.keras.layers.Dense(64*64, activation='relu')(c3_flattened)\n",
    "    c3_reshaped = tf.keras.layers.Reshape((64, 64, 1))(c3_reshaped)\n",
    "    p3 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c3_reshaped)\n",
    "\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(p3)\n",
    "    b = tf.keras.layers.Conv2D(512, 3, activation='relu', padding='same')(b)\n",
    "\n",
    "    u1 = tf.keras.layers.Conv2DTranspose(256, 2, strides=(2, 2), padding='same')(b)\n",
    "    c3_attention = attention_gate(c3_reshaped, u1, inter_channels=128)\n",
    "    u1 = tf.keras.layers.concatenate([u1, c3_attention])\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(u1)\n",
    "    c4 = tf.keras.layers.Conv2D(256, 3, activation='relu', padding='same')(c4)\n",
    "\n",
    "    u2 = tf.keras.layers.Conv2DTranspose(128, 2, strides=(2, 2), padding='same')(c4)\n",
    "    c2_attention = attention_gate(c2_reshaped, u2, inter_channels=64)\n",
    "    u2 = tf.keras.layers.concatenate([u2, c2_attention])\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(u2)\n",
    "    c5 = tf.keras.layers.Conv2D(128, 3, activation='relu', padding='same')(c5)\n",
    "\n",
    "    u3 = tf.keras.layers.Conv2DTranspose(64, 2, strides=(2, 2), padding='same')(c5)\n",
    "    c1_attention = attention_gate(c1_reshaped, u3, inter_channels=32)\n",
    "    u3 = tf.keras.layers.concatenate([u3, c1_attention])\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(u3)\n",
    "    c6 = tf.keras.layers.Conv2D(64, 3, activation='relu', padding='same')(c6)\n",
    "\n",
    "    outputs = tf.keras.layers.Conv2D(1, 1, activation='sigmoid')(c6)\n",
    "    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n",
    "    return model\n",
    "\n",
    "# Define custom metrics\n",
    "def custom_precision(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    predicted_positives = K.sum(y_pred_bin)\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def custom_recall(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_positives = K.sum(K.round(y_true * y_pred_bin))\n",
    "    possible_positives = K.sum(y_true)\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def custom_specificity(y_true, y_pred):\n",
    "    y_pred_bin = K.round(y_pred)\n",
    "    true_negatives = K.sum(K.round((1 - y_true) * (1 - y_pred_bin)))\n",
    "    possible_negatives = K.sum(1 - y_true)\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def custom_f1(y_true, y_pred):\n",
    "    precision = custom_precision(y_true, y_pred)\n",
    "    recall = custom_recall(y_true, y_pred)\n",
    "    return 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "\n",
    "# Define Focal Loss\n",
    "def focal_loss_fixed(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    epsilon = K.epsilon()\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    cross_entropy = -y_true * K.log(y_pred) - (1 - y_true) * K.log(1 - y_pred)\n",
    "    weight = alpha * y_true * K.pow((1 - y_pred), gamma) + (1 - alpha) * (1 - y_true) * K.pow(y_pred, gamma)\n",
    "    loss = weight * cross_entropy\n",
    "    return K.mean(loss)\n",
    "\n",
    "# Compile the model\n",
    "model = unet_capsule_model()\n",
    "model.compile(optimizer='adam', loss=focal_loss_fixed, metrics=['accuracy', custom_precision, custom_recall, custom_specificity, custom_f1])\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 16\n",
    "\n",
    "# Create data generators\n",
    "train_gen = image_generator(train_img_dir, train_mask_dir, batch_size)\n",
    "valid_gen = image_generator(valid_img_dir, valid_mask_dir, batch_size)\n",
    "test_gen = image_generator(test_img_dir, test_mask_dir, batch_size)\n",
    "\n",
    "# Number of steps per epoch\n",
    "steps_per_epoch = len(os.listdir(train_img_dir)) // batch_size\n",
    "validation_steps = len(os.listdir(valid_img_dir)) // batch_size\n",
    "test_steps = len(os.listdir(test_img_dir)) // batch_size\n",
    "\n",
    "# Custom callback to print more metrics at each batch and epoch for training, validation, and test sets\n",
    "class MetricsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, total_batches, X_valid, X_test):\n",
    "        super().__init__()\n",
    "        self.batch_counter = 1\n",
    "        self.total_batches = total_batches\n",
    "        self.current_epoch = 1\n",
    "        self.X_valid = X_valid\n",
    "        self.X_test = X_test\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.current_epoch = epoch + 1\n",
    "        print(f\"\\nEpoch {self.current_epoch}/{self.params['epochs']}\")\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        current_time = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        print(f\"Batch {self.batch_counter}/{self.total_batches} ━━━━━━━━━━━━━━━━━━━━ {current_time}\")\n",
    "        print(f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - Specificity: {specificity:.4f} - F1: {f1:.4f} - Loss: {loss:.4f}\\n\")\n",
    "        self.batch_counter += 1\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        accuracy = logs.get('accuracy', 0)\n",
    "        val_accuracy = logs.get('val_accuracy', 0)\n",
    "        loss = logs.get('loss', 0)\n",
    "        val_loss = logs.get('val_loss', 0)\n",
    "        precision = logs.get('custom_precision', 0)\n",
    "        val_precision = logs.get('val_custom_precision', 0)\n",
    "        recall = logs.get('custom_recall', 0)\n",
    "        val_recall = logs.get('val_custom_recall', 0)\n",
    "        f1 = logs.get('custom_f1', 0)\n",
    "        val_f1 = logs.get('val_custom_f1', 0)\n",
    "        specificity = logs.get('custom_specificity', 0)\n",
    "        val_specificity = logs.get('val_custom_specificity', 0)\n",
    "        print(f\"Epoch {epoch+1}/{self.params['epochs']}\")\n",
    "        print(f\"Validation - Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, Specificity: {val_specificity:.4f}, F1: {val_f1:.4f}, Loss: {val_loss:.4f}\\n\")\n",
    "\n",
    "        self.batch_counter = 1\n",
    "\n",
    "# Define early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Initialize the custom callback with validation and test data\n",
    "metrics_callback = MetricsCallback(total_batches=steps_per_epoch, X_valid=valid_gen, X_test=test_gen)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_gen,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    epochs=20,\n",
    "    validation_data=valid_gen,\n",
    "    validation_steps=validation_steps,\n",
    "    callbacks=[metrics_callback, early_stopping],\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save('dental_xray_capsule_unet_model.h5')\n",
    "\n",
    "# Evaluate on the test set **only after training is fully completed**\n",
    "y_test_pred = model.predict(test_gen, steps=test_steps)\n",
    "y_test_pred_bin = (y_test_pred > 0.5).astype(np.uint8)\n",
    "\n",
    "# Visualization: Show input image, true mask, and predicted mask for TEST set only\n",
    "def visualize_predictions(images, true_masks, pred_masks, title):\n",
    "    for i in range(3):  # Visualize first 3 predictions\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        # Original image\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(images[i].squeeze(), cmap='gray')\n",
    "        plt.title('Original Image')\n",
    "        \n",
    "        # Ground truth mask\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(true_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Ground Truth Mask')\n",
    "        \n",
    "        # Predicted mask\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(pred_masks[i].squeeze(), cmap='gray')\n",
    "        plt.title('Predicted Mask')\n",
    "        \n",
    "        plt.suptitle(title)\n",
    "        plt.show()\n",
    "\n",
    "# Visualize predictions for test set only\n",
    "X_test, y_test = next(test_gen)\n",
    "visualize_predictions(X_test, y_test, y_test_pred_bin, \"Test Set Predictions\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471815c7-84b1-4cd7-97b7-55cf73ece609",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61869f1-fe30-4135-8a61-c10d95ef1e04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8866c04-9dad-43ca-a0a9-0571575d22fe",
   "metadata": {},
   "outputs": [],
   "source": []
=======
>>>>>>> 09cd13ea (add)
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
<<<<<<< HEAD
=======
  }
>>>>>>> a9b1774f (update before helene storm!)
=======
>>>>>>> 09cd13ea (add)
